<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习小记 | Kyoku's Blog</title><meta name="author" content="Yuanpeng QU"><meta name="copyright" content="Yuanpeng QU"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习小记Created by: Yuanpeng QUCreated time: 2025年8月12日 17:41 I. 神经网络的核心组件与训练技巧1. 归一化层：BN 与 LN 的原理、区别与应用场景 核心目的：解决“内部协变量偏移 (Internal Covariate Shift)”问题。即在训练中，由于前层网络参数不断变化，导致后层网络接收到的数据分布一直在变，拖慢收敛速度。归一化层">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习小记">
<meta property="og:url" content="https://qyp9909.github.io/2025/08/12/DL_1/index.html">
<meta property="og:site_name" content="Kyoku&#39;s Blog">
<meta property="og:description" content="深度学习小记Created by: Yuanpeng QUCreated time: 2025年8月12日 17:41 I. 神经网络的核心组件与训练技巧1. 归一化层：BN 与 LN 的原理、区别与应用场景 核心目的：解决“内部协变量偏移 (Internal Covariate Shift)”问题。即在训练中，由于前层网络参数不断变化，导致后层网络接收到的数据分布一直在变，拖慢收敛速度。归一化层">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://qyp9909.github.io/images/cover/Rec_AD_Cover.png">
<meta property="article:published_time" content="2025-08-12T03:00:00.000Z">
<meta property="article:modified_time" content="2025-08-13T05:23:31.916Z">
<meta property="article:author" content="Yuanpeng QU">
<meta property="article:tag" content="Study">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qyp9909.github.io/images/cover/Rec_AD_Cover.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习小记",
  "url": "https://qyp9909.github.io/2025/08/12/DL_1/",
  "image": "https://qyp9909.github.io/images/cover/Rec_AD_Cover.png",
  "datePublished": "2025-08-12T03:00:00.000Z",
  "dateModified": "2025-08-13T05:23:31.916Z",
  "author": [
    {
      "@type": "Person",
      "name": "Yuanpeng QU",
      "url": "https://qyp9909.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qyp9909.github.io/2025/08/12/DL_1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习小记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" media="defer" onload="this.media='all'"><link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><link rel="preload" as="image" href="/img/1.jpg"><link rel="preload" href="/fonts/circle400w.ttf" as="font" type="font/ttf" crossorigin="anonymous"><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/img/1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/LOGO.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-vihara"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-newspaper"></i><span> Articles</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-mug-hot"></i><span> Life</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-camera-retro"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fab fa-itunes-note"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fab fa-youtube"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-torii-gate"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-star-of-david"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Kyoku's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习小记</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-vihara"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-newspaper"></i><span> Articles</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-mug-hot"></i><span> Life</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-camera-retro"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fab fa-itunes-note"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fab fa-youtube"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-torii-gate"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-star-of-david"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深度学习小记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-08-12T03:00:00.000Z" title="Created 2025-08-12 12:00:00">2025-08-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-08-13T05:23:31.916Z" title="Updated 2025-08-13 14:23:31">2025-08-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Recsys/">Recsys</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="深度学习小记"><a href="#深度学习小记" class="headerlink" title="深度学习小记"></a>深度学习小记</h2><p>Created by: Yuanpeng QU<br>Created time: 2025年8月12日 17:41</p>
<h2 id="I-神经网络的核心组件与训练技巧"><a href="#I-神经网络的核心组件与训练技巧" class="headerlink" title="I. 神经网络的核心组件与训练技巧"></a><strong>I. 神经网络的核心组件与训练技巧</strong></h2><h3 id="1-归一化层：BN-与-LN-的原理、区别与应用场景"><a href="#1-归一化层：BN-与-LN-的原理、区别与应用场景" class="headerlink" title="1. 归一化层：BN 与 LN 的原理、区别与应用场景"></a><strong>1. 归一化层：BN 与 LN 的原理、区别与应用场景</strong></h3><ul>
<li><strong>核心目的</strong>：解决“内部协变量偏移 (Internal Covariate Shift)”问题。即在训练中，由于前层网络参数不断变化，导致后层网络接收到的数据分布一直在变，拖慢收敛速度。归一化层通过将每层网络的输入强制拉回到一个稳定的分布（如均值为0，方差为1），从而加速训练。</li>
<li><strong>Batch Normalization (BN)</strong><ul>
<li><strong>原理</strong>：“纵向”或“按特征”归一化。它在一个批次（mini-batch）内，对<strong>每一个特征维度</strong>计算均值和方差，并进行归一化。</li>
<li><strong>关键机制</strong>：引入了两个可学习的参数 γ (缩放) 和 β (平移)，让网络可以自主决定是否以及在多大程度上<strong>恢复原始的分布</strong>，以保证模型的表达能力。</li>
<li><strong>训练 vs 推理</strong>：训练时使用当前批次的统计量，同时用滑动平均记录全局统计量；推理时则使用保存下来的全局统计量，以保证输出的确定性。</li>
<li><strong>应用场景</strong>：在<strong>卷积神经网络 (CNN)</strong> 和处理表格数据的<strong>多层感知机 (MLP)</strong> 中效果显著，前提是批次大小（Batch Size）不能过小。</li>
</ul>
</li>
<li><strong>Layer Normalization (LN)</strong><ul>
<li><strong>原理</strong>：“横向”或“按样本”归一化。它在<strong>单个样本内部</strong>，对<strong>该样本的所有特征维度</strong>计算均值和方差，并进行归一化。</li>
<li><strong>关键优势</strong>：其计算完全<strong>独立于批次大小</strong>，与批次内的其他样本无关。</li>
<li><strong>应用场景</strong>：完美适用于<strong>自然语言处理 (NLP)</strong> 领域，尤其是 <strong>RNN 和 Transformer</strong>。因为 NLP 任务的序列长度经常变化，需要 padding，BN 的效果会受到严重干扰，而 LN 则完全不受影响。在 Transformer 中，Pre-LN 的设计比 Post-LN 训练更稳定，是当前的主流。</li>
</ul>
</li>
</ul>
<h3 id="2-梯度问题：梯度消失与梯度爆炸"><a href="#2-梯度问题：梯度消失与梯度爆炸" class="headerlink" title="2. 梯度问题：梯度消失与梯度爆炸"></a><strong>2. 梯度问题：梯度消失与梯度爆炸</strong></h3><ul>
<li><p><strong>本质成因</strong>：两者都是由于<strong>深层网络的链式求导法则</strong>所引发的累积效应。梯度的计算是一个长链条的连乘，如果连乘因子持续大于1，则梯度爆炸；如果持续小于1，则梯度消失。当我们想计算第一层网络参数W1的梯度时，链式求导法则如下：</p>
<p>  $$<br>  \frac{\partial L}{\partial W_1} &#x3D; \frac{\partial L}{\partial a_n} \cdot \frac{\partial a_n}{\partial a_{n-1}} \cdot \frac{\partial a_{n-1}}{\partial a_{n-2}} \cdot \dots \cdot \frac{\partial a_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial W_1}<br>  $$</p>
<p>  这里的每一项 ∂ai&#x2F;∂ai−1 都与第 i 层的<strong>权重矩阵</strong>和<strong>激活函数的导数</strong>有关。整个梯度就变成了一个<strong>非常长的乘法链</strong>。</p>
<p>  现在，问题就出在这个“连乘”上：</p>
<ul>
<li>如果连乘的因子大部分都<strong>小于 1</strong>，那么最终的乘积就会<strong>指数级地趋近于 0</strong>。这就是<strong>梯度消失</strong>。</li>
<li>如果连乘的因子大部分都<strong>大于 1</strong>，那么最终的乘积就会<strong>指数级地变得巨大</strong>。这就是<strong>梯度爆炸</strong>。</li>
</ul>
</li>
<li><p><strong>梯度消失 (Gradient Vanishing)</strong></p>
<ul>
<li><strong>现象</strong>：靠近输入层的网络无法得到有效的梯度信号，参数更新缓慢，模型学不动。</li>
<li><strong>主要原因</strong>：不合适的激活函数（如 Sigmoid 在其饱和区的导数接近0）或过小的权重。</li>
<li><strong>解决方案“工具箱”</strong>：<ol>
<li><strong>更换激活函数</strong>：使用 <strong>ReLU</strong> 及其变体（如 GELU, SiLU），它们在正区间的导数为1，不会造成梯度衰减。</li>
<li><strong>使用残差连接 (Residual Connections)</strong>：ResNet 的核心。通过“快捷方式”让梯度可以直接流向前层，将“连乘”变为“连加”，从根本上解决问题。</li>
<li><strong>使用归一化层 (BN&#x2F;LN)</strong>：将数据拉回到激活函数的非饱和区，保证梯度的活性。</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>梯度爆炸 (Gradient Explosion)</strong></p>
<ul>
<li><strong>现象</strong>：梯度变得极大，导致参数更新步子过大，损失函数剧烈震荡甚至变为 <code>NaN</code>，训练崩溃。</li>
<li><strong>主要原因</strong>：过大的权重初始化。</li>
<li><strong>解决方案“工具箱”</strong>：<ol>
<li><strong>梯度裁剪 (Gradient Clipping)</strong>：最直接有效的方法。给梯度设置一个范数上限，超过则按比例缩放，从而限制单次更新的最大步长。</li>
<li><strong>使用归一化层 (BN&#x2F;LN)</strong>：同样能起到稳定激活值，间接稳定梯度的作用。</li>
<li><strong>合适的权重初始化</strong>：如 Xavier 或 He 初始化，从源头上避免权重过大。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="3-初始化策略"><a href="#3-初始化策略" class="headerlink" title="3. 初始化策略"></a><strong>3. 初始化策略</strong></h3><ul>
<li><strong>核心目的：打破对称性 (Break the Symmetry)</strong><ul>
<li><strong>问题</strong>：如果将同一层的所有权重初始化为相同的值（如全0或全1），那么在正向和反向传播中，这些神经元的行为将<strong>永远保持一致</strong>，它们会学到完全相同的特征。这等效于该层只有一个神经元在工作，完全浪费了网络的表达能力。</li>
<li><strong>解决</strong>：必须通过<strong>随机初始化</strong>来赋予每个神经元独特的初始状态，让它们有机会在训练中学习到不同的特征。</li>
</ul>
</li>
<li><strong>Transformer 为何不常用 He 初始化？</strong><ul>
<li><strong>He 初始化的专长</strong>：专为 <strong>ReLU</strong> 网络设计，目的是维持激活值的方差稳定。</li>
<li><strong>Transformer 的特殊性</strong>：<ol>
<li><strong>无处不在的 LayerNorm</strong>：Transformer 中大量使用 LN，它本身就是一个强大的稳定器，在每层之后都对数据分布进行“重置”，这使得网络对权重的初始尺度不那么敏感。</li>
<li><strong>投影层的需求</strong>：Transformer 中的许多线性层用于“投影”（如生成 QKV），其目标是稳定地传递信息。<strong>Xavier&#x2F;Glorot 初始化</strong>同时考虑了输入和输出维度，旨在维持信号方差在流经该层时保持不变，理论上更适合这种场景。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="II-优化算法的演进之路"><a href="#II-优化算法的演进之路" class="headerlink" title="II. 优化算法的演进之路"></a><strong>II. 优化算法的演进之路</strong></h2><h3 id="1-核心思想：一阶动量与二阶动量"><a href="#1-核心思想：一阶动量与二阶动量" class="headerlink" title="1. 核心思想：一阶动量与二阶动量"></a><strong>1. 核心思想：一阶动量与二阶动量</strong></h3><p>现代优化器的设计，都围绕着这两个核心概念，它们源于统计学中的“矩”：</p>
<ul>
<li><strong>一阶动量 (First-Order Momentum, mt)</strong><ul>
<li><strong>统计意义</strong>：梯度的<strong>指数移动平均值</strong>，是对梯度分布<strong>均值</strong>的估计。</li>
<li><strong>物理意义</strong>：“惯性”或“速度”。</li>
<li><strong>作用</strong>：决定参数更新的<strong>主要方向</strong>。通过累积历史梯度，它可以平滑更新路径，抑制在“峡谷”地带的震荡，并在梯度方向一致时加速前进。</li>
</ul>
</li>
<li><strong>二阶动量 (Second-Order Momentum, Vt)</strong><ul>
<li><strong>统计意义</strong>：梯度<strong>平方</strong>的指数移动平均值，是对梯度分布<strong>方差</strong>的估计。</li>
<li><strong>物理意义</strong>：“摩擦力”或“路况感知器”。</li>
<li><strong>作用</strong>：实现<strong>自适应学习率</strong>。它被放在学习率计算公式的分母位置。对于梯度变化剧烈（方差大）的参数，其有效学习率会变小，步子更稳；对于梯度一直很小（方差小）的参数，其有效学习率会变大，鼓励其更新。</li>
</ul>
</li>
</ul>
<p><strong>优化算法的统一框架</strong></p>
<ol>
<li><p><strong>计算梯度</strong>：计算当前参数 wt 的梯度 <code>g_t=∇f(w_t)</code>。</p>
</li>
<li><p><strong>更新动量</strong>：根据历史梯度计算<code>一阶动量 m_t（梯度的均值）</code>和<code>二阶动量 V_t（梯度平方的均值）</code>。</p>
</li>
<li><p><strong>计算下降步长</strong>：<code>η_t=α⋅m_t/V_t</code>。</p>
<p> [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
 c69,-144,104.5,-217.7,106.5,-221
 l0 -0
 c5.3,-9.3,12,-14,20,-14
 H400000v40H845.2724
 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
 M834 80h400000v40h-400000z"/></svg>)</p>
</li>
<li><p><strong>更新参数</strong>：<code>w_&#123;t+1&#125;=w_t−η_t</code>。</p>
</li>
</ol>
<p>这个框架的核心在于：</p>
<p><strong>参数的更新方向由一阶动量 mt 决定，而更新的步长（学习率）由二阶动量 Vt 进行动态调整</strong></p>
<p>。不同的优化算法，就是在这四步，特别是第 2 步上做了不同的设计。</p>
<h3 id="2-SGD-家族：从“盲人摸象”到“带惯性下坡”"><a href="#2-SGD-家族：从“盲人摸象”到“带惯性下坡”" class="headerlink" title="2. SGD 家族：从“盲人摸象”到“带惯性下坡”"></a><strong>2. SGD 家族：从“盲人摸象”到“带惯性下坡”</strong></h3><p>这个家族的特点是使用<strong>全局统一的学习率</strong>。</p>
<ul>
<li><strong>SGD (随机梯度下降)</strong><ul>
<li><strong>做法</strong>：只看当前一步的梯度方向，然后走一小步。</li>
<li>SGD <strong>没有动量 (Momentum)</strong> 的概念。可以理解为：一阶动量mt就是当前梯度gt，二阶动量Vt不使用，或者说恒为1。</li>
<li>所以按照上面的公式3代入SGD的下降步长为<code>η_t=α·gt</code>，更新参数则为<code>w_&#123;t+1&#125;=w_t−α·gt</code></li>
<li><strong>问题</strong>：非常“短视”，容易在曲折的路径上震荡，收敛缓慢。</li>
</ul>
</li>
<li><strong>SGDM (SGD with Momentum)</strong><ul>
<li><p><strong>改进</strong>：引入了<strong>一阶动量</strong>，像一个有惯性的小球，累积过去的速度。</p>
<p>  $$<br>  m_t &#x3D; \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t<br>  $$</p>
</li>
<li><p>mt：t时刻的动量，代表了历史梯度的累积方向。</p>
</li>
<li><p>beta1: 动量系数，通常取 0.9。这个值意味着，当前的更新方向<strong>主要由历史方向决定 (90%)</strong>，只受当前梯度的一点点影响 (10%)。</p>
</li>
<li><p>二阶动量Vt仍然不使用。</p>
</li>
<li><p>代入公式3，得到SGDM下降步长为<code>η_t=α·mt</code>,更新参数则为<code>w_&#123;t+1&#125;=w_t−α·mt</code></p>
</li>
<li><p><strong>效果</strong>：极大地缓解了震荡，加速了收敛。</p>
</li>
<li><p><strong>新问题</strong>：惯性是“盲目”的，可能会因为速度太快而冲过最优解。</p>
</li>
</ul>
</li>
<li><strong>NAG (Nesterov Accelerated Gradient)</strong><ul>
<li><strong>改进</strong>：引入了“预判”机制。它会先“探头”看一下按照当前动量走一步后会到达什么位置，然后用那个“未来”位置的梯度来修正当前的方向。</li>
<li><strong>效果</strong>：更“聪明”，能有效防止冲过头，收敛通常更稳定。</li>
</ul>
</li>
</ul>
<h3 id="3-自适应家族：为每个参数定制学习率"><a href="#3-自适应家族：为每个参数定制学习率" class="headerlink" title="3. 自适应家族：为每个参数定制学习率"></a><strong>3. 自适应家族：为每个参数定制学习率</strong></h3><p>这个家族解决了 SGD 家族“一刀切”学习率的问题。</p>
<ul>
<li><p><strong>AdaGrad</strong></p>
<p>  AdaGrad 第一个提出了这个革命性的想法。</p>
<ul>
<li><p><strong>核心动机</strong>：<br>  一个参数过去的梯度历史，可以指导它未来的更新步伐。</p>
<ul>
<li>如果一个参数的梯度一直很大，说明它可能处于一个“陡峭”的区域，我们应该小心翼翼，用一个<strong>较小的学习率</strong>。</li>
<li>如果一个参数的梯度一直很小（甚至是0，例如稀疏特征），说明它很少被更新。一旦它有机会更新，我们应该给它一个<strong>较大的学习率</strong>，让它能“迎头赶上”。</li>
</ul>
</li>
<li><p><strong>实现机制</strong>：<br>  AdaGrad 为每一个参数 w(i) 维护一个历史梯度平方的<strong>累加器</strong> Vt(i)。</p>
<ol>
<li><p><strong>累积梯度平方和</strong>：在每一步 t，计算当前梯度 gt(i)，并将其平方累加到 Vt(i) 中。</p>
<p> $$<br> V_t^{(i)} &#x3D; V_{t-1}^{(i)} + (g_t^{(i)})^2<br> $$</p>
</li>
<li><p><strong>更新参数</strong>：在更新时，将全局学习率 α 除以 根号Vt(i)。ϵ 是一个极小值防止分母为零</p>
<p> [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
 l0 -0
 c4,-6.7,10,-10,18,-10 H400000v40
 H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
 s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
 c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
 c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
 c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
 c53.7,-170.3,84.5,-266.8,92.5,-289.5z
 M1001 80h400000v40h-400000z"/></svg>)</p>
<p> [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
 l0 -0
 c4,-6.7,10,-10,18,-10 H400000v40
 H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
 s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
 c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
 c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
 c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
 c53.7,-170.3,84.5,-266.8,92.5,-289.5z
 M1001 80h400000v40h-400000z"/></svg>)</p>
<p> $$<br> w_{t+1}^{(i)} &#x3D; w_t^{(i)} - \frac{\alpha}{\sqrt{V_t^{(i)} + \epsilon}} \cdot g_t^{(i)}<br> $$</p>
</li>
</ol>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
  l0 -0
  c4,-6.7,10,-10,18,-10 H400000v40
  H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
  s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
  c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
  c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
  c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
  c53.7,-170.3,84.5,-266.8,92.5,-289.5z
  M1001 80h400000v40h-400000z"/></svg>)</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
  l0 -0
  c4,-6.7,10,-10,18,-10 H400000v40
  H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
  s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
  c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
  c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
  c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
  c53.7,-170.3,84.5,-266.8,92.5,-289.5z
  M1001 80h400000v40h-400000z"/></svg>)</p>
<ul>
<li><strong>优点</strong>：<ul>
<li>在处理<strong>稀疏数据</strong>（如搜广推中的海量ID类特征）时表现极其出色。对于那些不常出现的特征，其 Vt 累积很慢，因此能保持较大的学习率，保证了有效的学习。</li>
</ul>
</li>
<li><strong>致命缺陷</strong>：<ul>
<li><p>由于 (gt(i))2 永远是正数，累加器 Vt(i) 是<strong>严格单调递增</strong>的。</p>
</li>
<li><p>随着训练的进行，Vt 会变得越来越大，导致分母 Vt(i) 也无限增大。</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
  l0 -0
  c4,-6.7,10,-10,18,-10 H400000v40
  H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
  s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
  c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
  c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
  c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
  c53.7,-170.3,84.5,-266.8,92.5,-289.5z
  M1001 80h400000v40h-400000z"/></svg>)</p>
</li>
<li><p>这使得有效学习率 α&#x2F;根号Vt(i) 会<strong>持续衰减，并最终趋近于零</strong>。这会导致模型在训练后期“学习乏力”，过早地停止更新，即使它还没有达到最优状态。</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.8286em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
  c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
  c340,-704.7,510.7,-1060.3,512,-1067
  l0 -0
  c4.7,-7.3,11,-11,19,-11
  H40000v40H1012.3
  s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
  c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
  s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
  c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
  M1001 80h400000v40h-400000z"/></svg>)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>RMSProp</strong></p>
<ul>
<li><strong>改进</strong>：将 AdaGrad 的“累加和”改为了<strong>指数移动平均（即二阶动量）</strong>。<ol>
<li><p><strong>更新二阶动量</strong>：不再是简单相加，而是加权平均。</p>
<p> $$<br> V_t^{(i)} &#x3D; \beta_2 \cdot V_{t-1}^{(i)} + (1 - \beta_2) \cdot (g_t^{(i)})^2<br> $$</p>
<p> 其中，衰减率 β2 是一个接近 1 的值（如0.999），它决定了历史信息的“遗忘速度”。</p>
</li>
<li><p><strong>更新参数</strong>：更新规则的形式与 AdaGrad 完全相同。</p>
<p> [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
 l0 -0
 c4,-6.7,10,-10,18,-10 H400000v40
 H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
 s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
 c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
 c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
 c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
 c53.7,-170.3,84.5,-266.8,92.5,-289.5z
 M1001 80h400000v40h-400000z"/></svg>)</p>
</li>
</ol>
</li>
</ul>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
  l0 -0
  c4,-6.7,10,-10,18,-10 H400000v40
  H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
  s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
  c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
  c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
  c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
  c53.7,-170.3,84.5,-266.8,92.5,-289.5z
  M1001 80h400000v40h-400000z"/></svg>)</p>
<ul>
<li><strong>优点</strong>：<ul>
<li>由于 Vt 是一个移动平均值，它不再会无限增长。如果最近的梯度变小了， Vt 也会随之减小，从而让学习率有机会“回升”。这使得 RMSProp 非常鲁棒，在各种任务上都表现良好。</li>
</ul>
</li>
<li><strong>潜在不足</strong>：<ul>
<li>它只解决了自适应学习率的问题（二阶动量），但没有集成 SGD with Momentum 那样的“惯性”（一阶动量）。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Adam (Adaptive Moment Estimation)</strong></p>
<ul>
<li><p><strong>改进</strong>：<strong>集大成者</strong>。既然一阶动量（如 Momentum）能帮助我们确定更好的更新方向，而二阶动量（如 RMSProp）能为每个参数找到合适的学习率，为什么不把它们都用上呢？它将 <strong>SGDM 的一阶动量</strong>（控制方向）和 <strong>RMSProp 的二阶动量</strong>（控制自适应学习率）完美结合。</p>
</li>
<li><p><strong>实现机制</strong>：<br>  Adam 为每个参数维护了<strong>两个</strong>移动平均值：</p>
<ol>
<li><p><strong>一阶动量 mt (梯度的均值)</strong>：</p>
<p> $$<br> m_t^{(i)} &#x3D; \beta_1 \cdot m_{t-1}^{(i)} + (1 - \beta_1) \cdot g_t^{(i)}<br> $$</p>
</li>
<li><p><strong>二阶动量 Vt (梯度平方的均值)</strong>：</p>
<p> $$<br> V_t^{(i)} &#x3D; \beta_2 \cdot V_{t-1}^{(i)} + (1 - \beta_2) \cdot (g_t^{(i)})^2<br> $$</p>
</li>
</ol>
</li>
<li><p><strong>独有设计：偏差修正 (Bias Correction)</strong></p>
<ul>
<li><p><strong>问题</strong>：mt 和 Vt 都是从 0 开始初始化的。在训练初期，由于 β1 和 β2 都很接近 1，会导致 mt 和 Vt 的值系统性地<strong>偏向于 0</strong>。</p>
</li>
<li><p><strong>修正</strong>：为了解决这个冷启动问题，Adam 在使用它们之前，先对其进行修正：</p>
<p>  $$<br>  \hat{m}_t^{(i)} &#x3D; \frac{m_t^{(i)}}{1 - \beta_1^t}<br>  $$</p>
<p>  $$<br>  \hat{V}_t^{(i)} &#x3D; \frac{V_t^{(i)}}{1 - \beta_2^t}<br>  $$</p>
<p>  在训练初期（t 很小），分母会显著地放大 mt 和 Vt 的值，以纠正偏差。随着训练的进行（t 变大），分母会趋近于 1，修正作用随之消失。</p>
</li>
</ul>
</li>
<li><p><strong>最终更新规则</strong>：</p>
<p>  $$<br>  w_{t+1}^{(i)} &#x3D; w_t^{(i)} - \frac{\alpha}{\sqrt{\hat{V}_t^{(i)} + \epsilon}} \cdot \hat{m}_t^{(i)}<br>  $$</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.88em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
  l0 -0
  c4,-6.7,10,-10,18,-10 H400000v40
  H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
  s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
  c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
  c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
  c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
  c53.7,-170.3,84.5,-266.8,92.5,-289.5z
  M1001 80h400000v40h-400000z"/></svg>)</p>
<p>  这里，参数更新的<strong>方向</strong>由修正后的一阶动量 m^t 提供，而每一步的<strong>学习率</strong>由修正后的二阶动量 V^t 进行<strong>自适应地、逐参数地</strong>调节。</p>
</li>
</ul>
<p>  <strong>总结</strong>：这条演进之路清晰地展示了研究者们如何不断发现问题并解决问题：<br>  <strong>AdaGrad</strong> 提出了自适应思想但有硬伤 -&gt; <strong>RMSProp</strong> 修复了硬伤但不够全面 -&gt; <strong>Adam</strong> 融合了 Momentum 和 RMSProp 的优点并加入了偏差修正，最终成为了一个极其强大和通用的优化器。</p>
</li>
</ul>
<h3 id="4-高级优化器：面向工业界的实战选择"><a href="#4-高级优化器：面向工业界的实战选择" class="headerlink" title="4. 高级优化器：面向工业界的实战选择"></a><strong>4. 高级优化器：面向工业界的实战选择</strong></h3><ul>
<li><p><strong>AdamW (Adam with Decoupled Weight Decay)</strong></p>
<p>  要深刻理解 AdamW，我们必须先厘清一个经常被混淆的概念：<strong>L2 正则化 (L2 Regularization)</strong> 和 <strong>权重衰减 (Weight Decay)</strong>。</p>
<ul>
<li>在 SGD 这样的简单优化器中，这两者在数学上是等价的。</li>
<li>但在 Adam 这样的自适应优化器中，它们的效果截然不同，而这个差异正是 AdamW 诞生的原因。</li>
</ul>
<p>  <strong>1. 问题：Adam + L2 正则化错在哪里？</strong></p>
<ul>
<li><p><strong>标准 L2 正则化的做法</strong>：将 L2 惩罚项 λ&#x2F;2 ∥w∥2 加入到损失函数中。这会导致反向传播时，梯度中增加了一项 λw。</p>
<p>  $$<br>  g_{total}&#x3D;g_{loss}+λw<br>  $$</p>
</li>
<li><p><strong>Adam 的处理方式</strong>：Adam 会将这个<strong>包含了正则项的完整梯度</strong> gtotal，送入它的一阶和二阶动量计算中。我们重点看二阶动量 Vt：</p>
<p>  $$<br>  V_t &#x3D; \beta_2 V_{t-1} + (1 - \beta_2)(g_{loss} + \lambda w)^2<br>  $$</p>
<p>  最终的参数更新步长大致为：</p>
<p>  $$<br>  \Delta w \propto \frac{\alpha}{\sqrt{V_t}} \cdot m_t<br>  $$</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
  c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
  c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
  c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
  s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
  c69,-144,104.5,-217.7,106.5,-221
  l0 -0
  c5.3,-9.3,12,-14,20,-14
  H400000v40H845.2724
  s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
  c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
  M834 80h400000v40h-400000z"/></svg>)</p>
</li>
<li><p><strong>矛盾出现</strong>：</p>
<ul>
<li><p>L2 正则化的初衷是：<strong>权重 w 越大，惩罚（衰减）就应该越厉害</strong>，以防止过拟合。</p>
</li>
<li><p>但在 Adam 中，对于那些本身梯度很大（gloss 很大）或者权重很大（λw 很大）的参数，它们的二阶动量 Vt 也会变得很大。</p>
</li>
<li><p>这导致<strong>分母 Vt 变大</strong>，反而<strong>削弱了</strong>对这些重要参数的学习率和正则化效果。</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
  c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
  c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
  c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
  s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
  c69,-144,104.5,-217.7,106.5,-221
  l0 -0
  c5.3,-9.3,12,-14,20,-14
  H400000v40H845.2724
  s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
  c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
  M834 80h400000v40h-400000z"/></svg>)</p>
</li>
<li><p><strong>结论</strong>：Adam 的自适应学习率机制与 L2 正则项发生了“耦合”，扭曲了 L2 正则化原本的意图，使其效果大打折扣。</p>
</li>
</ul>
</li>
</ul>
<p>  <strong>2. AdamW 的解决方案：解耦 (Decouple)</strong></p>
<p>  AdamW 的思想是返璞归真，让正则化和梯度优化“分道扬镳”。</p>
<ul>
<li><p><strong>第一步：纯粹的梯度优化</strong></p>
<ul>
<li><p>在计算一阶动量 mt 和二阶动量 Vt 时，<strong>只使用原始的损失梯度 gloss</strong>。这样，mt 和 Vt 就纯粹地反映了损失函数本身的几何特性。</p>
</li>
<li><p>计算出不含正则项的“Adam 更新量”：</p>
<p>  $$<br>  \text{Adam_update} &#x3D; \frac{\alpha}{\sqrt{\hat{V}_t} + \epsilon} \cdot \hat{m}_t<br>  $$</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
  c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
  c340,-704.7,510.7,-1060.3,512,-1067
  l0 -0
  c4.7,-7.3,11,-11,19,-11
  H40000v40H1012.3
  s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
  c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
  s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
  c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
  M1001 80h400000v40h-400000z"/></svg>)</p>
</li>
</ul>
</li>
<li><p><strong>第二步：独立的权重衰减</strong></p>
<ul>
<li><p>在参数更新的最后，<strong>直接减去一个与权重大小成正比的衰减项</strong>。</p>
</li>
<li><p>最终更新规则：</p>
<p>  $$<br>  w_{t+1} &#x3D; w_t - \text{Adam_update} - \alpha \cdot \lambda’ \cdot w_t<br>  $$</p>
<p>  (这里的 λ′ 是权重衰减系数，注意这个衰减项<strong>没有被 Vt 除</strong>！)</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
  c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
  c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
  c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
  s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
  c69,-144,104.5,-217.7,106.5,-221
  l0 -0
  c5.3,-9.3,12,-14,20,-14
  H400000v40H845.2724
  s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
  c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
  M834 80h400000v40h-400000z"/></svg>)</p>
</li>
</ul>
</li>
<li><p><strong>效果与应用</strong>：</p>
<ul>
<li>AdamW 的权重衰减效果更稳定、更符合直觉。它能带来更好的泛化性能和更低的训练损失。</li>
<li>由于其出色的表现，AdamW 已成为训练 <strong>Transformer</strong>、<strong>BERT</strong>、<strong>GPT</strong> 等大型语言模型的<strong>事实标准</strong>，是现代深度学习工具箱中的必备选项。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>FTRL (Follow-The-Regularized-Leader)</strong></p>
<p>  FTRL 的设计哲学和应用场景与 Adam 完全不同，它是<strong>在线学习 (Online Learning)</strong> 和<strong>广告点击率 (CTR) 预估</strong>领域的王者。</p>
<p>  <strong>1. 核心动机与场景</strong></p>
<ul>
<li><strong>场景</strong>：在推荐和广告系统中，特征空间是<strong>亿级别、甚至百亿级别的</strong>，并且是<strong>极度稀疏</strong>的。例如，一个用户的特征由他的ID、他正在看的商品ID、广告ID等组成，这些ID绝大部分都不会在下一个样本中出现。</li>
<li><strong>需求</strong>：<ol>
<li>模型必须非常小，以便于快速部署和线上<strong>低延迟</strong>推理。</li>
<li>模型必须能高效地处理稀疏特征。</li>
</ol>
</li>
<li><strong>核心目标</strong>：训练出一个<strong>高度稀疏</strong>的模型，即模型的大部分权重都<strong>等于零</strong>。</li>
</ul>
<p>  <strong>2. 实现机制：带 L1 正则化的“懒惰”更新</strong></p>
<p>  FTRL 的核心在于它独特的、逐坐标的更新法则，其中巧妙地融入了 L1 正则化，以强制产生稀疏性。</p>
<p>  我们可以这样理解它的更新步骤：</p>
<ol>
<li><strong>累积信息</strong>：和 AdaGrad 类似，FTRL 为每个参数 w(i) 维护两个累加器：<ul>
<li>zt(i): 历史梯度之和。</li>
<li>Vt(i): 历史梯度平方之和（用于自适应学习率）。</li>
</ul>
</li>
<li><strong>“懒惰”的更新决策（稀疏性的来源）</strong>：<br>  在决定是否更新 w(i) 之前，FTRL会先做一个判断：<ul>
<li><strong>如果累积的梯度信号 ∣zt(i)∣ 还不足以“战胜”L1 正则化的惩罚力度 λ1</strong>，即 ∣zt(i)∣&lt;λ1：<ul>
<li><strong>决策</strong>：FTRL 认为这个特征的信号太弱，不值得为其分配非零权重。于是，<strong>直接将该参数设为 0</strong>。</li>
</ul>
</li>
<li><strong>如果累积的梯度信号足够强</strong>，即 ∣zt(i)∣≥λ1：<ul>
<li><strong>决策</strong>：此时才认为该特征是有效的，并根据“超出” L1 惩罚的那部分梯度，以及自适应学习率，计算出一个<strong>非零</strong>的权重值。</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li><strong>效果与应用</strong>：<ul>
<li><strong>极致稀疏</strong>：通过这种“阈值过滤”机制，FTRL 能将大量不重要、信号不强的特征权重直接剪枝为零，产出的模型非常小巧。</li>
<li><strong>工业标准</strong>：在处理大规模类别特征的 LR、FM 这类模型上，FTRL 是当之无愧的工业标准优化器，广泛应用于各大互联网公司的广告和推荐系统的排序阶段，尤其是 <strong>Wide &amp; Deep 模型中的 Wide 部分</strong>。</li>
</ul>
</li>
</ul>
<p>  <strong>AdamW vs. FTRL 面试总结</strong></p>
<table>
<thead>
<tr>
<th>对比维度</th>
<th>AdamW</th>
<th>FTRL</th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心优势</strong></td>
<td>优秀的泛化能力，稳定的正则化效果</td>
<td>产出<strong>高度稀疏</strong>的模型，内存占用小，推理速度快</td>
</tr>
<tr>
<td><strong>处理对象</strong></td>
<td><strong>稠密模型</strong>，如深度神经网络、Transformer</td>
<td><strong>稀疏模型</strong>，如带海量ID特征的线性模型（LR&#x2F;FM）</td>
</tr>
<tr>
<td><strong>应用场景</strong></td>
<td>训练大语言模型、计算机视觉、推荐系统的<strong>Deep部分</strong></td>
<td>广告&#x2F;推荐系统的CTR预估、在线学习、推荐系统的<strong>Wide部分</strong></td>
</tr>
<tr>
<td><strong>一句话总结</strong></td>
<td>训练复杂<strong>深度模型</strong>的“瑞士军刀”</td>
<td>处理海量<strong>稀疏特征</strong>、追求极致效率的“手术刀”</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="5-学习率调度：掌握训练的“节奏”"><a href="#5-学习率调度：掌握训练的“节奏”" class="headerlink" title="5. 学习率调度：掌握训练的“节奏”"></a><strong>5. 学习率调度：掌握训练的“节奏”</strong></h3><ul>
<li><strong>问题</strong>：单一的、固定的学习率无法适应整个训练过程的需求。</li>
<li><strong>常用策略</strong>：<strong>Warmup + Decay</strong><ul>
<li><strong>Warmup (热身)</strong>：在训练初期，使用一个很小的学习率，并逐步<strong>线性增加</strong>到预设的目标值。这可以防止模型在初始权重混乱时因步子太大而“崩溃”。</li>
<li><strong>Decay (衰减)</strong>：在 Warmup 结束后，让学习率随着训练的进行而<strong>逐渐降低</strong>。这有助于模型在接近最优解时，能以更小的步长进行精细探索，从而收敛得更好。</li>
<li><strong>组合推荐</strong>：<strong>“线性 Warmup + 余弦退火 (Cosine Annealing) Decay”</strong> 是一个非常鲁棒且流行的现代训练策略。</li>
</ul>
</li>
</ul>
<h2 id="III-文本表示的进化"><a href="#III-文本表示的进化" class="headerlink" title="III. 文本表示的进化"></a><strong>III. 文本表示的进化</strong></h2><h3 id="1-统计时代：TF-IDF-—-“以数取胜”的关键词大师"><a href="#1-统计时代：TF-IDF-—-“以数取胜”的关键词大师" class="headerlink" title="1. 统计时代：TF-IDF — “以数取胜”的关键词大师"></a><strong>1. 统计时代：TF-IDF — “以数取胜”的关键词大师</strong></h3><p>在深度学习普及之前，我们用巧妙的统计学方法来衡量词的重要性。其中，TF-IDF 是最经典、最有效的代表。</p>
<ul>
<li><strong>核心思想</strong>：一个词的重要性，由它在**本文档内的频率（TF）<strong>和在</strong>整个语料库中的稀有度（IDF）**共同决定。</li>
<li><strong>TF (Term Frequency, 词频)</strong><ul>
<li><strong>是什么</strong>：一个词在本篇文章中出现的频率。</li>
<li><strong>计算</strong>：<code>TF = (某词在文章中出现的次数) / (文章总词数)</code></li>
<li><strong>直觉</strong>：在一篇关于“自动驾驶”的论文中，“传感器”这个词的 TF 值会很高。但这有个问题：像“的”、“是”这种常用词的 TF 值也会很高。</li>
</ul>
</li>
<li><strong>IDF (Inverse Document Frequency, 逆文档频率)</strong><ul>
<li><strong>是什么</strong>：对 TF 值进行修正的“惩罚”项，用来衡量一个词的“信息量”或“稀有度”。</li>
<li><strong>计算</strong>：<code>IDF = log(总文档数 / (包含该词的文档数 + 1))</code></li>
<li><strong>直觉</strong>：像“的”、“是”这种词，几乎所有文档都包含，分母会很大，因此它们的 IDF 值会极其接近 0。而“自动驾驶”这种专业术语，只在少数文档中出现，其 IDF 值就会很高。</li>
</ul>
</li>
<li><strong>最终形态：TF-IDF</strong><ul>
<li><strong>计算</strong>：<code>TF-IDF = TF × IDF</code></li>
<li><strong>效果</strong>：只有那些<strong>在本文档中频繁出现（高TF），但在别的文档中很少见（高IDF）的词，才能获得很高的 TF-IDF 分数。这使得它成为一个极其出色的关键词提取</strong>算法。</li>
<li><strong>局限性</strong>：<ul>
<li><strong>语义缺失</strong>：它完全不理解词语的含义。在 TF-IDF 的世界里，“国王”和“女王”是两个毫无关系的词。</li>
<li><strong>稀疏性</strong>：它为每个文档生成一个维度等于整个词典大小的向量，其中绝大部分都是零，非常稀疏。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-静态向量时代：Word2Vec-—-“物以类聚”的语义先锋"><a href="#2-静态向量时代：Word2Vec-—-“物以类聚”的语义先锋" class="headerlink" title="2. 静态向量时代：Word2Vec — “物以类聚”的语义先锋"></a><strong>2. 静态向量时代：Word2Vec — “物以类聚”的语义先锋</strong></h3><p>为了解决 TF-IDF 无法理解语义的问题，神经网络方法应运而生，其核心思想是<strong>分布式假设 (Distributional Hypothesis)</strong>：“一个词的含义，由其上下文决定”。</p>
<ul>
<li><strong>核心思想</strong>：<br>不再统计词频，而是通过一个神经网络，为词典中的每一个词学习一个<strong>稠密的、低维的向量（即 Embedding）</strong>。在这个向量空间中，意思相近的词，其向量距离也相近。</li>
<li><strong>两种经典训练范式</strong>：<ol>
<li><strong>CBOW (Continuous Bag-of-Words)</strong>：<strong>上下文 -&gt; 中心词</strong>。<ul>
<li><strong>任务</strong>：“完形填空”。模型看到 <code>[深度, __, 网络]</code>，然后去预测中间的词是“学习”。</li>
<li><strong>特点</strong>：训练速度快，对高频词效果好。</li>
</ul>
</li>
<li><strong>Skip-gram</strong>：<strong>中心词 -&gt; 上下文</strong>。<ul>
<li><strong>任务</strong>：“头脑风暴”。模型看到“学习”，然后去预测它周围可能出现的词是 <code>[深度, __, 网络]</code>。</li>
<li><strong>特点</strong>：训练速度慢一些，但对低频词和生僻词的学习效果更好。</li>
</ul>
</li>
</ol>
</li>
<li><strong>在推荐系统中的应用：Item2Vec</strong><ul>
<li>这是一个绝妙的类比：将<strong>用户的行为序列（如点击、购买）看作一个“句子”</strong>，将<strong>商品ID看作“词语”</strong>。</li>
<li>通过 Skip-gram 模型训练这些“句子”，我们就能得到每个商品的 Embedding。</li>
<li><strong>效果</strong>：经常被一同购买或浏览的商品（如“手机壳”和“手机膜”），它们的 Embedding 就会非常接近。这是构建“相关推荐”、“猜你喜欢”等模块的强大基石。</li>
</ul>
</li>
<li><strong>致命缺陷：无法解决多义词 (Polysemy)</strong><ul>
<li>Word2Vec 为每个词生成一个<strong>全局唯一、固定不变</strong>的静态向量。</li>
<li><strong>经典例子</strong>：<ol>
<li>我把钱存进了<strong>银行 (bank)</strong>。 (金融机构)</li>
<li>我坐在<strong>河岸 (bank)</strong> 上。 (地理位置)</li>
</ol>
</li>
<li>对于 Word2Vec，这两个 <code>bank</code> 的向量是<strong>一模一样的</strong>。它无法根据上下文动态地调整词的表示，这是其最大的局限性。</li>
</ul>
</li>
</ul>
<h3 id="3-动态向量时代：BERT-—-“千人千面”的语境大师"><a href="#3-动态向量时代：BERT-—-“千人千面”的语境大师" class="headerlink" title="3. 动态向量时代：BERT — “千人千面”的语境大师"></a><strong>3. 动态向量时代：BERT — “千人千面”的语境大师</strong></h3><p>BERT 的出现，标志着 NLP 进入了一个新纪元，其核心是提供<strong>上下文相关的动态词向量 (Contextualized Word Embeddings)</strong>。</p>
<ul>
<li><strong>核心思想</strong>：一个词没有固定的含义，它的意义完全由其所在的上下文语境所定义。</li>
<li><strong>如何实现？</strong><ol>
<li><strong>强大的骨架：Transformer Encoder</strong>。我们已经深入讨论过，其核心的<strong>自注意力机制</strong>，允许输入序列中的每一个词都能“看到”并与所有其他词进行信息交互。</li>
<li><strong>动态计算</strong>：BERT <strong>不会预先存好每个词的向量</strong>。当一个句子输入后，句子中的每个词的初始 Embedding 会流经 BERT 的多层（如12层）Transformer。在<strong>每一层</strong>，每个词的向量都会根据<strong>整个句子</strong>的信息被重新计算和优化。</li>
<li><strong>最终输出</strong>：从 BERT 的最后一层输出的，才是这个词在<strong>当前特定语境下</strong>的最终向量表示。</li>
</ol>
</li>
<li><strong>解决多义词问题</strong>：<ul>
<li>当处理 “I went to the <strong>bank</strong>“ 时，<code>bank</code> 的向量会因为它看到了 <code>went to</code> 等信息，而被塑造成带有“金融”含义的向量。</li>
<li>当处理 “The river <strong>bank</strong>“ 时，<code>bank</code> 的向量会因为它看到了 <code>river</code>，而被塑造成带有“地理”含义的向量。</li>
<li>这两个 <code>bank</code> 的最终输出向量在向量空间中会相距很远，从而完美解决了多义词问题。</li>
</ul>
</li>
</ul>
<p><strong>总结</strong>：从 TF-IDF 的词频统计，到 Word2Vec 的静态语义向量，再到 BERT 的动态上下文向量，我们看到机器对文本的理解，从<strong>表层统计</strong>，发展到<strong>固定语义</strong>，最终达到了<strong>深度、动态的语境理解</strong>。这条进化路径是所有现代搜广推应用的基础。</p>
<h2 id="IV-Transformer-与-BERT-模型深度剖析"><a href="#IV-Transformer-与-BERT-模型深度剖析" class="headerlink" title="IV. Transformer 与 BERT 模型深度剖析"></a><strong>IV. Transformer 与 BERT 模型深度剖析</strong></h2><h3 id="1-Transformer-基石：自注意力机制-Self-Attention"><a href="#1-Transformer-基石：自注意力机制-Self-Attention" class="headerlink" title="1. Transformer 基石：自注意力机制 (Self-Attention)"></a><strong>1. Transformer 基石：自注意力机制 (Self-Attention)</strong></h3><p>[](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg>)</p>
<p>[](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg>)</p>
<p>自注意力机制是 Transformer 的心脏。它的核心思想是：一个序列中的每个元素，其新的表示，应该由序列中<strong>所有元素</strong>根据相关性进行加权求和得到。</p>
<p><strong>1.1 QKV 矩阵的角色分工与生成方式</strong></p>
<p>为了实现上述思想，自注意力机制为每个输入向量 <code>x</code> 创造了三个不同的身份，这个过程是通过三个独立、可学习的线性变换矩阵 WQ,WK,WV 完成的：</p>
<ul>
<li><strong>Query (Q, 查询)</strong>: Q&#x3D;X⋅WQ<ul>
<li><strong>角色</strong>：代表当前词为了更好地理解自己，主动发出的一个“<strong>查询</strong>”或“<strong>问题</strong>”。它在问：“为了理解我，我应该关注序列中的哪些信息？”</li>
</ul>
</li>
<li><strong>Key (K, 键)</strong>: K&#x3D;X⋅WK<ul>
<li><strong>角色</strong>：代表序列中每个词所拥有的一个“<strong>索引</strong>”或“<strong>标签</strong>”，用来被别人查询。它在说：“这是我的身份标签，你们可以根据这个标签来判断我是否与你们相关。”</li>
</ul>
</li>
<li><strong>Value (V, 值)</strong>: V&#x3D;X⋅WV<ul>
<li><strong>角色</strong>：代表序列中每个词实际蕴含的**“内容”或“信息”**。它在说：“如果你们关注我，这就是我能提供给你们的丰富信息。”</li>
</ul>
</li>
</ul>
<p><strong>计算流程：</strong></p>
<ol>
<li><strong>匹配打分</strong>：用每个词的 Query 向量，去和所有词的 Key 向量进行点积运算 (QKT)，得到一个注意力分数矩阵。这个分数代表了“查询”与“索引”之间的匹配程度。</li>
<li><strong>加权求和</strong>：将分数通过 Softmax 归一化为权重，然后用这个权重去对所有词的 Value 向量进行加权求和。</li>
</ol>
<p><strong>Encoder vs Decoder 中的区别</strong>:</p>
<ul>
<li><strong>Encoder 中 (Self-Attention)</strong>：Q, K, V 全部来自于<strong>上一层 Encoder 的输出</strong>。它是在对输入序列自身进行信息重组，目的是为了深刻理解输入。</li>
<li><strong>Decoder 中 (包含两种 Attention)</strong>：<ol>
<li><strong>Masked Self-Attention</strong>: Q, K, V 全部来自于<strong>上一层 Decoder 的输出</strong>。目的是让已生成的部分序列理解自身，为生成下一个词做准备。</li>
<li><strong>Cross-Attention</strong>: <strong>Q 来自于 Decoder</strong> (上一步 Masked Self-Attention 的输出)，而 <strong>K 和 V 来自于 Encoder 的最终输出</strong>。这是让 Decoder 在生成新内容时，能够查询和参考完整的原始输入信息的关键桥梁。</li>
</ol>
</li>
</ul>
<p><strong>1.2 位置编码 (Positional Encoding): 弥补顺序信息的缺失</strong></p>
<ul>
<li><strong>问题</strong>：自注意力机制本身是“无序”的，它平等地看待序列中的每一个词，无法感知它们的先后顺序。</li>
<li><strong>解决方案</strong>：在模型的输入层，将每个词的 Embedding 与一个代表其<strong>绝对位置</strong>的“位置向量”相加，强行把顺序信息注入模型。</li>
<li><strong>两种主流方式</strong>：<ol>
<li><strong>固定式 (Sinusoidal PE)</strong>：原始 Transformer 使用的方式。它通过不同频率的 <code>sin</code> 和 <code>cos</code> 函数生成一个固定的、不参与训练的位置向量。其优点是能够处理比训练时更长的序列，并且其周期性使得模型能轻易学习到相对位置关系。</li>
<li><strong>可学习式 (Learnable PE)</strong>：BERT 等模型使用的方式。它创建一个位置 Embedding 矩阵（就像一个词表一样），让模型在训练过程中自己去学习每个位置最合适的向量表示。优点是更灵活、更能适应特定任务，但通常有最大长度限制。</li>
</ol>
</li>
</ul>
<p><strong>1.3 注意力掩码 (Attention Mask): 控制信息的流动</strong></p>
<ul>
<li><strong>原理</strong>：在计算好的注意力分数矩阵上，在送入 Softmax 之前，将需要被屏蔽的位置的分数<strong>设置为一个巨大的负数</strong>（如 -1e9）。这样，经过 Softmax 后，这些位置的注意力权重就会变成 0。</li>
<li><strong>两种核心掩码</strong>：<ol>
<li><strong>Padding Mask (填充掩码)</strong>：<ul>
<li><strong>应用场景</strong>：Encoder 和 Decoder。</li>
<li><strong>目的</strong>：在处理一个批次的变长序列时，我们需要用 <code>&lt;pad&gt;</code> 标记将短序列补齐。此掩码的作用就是告诉模型，在计算注意力时，<strong>完全忽略这些无意义的 <code>&lt;pad&gt;</code> 标记</strong>。</li>
</ul>
</li>
<li><strong>Look-ahead Mask (前瞻&#x2F;因果掩码)</strong>：<ul>
<li><strong>应用场景</strong>：<strong>仅用于 Decoder</strong>。</li>
<li><strong>目的</strong>：在生成任务中，为了保证模型的自回归特性，在预测第 <code>i</code> 个词时，<strong>绝对不能让它“偷看”到第 <code>i</code> 个词之后的信息</strong>。</li>
<li><strong>实现</strong>：通过一个上三角矩阵，将所有“未来”位置的注意力分数全部屏蔽掉。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><strong>1.4 注意力缩放因子根号 dk 的作用</strong></p>
<p>[](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg>)</p>
<ul>
<li><p><strong>问题</strong>：在计算 Q 和 K 的点积时，如果向量维度 dk 很大（例如 512），点积的结果的方差也会变得很大。</p>
</li>
<li><p><strong>危害</strong>：过大的点积结果会把 Softmax 函数推向其<strong>饱和区</strong>，导致梯度变得极小，从而引发<strong>梯度消失</strong>，使训练过程不稳定甚至失败。</p>
</li>
<li><p><strong>解决方案</strong>：将计算出的点积分数<strong>除以根号 dk</strong>。</p>
<p>  [](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
  c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
  c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
  c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
  s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
  c69,-144,104.5,-217.7,106.5,-221
  l0 -0
  c5.3,-9.3,12,-14,20,-14
  H400000v40H845.2724
  s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
  c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
  M834 80h400000v40h-400000z"/></svg>)</p>
</li>
<li><p><strong>数学原理</strong>：原论文作者证明，假设 Q 和 K 的元素是均值为 0、方差为 1 的独立随机变量，那么它们的点积结果的方差就是 dk。因此，除以 <strong>根号</strong>dk 这一步，恰好能将点积结果的方差重新<strong>归一化为 1</strong>，从而保证了送入 Softmax 的数值在一个合理的、梯度稳定的范围内。这是一个至关重要的<strong>稳定化技巧</strong>。</p>
</li>
</ul>
<h3 id="2-BERT-模型解析"><a href="#2-BERT-模型解析" class="headerlink" title="2. BERT 模型解析"></a>2. BERT 模型解析</h3><p>[](data:image&#x2F;svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg>)</p>
<p>BERT (Bidirectional Encoder Representations from Transformers) 的发布是 NLP 领域的里程碑事件。它的核心贡献在于，通过巧妙的预训练任务，第一次实现了<strong>深度的、无监督的双向语言表示</strong>学习。</p>
<p><strong>2.1 预训练任务：MLM 与 NSP 的原理、缺陷及改进</strong></p>
<p>BERT 的强大能力，源于它在海量文本上进行的两项“自我修炼”任务。</p>
<ul>
<li><strong>MLM (Masked Language Model &#x2F; 掩码语言模型)</strong><ul>
<li><strong>原理</strong>：这是 BERT 的<strong>核心创新</strong>。它采用“完形填空”的方式进行训练。<ol>
<li>随机选取输入序列中 15% 的 Token。</li>
<li>在这 15% 的 Token 中，80% 的情况用一个特殊的 <code>[MASK]</code> 标记替换掉，10% 的情况用一个随机的词替换，10% 的情况保持原样。</li>
<li>模型的目标，是<strong>仅根据未被遮盖的上下文，准确预测出这些被遮盖位置的原始 Token 是什么</strong>。</li>
</ol>
</li>
<li><strong>意义（为什么是双向的？）</strong>：与只能看到左侧文本的 GPT 不同，MLM 任务天然地强迫模型必须<strong>同时利用被遮盖位置的左侧和右侧的全部上下文信息</strong>才能做出正确的预测。这使得 BERT 能够学习到真正意义上的<strong>深层双向语境表示</strong>。</li>
<li><strong>缺陷与改进</strong>：<ul>
<li><strong>独立性假设问题</strong>：BERT 在预测多个 <code>[MASK]</code> 时是相互独立的，无法利用被预测词之间的关系。</li>
<li><strong>改进 1: WWM (Whole Word Masking)</strong>：对策是，如果一个词被 WordPiece 分割成多个子词（如 <code>playing</code> -&gt; <code>[&#39;play&#39;, &#39;##ing&#39;]</code>），那么就将这个完整单词对应的<strong>所有子词一同进行 Mask</strong>。这增加了任务难度，也更符合语言逻辑。</li>
<li><strong>改进 2: SpanBERT</strong>：更进一步，不再 Mask 零散的词，而是 <strong>Mask 一个连续的文本片段 (Span)</strong>。模型需要利用片段<strong>两端边界</strong>的信息来预测整个片段的内容，这促使模型学习更强的短语级语义关系。</li>
</ul>
</li>
</ul>
</li>
<li><strong>NSP (Next Sentence Prediction &#x2F; 下一句预测)</strong><ul>
<li><strong>原理</strong>：这是一个二分类任务，用于学习<strong>句子间的关系</strong>。<ol>
<li>模型接收一对句子 (A, B)。</li>
<li>50% 的情况下，B 是 A 在原文中真实的下一句（正样本）。</li>
<li>50% 的情况下，B 是从其他文档中随机抽取的一个句子（负样本）。</li>
<li>模型需要预测 B 是否是 A 的下一句。</li>
</ol>
</li>
<li><strong>缺陷</strong>：后来的研究发现，这个任务存在“捷径”。由于负样本来自完全不同的文档，模型很可能仅通过判断<strong>两个句子的主题是否相关</strong>就能做出正确判断，而没有真正学到更深层次的<strong>逻辑连贯性</strong>。</li>
<li><strong>改进</strong>：<ul>
<li><strong>RoBERTa</strong> 发现此任务弊大于利，<strong>直接将其废除</strong>，仅使用 MLM 在更长的连续文本上训练，效果反而更好。</li>
<li><strong>ALBERT</strong> 提出了 <strong>SOP (Sentence Order Prediction)</strong> 任务。它取同一文档中的两个连续句子，50% 保持原序，50% 调换顺序。这排除了主题相关性的干扰，强迫模型去学习更细致的<strong>语篇连贯性</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>2.2 BERT 的输入构成：三位一体的 Embedding</strong></p>
<p>为了让模型同时理解“词义”、“位置”和“句间关系”，BERT 的每个输入 Token 的最终向量表示，是由<strong>三种 Embedding 按元素相加</strong>而成的。</p>
<ul>
<li><strong>Token Embeddings (词元嵌入)</strong>：最基础的部分，代表了每个词或子词（WordPiece）本身的语义。</li>
<li><strong>Segment Embeddings (分段嵌入)</strong>：用于区分两个不同的输入句子。所有属于第一句话的 Token，其 Segment Embedding 都是同一个向量（通常是 EA）；所有属于第二句话的 Token，其 Segment Embedding 都是另一个向量（EB）。这是为 NSP 任务量身设计的。</li>
<li><strong>Position Embeddings (位置嵌入)</strong>：我们讨论过的<strong>可学习的</strong>位置向量。每个位置（0, 1, 2, …, 511）都有一个独立的、在训练中不断优化的向量，来为模型提供顺序信息。</li>
</ul>
<p><code>最终输入 = Token Embedding + Segment Embedding + Position Embedding</code></p>
<p><strong>2.3 [CLS] 标记：序列表示的“代言人”</strong></p>
<ul>
<li><strong>是什么</strong>：一个特殊的标记，永远被放在输入序列的<strong>最开头</strong>。</li>
<li><strong>工作原理</strong>：<br><code>[CLS]</code> 本身没有语义，但因为它和其他所有词一样，参与了 BERT 所有 Encoder 层的<strong>全局自注意力计算</strong>，所以在经过 12 层的信息充分“搅拌”和“融合”后，<code>[CLS]</code> 标记在最后一层对应的<strong>输出向量</strong>，就成了一个<strong>包含了整个输入序列信息的“聚合表示”</strong>。它像一个指定的“会议总结人”，最终产出了代表会议精神的总结报告。</li>
<li><strong>意义与应用</strong>：<ul>
<li><strong>提供固定维度的句子表示</strong>：无论输入句子多长，我们都可以从 <code>[CLS]</code> 位置得到一个固定维度（如 768 维）的向量来代表整个句子。</li>
<li><strong>简化下游任务微调</strong>：在进行句子级别的分类任务时（如情感分析、文本分类），我们只需要：<ol>
<li>取出 BERT 最后一层的 <code>[CLS]</code> 输出向量。</li>
<li>将这个向量接入一个简单的、小型的分类器（如一个全连接层）。</li>
<li>进行微调。此时，<strong>分类任务的梯度会通过 <code>[CLS]</code> 向量反向传播到整个 BERT 模型</strong>，等于在告诉模型：“请调整你所有的参数，把对我的任务最有用的信息，都浓缩到 <code>[CLS]</code> 这个向量里来！”</li>
</ol>
</li>
</ul>
</li>
</ul>
<p>通过这种方式，<code>[CLS]</code> 成为了连接强大的预训练模型和各种具体下游任务的标准、高效的桥梁。</p>
<h3 id="3-Transformer-BERT-的局限与优化"><a href="#3-Transformer-BERT-的局限与优化" class="headerlink" title="3. Transformer &amp; BERT 的局限与优化"></a><strong>3. Transformer &amp; BERT 的局限与优化</strong></h3><p><strong>3.1 注意力机制效率优化</strong></p>
<p>这部分主要关注两个场景：一是<strong>推理 (Inference)</strong> 时的显存和速度优化，二是<strong>训练 (Training)</strong> 时的计算效率优化。</p>
<p><strong>从 MHA 到 GQA 和 MQA 的演进：优化推理中的 KV Cache</strong></p>
<p>在进行自回归生成（即一个词一个词地生成文本）时，为了避免重复计算，模型需要将过去所有 token 的 Key (K) 和 Value (V) 缓存起来，这被称为 <strong>KV Cache</strong>。随着生成文本越来越长，KV Cache 会变得非常巨大，成为推理时的显存瓶颈。</p>
<ul>
<li><strong>MHA (Multi-Head Attention)</strong>：标准的多头注意力。<ul>
<li><strong>机制</strong>：假设有 32 个头，那么就有 32 组独立的 Q, K, V 投影矩阵。</li>
<li><strong>问题</strong>：在推理时，需要缓存 32 组独立的 K 和 V。这导致 KV Cache 非常庞大，不仅消耗大量显存，而且在生成每个新 token 时，从显存中读取巨大的 K 和 V 矩阵也非常耗时。</li>
</ul>
</li>
<li><strong>MQA (Multi-Query Attention)</strong>：一个极致的优化方案。<ul>
<li><strong>核心思想</strong>：“所有头真的需要各自独立的 K 和 V 吗？也许它们可以共享？”</li>
<li><strong>机制</strong>：仍然保留 32 个独立的 Query 头，但让所有这 32 个头<strong>共享同一套 Key 和 Value 投影</strong>。</li>
<li><strong>效果</strong>：KV Cache 的大小被急剧压缩为原来的 1&#x2F;32！这极大地降低了推理时的显存占用，并显著提升了生成速度。</li>
<li><strong>代价</strong>：牺牲了一定的模型性能和容量，因为所有头被迫从同一个“知识库”中提取信息。</li>
</ul>
</li>
<li><strong>GQA (Grouped-Query Attention)</strong>：一个介于 MHA 和 MQA 之间的“黄金平衡点”。<ul>
<li><strong>机制</strong>：将 32 个 Query 头分成若干组，比如 4 组，每组 8 个头。<strong>在同一组内的 8 个头，共享同一套 Key 和 Value</strong>。这样，我们就从 MHA 的 32 组 K&#x2F;V，和 MQA 的 1 组 K&#x2F;V，变成了 4 组 K&#x2F;V。</li>
<li><strong>效果</strong>：它获得了 MQA 大部分的加速和显存节省效果，同时比 MQA 保留了更好的模型质量。GQA 是当前最先进的开源大模型（如 Llama 2&#x2F;3）采用的主流方案。</li>
</ul>
</li>
</ul>
<p><strong>FlashAttention 的 I&#x2F;O 感知优化原理：重塑训练效率</strong></p>
<p>FlashAttention 是一项革命性的算法层面的优化，它在**不改变任何数学结果（即精确注意力）**的前提下，极大地提升了训练和长文本推理的速度。</p>
<ul>
<li><strong>核心洞察</strong>：标准注意力的瓶颈不在于浮点数计算（FLOPs），而在于<strong>内存的读写（I&#x2F;O）</strong>。在计算过程中，需要生成一个巨大的 N×N 的中间矩阵（QKT 的结果），并反复地在 GPU 核心的高速缓存（SRAM）和 GPU 的主显存（HBM）之间进行读写。HBM 带宽远低于 SRAM 的计算速度，这造成了大量的等待时间。</li>
<li><strong>举例</strong>：对于一个不算太长的序列，比如 N &#x3D; 16k (16384)，如果用半精度(2字节)存储，这个矩阵的大小就是 <code>16384 * 16384 * 2 bytes ≈ 512 MB</code>。</li>
<li><strong>瓶颈</strong>：这个 512MB 的矩阵，对于 GPU 核心旁边的高速缓存 (SRAM, 只有几 MB) 来说太巨大了，根本放不下。因此，GPU 必须将这个巨大的矩阵写入速度慢得多的主显存 (HBM)。在整个计算过程中，GPU 核心不得不频繁地暂停计算，去等待这个大矩阵从 HBM 中读出或写入。<strong>计算单元大部分时间都在“等”，而不是在“算”</strong>。这就是所谓的 I&#x2F;O 瓶颈，其读写次数是 O(N2) 的。</li>
<li><strong>FlashAttention 的解决方案</strong>：<ol>
<li><strong>分块计算 (Tiling)</strong>：它不再试图一次性生成整个 N×N 的大矩阵。它将输入的 Q, K, V 矩阵切分成一个个更小的“块 (Tile)”，小到可以完全放进高速的 SRAM 中。</li>
<li><strong>算子融合 (Kernel Fusion)</strong>：FlashAttention 将注意力的多个计算步骤（矩阵乘法、Mask、Softmax、与V相乘等）<strong>融合成了单个 GPU 计算核 (Kernel)</strong>。数据块一旦从慢速的 HBM 载入到高速的 SRAM 后，就会在这个“小作坊”里完成所有的计算，得到最终结果，然后再写回 HBM。</li>
<li><strong>执行流程</strong>：<ul>
<li>将一小块 Q 和一小块 K 从慢速 HBM 加载进高速 SRAM。</li>
<li><strong>在 SRAM 内部</strong>，完成这部分的 QK^T 计算、Mask、Softmax 和与 V 相乘的全部操作。</li>
<li><strong>只将最终计算好的那一小块输出结果</strong>写回到慢速 HBM 中。</li>
<li><strong>关键</strong>：那个巨大的 N×N 中间矩阵，自始至终<strong>从未被完整地创建和写入到慢速 HBM 中</strong>。它只在高速 SRAM 的“小作坊”里昙花一现。</li>
</ul>
</li>
<li><strong>在线 Softmax</strong>：它使用了一种巧妙的数值稳定技巧，可以在不看到全局的情况下，分块地计算出完全正确的 Softmax 结果。</li>
</ol>
</li>
<li><strong>效果</strong>：通过<strong>最大化高效率的片上计算，最小化低效率的内存读写</strong>，FlashAttention 将注意力的实际复杂度从 O(N2) 降低到了接近线性的 O(N)，使得在数万长度的序列上训练 Transformer 成为可能。</li>
</ul>
<p><strong>3.2 长文本处理策略</strong></p>
<p>当序列长度超出了模型本身或硬件的极限时，我们需要一些工程策略来处理。</p>
<ul>
<li><strong>截断&#x2F;分块 (Truncation&#x2F;Segmentation)</strong><ul>
<li><strong>做法</strong>：最简单粗暴的方法。直接将长文本切成固定长度（如 512）的块，分别输入模型，最后再对结果进行聚合（如取平均）。</li>
<li><strong>缺点</strong>：完全丢失了块与块之间的上下文信息。</li>
</ul>
</li>
<li><strong>滑动窗口 (Sliding Window)</strong><ul>
<li><strong>做法</strong>：同样是分块，但在块与块之间设置<strong>重叠区域 (Overlap)</strong>。例如，第一块是 <code>[0..512]</code>，第二块可以是 <code>[384..896]</code>。</li>
<li><strong>优点</strong>：通过重叠部分，模型能够感知到一部分跨块的上下文，效果优于简单截断。</li>
</ul>
</li>
<li><strong>稀疏注意力模型 (Sparse Attention Models)</strong><ul>
<li><strong>做法</strong>：从模型架构层面进行修改，不再让每个 token 都关注所有其他 token。</li>
<li><strong>例子</strong>：<strong>Longformer</strong> 模型使用了<strong>局部窗口注意力</strong>（每个 token 只关注自己附近的一小块区域）和<strong>全局注意力</strong>（少数几个重要的 token 可以被所有 token 关注）的组合。</li>
<li><strong>注意</strong>：这类方法是标准注意力的<strong>一种近似</strong>，会损失部分信息，而 FlashAttention 是对标准注意力的<strong>精确加速</strong>。</li>
</ul>
</li>
</ul>
<p><strong>3.3 QLoRA 能用 FlashAttention 吗？</strong></p>
<p><strong>答案是：不但能，而且是“天作之合”！</strong></p>
<p>在实际应用中，将 QLoRA 和 FlashAttention 结合使用，是当前在有限资源下<strong>最高效地微调长文本大语言模型</strong>的<strong>黄金标准方案</strong>。</p>
<p>它们之所以能完美配合，是因为它们解决了两个<strong>正交的、互补的</strong>问题。</p>
<ul>
<li><strong>QLoRA 的职责：解决“静态存储”问题</strong><ul>
<li><strong>目标</strong>：降低模型<strong>权重参数 (Weights)</strong> 的存储体积。</li>
<li><strong>手段</strong>：通过 4-bit 量化，将一个原本需要 140GB 显存的 70B 模型压缩到约 35GB，使其能够**“装进”** 单张 GPU。</li>
<li><strong>关注点</strong>：模型文件本身的大小。</li>
</ul>
</li>
<li><strong>FlashAttention 的职责：解决“动态计算”问题</strong><ul>
<li><strong>目标</strong>：降低模型在<strong>前向&#x2F;反向传播</strong>过程中，<strong>中间激活值 (Activations)</strong> 产生的内存和 I&#x2F;O 开销。</li>
<li><strong>手段</strong>：通过 I&#x2F;O 感知的算法，避免生成巨大的中间矩阵。</li>
<li><strong>关注点</strong>：计算过程中的效率和临时内存占用。</li>
</ul>
</li>
</ul>
<p><strong>它们如何协同工作？</strong></p>
<p>设想一下用 QLoRA + FlashAttention 微调一个长文本任务的完整流程：</p>
<ol>
<li><strong>模型加载</strong>：首先，<strong>QLoRA</strong> 发挥作用。一个巨大的、被量化成 4-bit 的基础模型和微小的、16-bit 的 LoRA 适配器被加载到 GPU 显存中。<strong>（解决了静态存储问题）</strong></li>
<li><strong>前向传播开始</strong>：当数据输入到模型的某个注意力层时：<br>a. <strong>QLoRA</strong> 将当前计算需要的<strong>一小部分</strong> 4-bit 权重<strong>即时反量化</strong>成 16-bit。<br>b. 用这些 16-bit 的权重去计算出 16-bit 的 Q, K, V 矩阵。</li>
<li><strong>注意力计算</strong>：此时，<strong>FlashAttention</strong> 接管工作。<br>a. 它接收 16-bit 的 Q, K, V 矩阵。<br>b. 开始它的“分块+融合”计算流程，高效地算出注意力输出，全程<strong>避免了生成那个巨大的 N x N 矩阵</strong>。<strong>（解决了动态计算问题）</strong></li>
<li><strong>反向传播</strong>：梯度计算的过程也同样受益于 FlashAttention 的优化。</li>
</ol>
<p><strong>总结</strong>：QLoRA 解决了**“模型太大放不进显卡”<strong>的问题，而 FlashAttention 解决了</strong>“序列太长导致计算过程爆炸”**的问题。两者联手，使得我们可以在单张消费级或专业级 GPU 上，高效地微调一个百亿级别参数的大模型来处理数万长度的超长文本，这在以前是完全无法想象的。</p>
<h2 id="V-大语言模型（LLM）的训练与应用实践"><a href="#V-大语言模型（LLM）的训练与应用实践" class="headerlink" title="V. 大语言模型（LLM）的训练与应用实践"></a><strong>V. 大语言模型（LLM）的训练与应用实践</strong></h2><h3 id="1-参数高效微调-PEFT"><a href="#1-参数高效微调-PEFT" class="headerlink" title="1. 参数高效微调 (PEFT)"></a><strong>1. 参数高效微调 (PEFT)</strong></h3><ul>
<li><strong>核心动机</strong>：<br>对一个拥有数十亿甚至上百亿参数的大语言模型进行全量微调（即更新所有参数），其代价是极其高昂的。不仅需要海量的计算资源，更需要巨大的 GPU 显存来存储模型参数、梯度、以及像 Adam 这样的优化器所产生的额外状态。PEFT 的目标就是，在<strong>尽可能少地改动原模型</strong>的前提下，达到<strong>接近全量微调的效果</strong>。</li>
<li><strong>基本思路</strong>：<br><strong>冻结</strong>大部分原始模型的参数，只向模型中注入一小部分<strong>可训练的“适配器”模块</strong>，在微调时，只更新这些新增的、极少数的参数。</li>
</ul>
<p><strong>1.1 LoRA 的核心原理：低秩适应、冻结参数与“旁路”训练</strong></p>
<p>LoRA (Low-Rank Adaptation) 是目前最成功、应用最广泛的 PEFT 方法之一。</p>
<ul>
<li><strong>核心洞察 (Low-Rank Hypothesis)</strong>：<br>一个预训练好的大模型在适配下游新任务时，其参数的“改变量” ΔW 是具有“低内在秩”的。通俗地说，这个巨大的改动矩阵，可以被近似地分解为两个非常“瘦长”的小矩阵 B⋅A 的乘积。</li>
<li><strong>实现机制：注入“旁路”</strong><ol>
<li><p><strong>冻结原始权重</strong>：将 Transformer 中需要微调的层（通常是 QKV 投影层和 MLP 层）的原始权重矩阵 W <strong>完全冻结</strong>，使其在训练中不被更新。</p>
</li>
<li><p><strong>创建低秩旁路</strong>：在原始权重旁边，并联一个新的、由两个低秩矩阵 A 和 B 组成的“旁路”。</p>
<ul>
<li>矩阵 A 的维度是 <code>d x r</code>（d是输入维度，r是极小的秩，如8）。</li>
<li>矩阵 B 的维度是 <code>r x k</code>（k是输出维度）。</li>
<li>这两个小矩阵 A 和 B 是<strong>可训练的</strong>。</li>
</ul>
</li>
<li><p><strong>合并计算</strong>：对于输入 <code>x</code>，最终的输出是“主路”和“旁路”输出的加和。</p>
<p> h&#x3D;Wx+B(Ax)&#x3D;(W+BA)x</p>
</li>
<li><p><strong>训练过程</strong>：在训练时，只有矩阵 A 和 B 的参数会被梯度更新。因为 <code>r</code> 非常小，所以需要训练的参数量只有全量微调的 <strong>0.1% ~ 1%</strong>，极大地降低了计算和存储开销。</p>
</li>
</ol>
</li>
<li><strong>优点</strong>：<ul>
<li><strong>高效</strong>：训练速度快，显存占用小。</li>
<li><strong>易于部署</strong>：对于不同的任务，只需要保存各自训练好的、非常小巧的 LoRA 权重（A和B），而共享同一个巨大的基础模型。</li>
</ul>
</li>
</ul>
<p><strong>1.2 QLoRA 的核心原理：4-bit 量化、块缩放与即时反量化</strong></p>
<p>QLoRA 将 LoRA 的效率推向了极致，解决了在微调时<strong>加载整个基础模型</strong>的显存瓶颈问题。</p>
<ul>
<li><strong>核心思想</strong>：<br>既然基础模型 W 在 LoRA 训练中是被冻结的，我们是否可以用一种精度更低、更省空间的方式来<strong>存储</strong>它呢？答案就是<strong>量化 (Quantization)</strong>。</li>
<li><strong>关键技术</strong>：<ol>
<li><strong>4-bit 量化 (NF4)</strong>：<ul>
<li><strong>做法</strong>：QLoRA 将原来用 16-bit (FP16&#x2F;BF16) 存储的基础模型权重，压缩成用一种新的、信息论最优的 <strong>4-bit NormalFloat (NF4)</strong> 数据类型来存储。</li>
<li><strong>效果</strong>：仅此一项，就将基础模型的显存占用<strong>降低为原来的 1&#x2F;4</strong>。</li>
</ul>
</li>
<li><strong>块自适应缩放 (Block-wise Scaling)</strong>：<ul>
<li><strong>做法</strong>：为了提高量化精度，模型权重被分成很多小块（如256个元素一组）。<strong>每一块都独立计算一个缩放因子</strong>。</li>
<li><strong>效果</strong>：这使得量化过程可以自适应地处理不同数值范围的权重，极大地减小了量化误差。</li>
</ul>
</li>
<li><strong>即时反量化 (On-the-fly Dequantization)</strong>：<ul>
<li><strong>工作方式</strong>：在模型进行前向传播时，<strong>计算需要用到哪一小块权重，就即时地将这一块 4-bit 的数据反量化回 16-bit 的计算精度</strong>。计算完成后，这个 16-bit 的临时数据立刻被丢弃，显存中只保留 4-bit 的版本。</li>
<li><strong>效果</strong>：这保证了计算的精度，同时维持了极低的常驻显存占用。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="2-解码与生成策略：控制模型的“创造力”"><a href="#2-解码与生成策略：控制模型的“创造力”" class="headerlink" title="2. 解码与生成策略：控制模型的“创造力”"></a><strong>2. 解码与生成策略：控制模型的“创造力”</strong></h3><p>LLM 在生成文本时，本质上是在每一步都计算出一个覆盖整个词汇表的概率分布。解码策略就是我们从这个概率分布中挑选下一个词的方法。</p>
<ul>
<li><strong>Greedy Search (贪心搜索)</strong><ul>
<li><strong>做法</strong>：最简单直接的方法，每一步都选择当前概率最高的那个词。</li>
<li><strong>问题</strong>：虽然安全，但极其容易导致生成内容<strong>枯燥、重复</strong>，甚至陷入“我我我我……”这样的死循环。它只顾眼前最优，可能会错失全局更优的句子。</li>
</ul>
</li>
<li><strong>Top-k Sampling (Top-k 采样)</strong><ul>
<li><strong>做法</strong>：为了增加多样性，我们不再只看第一名，而是划定一个范围。<ol>
<li>找出概率最高的 <code>k</code> 个候选词。</li>
<li>在这 <code>k</code> 个词内部，按照它们自身的概率分布进行<strong>随机采样</strong>。</li>
</ol>
</li>
<li><strong>效果</strong>：通过引入可控的随机性，生成的内容变得更生动、更多样。<code>k</code> 是一个超参数，<code>k</code> 越大，随机性越强；<code>k</code> 越小，生成结果越稳定。</li>
</ul>
</li>
<li><strong>Top-p (Nucleus) Sampling (Top-p &#x2F; 核心采样)</strong><ul>
<li><strong>做法</strong>：这是目前更受青睐的一种方法，它比 Top-k 更智能、更自适应。<ol>
<li>它不固定候选词的数量 <code>k</code>，而是设定一个<strong>累积概率阈值 <code>p</code></strong>（例如 0.95）。</li>
<li>从概率最高的词开始逐个累加，直到这部分词的<strong>总概率超过 <code>p</code></strong> 为止，形成一个“核心候选集 (Nucleus)”。</li>
<li>在这个动态大小的核心候选集中进行随机采样。</li>
</ol>
</li>
<li><strong>优点</strong>：<ul>
<li>当模型对下一个词非常**“确定”**时（例如，“法国的首都是” -&gt; “巴”的概率极高），核心集会很小，保证了事实的准确性。</li>
<li>当模型非常**“不确定”**时（例如，写故事的开头），核心集会很大，允许模型进行更有创造性的探索。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>2. Decoder-Only 架构成为主流的原因</strong></p>
<p>近年来，我们看到像 GPT、Llama、PaLM 等顶级模型，都纷纷采用了 Decoder-only 架构，而不是像 T5、BART 那样的 Encoder-Decoder 架构。</p>
<ul>
<li><strong>原因一：任务的统一与架构的简洁</strong><ul>
<li><strong>Encoder-Decoder</strong> 架构天然为**“源-目标”**式的转换任务（如翻译、摘要）而生，它需要一个 Encoder 先完整理解源文本。</li>
<li><strong>Decoder-only</strong> 架构的本质是<strong>自回归 (Autoregressive)</strong>，即“根据上文预测下文”。事实证明，几乎所有的自然语言任务都可以被统一地、巧妙地转化为这种**“提示工程&#x2F;文本补全 (Prompting&#x2F;Completion)”**的范式。无论是问答、分类还是生成，都可以通过设计不同的 Prompt 来让模型“续写”出答案。这种“万物皆可续写”的模式，使得更简洁的 Decoder-only 架构足以应对绝大多数场景，工程上更具优势。</li>
</ul>
</li>
<li><strong>原因二：预训练任务的强大与通用</strong><ul>
<li>Decoder-only 模型的核心预训练任务极其简单且强大：<strong>“预测下一个词 (Next Token Prediction)”</strong>。</li>
<li>当这个简单的任务在一个接近“无限”的、高质量的互联网语料上进行训练时，模型为了降低预测的困惑度，被迫学习到了关于语法、事实、逻辑、风格等极其丰富的世界知识。</li>
<li>这个单一、通用的预训练目标，被证明是创造一个强大“通才”模型的极佳路径。</li>
</ul>
</li>
</ul>
<p><strong>3. 知识蒸馏技术（以 DistilBERT 为例）</strong></p>
<ul>
<li><strong>核心目的</strong>：进行<strong>模型压缩</strong>。将一个庞大、精确但笨重的“<strong>教师模型</strong>”（如 BERT），其所学到的知识，“蒸馏”到一个更小、更快的“<strong>学生模型</strong>”（如 DistilBERT）中，以方便实际部署。</li>
<li><strong>核心思想：学习“软标签 (Soft Labels)”</strong><ul>
<li><strong>硬标签</strong>：是数据集中非黑即白的标准答案，例如一张图片，标签是 <code>[0, 0, 1, 0]</code>，代表“猫”。</li>
<li><strong>软标签</strong>：是“教师模型”经过思考后，输出的带有不确定性的概率分布。例如，教师模型可能认为图片是 <code>[狗:0.05, 虎:0.04, 猫:0.90, 兔:0.01]</code>。</li>
<li><strong>蒸馏的精髓</strong>：学生模型不仅要学习硬标签（学会做对题），更要学习教师模型的软标签（<strong>学会老师的“解题思路”</strong>）。软标签中蕴含的类别间的相似性等知识，是硬标签无法提供给学生模型的宝贵信息。</li>
</ul>
</li>
<li><strong>DistilBERT 的损失函数</strong><br>为了让学生全方位地模仿老师，DistilBERT 在训练时会同时优化三个目标：<ol>
<li><strong>监督学习损失 (L_CE)</strong>：学生模型的预测结果与<strong>真实硬标签</strong>之间的损失。确保学生能做对题。</li>
<li><strong>蒸馏损失 (L_Distill)</strong>：学生模型的<strong>软标签</strong>与教师模型的<strong>软标签</strong>之间的差异（通常用 KL 散度衡量）。这是模仿老师“思路”的核心。</li>
<li><strong>特征对齐损失 (L_Cos)</strong>：鼓励学生模型的<strong>中间层隐状态向量</strong>与教师模型的隐状态向量在方向上保持一致（用余弦相似度衡量）。这是在模仿老师“思考的中间过程”。</li>
</ol>
</li>
</ul>
<p>通过这种方式，DistilBERT 在参数量减少 40%、速度提升 60% 的情况下，依然能保留 BERT 约 97% 的性能。</p>
<h3 id="3-LLM-训练中的显存占用分析"><a href="#3-LLM-训练中的显存占用分析" class="headerlink" title="3. LLM 训练中的显存占用分析"></a><strong>3. LLM 训练中的显存占用分析</strong></h3><p>GPU 显存是训练大模型最宝贵的资源。一份完整的“显存账单”主要包括以下四个部分：</p>
<ol>
<li><strong>模型参数 (Model Parameters)</strong><ul>
<li><strong>是什么</strong>：模型自身的权重（weights）和偏置（biases）。</li>
<li><strong>特点</strong>：这是“静态”成本，一旦模型架构和精度（如 FP16&#x2F;BF16）确定，这部分大小就固定了。</li>
<li><strong>例子</strong>：一个 7B（70亿）参数的模型，用半精度（2字节&#x2F;参数）存储，需要 <code>7B × 2 = 14 GB</code> 显存。</li>
</ul>
</li>
<li><strong>优化器状态 (Optimizer States)</strong><ul>
<li><strong>是什么</strong>：优化器为每个<strong>可训练参数</strong>存储的额外信息。</li>
<li><strong>特点</strong>：这是最容易被忽略的“隐藏”成本。对于 <strong>AdamW</strong> 优化器，它需要为每个参数存储<strong>一阶动量</strong>和<strong>二阶动量</strong>，这相当于将参数的显存需求<strong>乘以了 2</strong>。</li>
<li><strong>例子</strong>：对于上述 7B 模型进行全量微调，优化器状态就需要 <code>14 GB × 2 = 28 GB</code> 显存。</li>
<li><strong>启示</strong>：这也是为什么 LoRA&#x2F;QLoRA 如此高效的原因之一，因为它们只训练极少数的适配器参数，所以优化器状态的显存开销也极小。</li>
</ul>
</li>
<li><strong>梯度 (Gradients)</strong><ul>
<li><strong>是什么</strong>：反向传播时为每个可训练参数计算出的梯度值。</li>
<li><strong>特点</strong>：其大小与可训练参数完全一致。例如，全量微调 7B 模型，梯度也需要 14GB 显存。</li>
</ul>
</li>
<li><strong>激活值 (Activations)</strong><ul>
<li><strong>是什么</strong>：模型在前向传播过程中，每一层网络计算产生的中间输出。这些输出需要被存储下来，以供反向传播时使用。</li>
<li><strong>特点</strong>：这是“动态”成本，其大小与<strong>批次大小 (Batch Size)</strong>、<strong>序列长度 (Sequence Length)</strong> 和<strong>模型层数</strong>等因素成正比。这是我们在显存不足时，最常用来“开刀”进行调整的部分。</li>
</ul>
</li>
</ol>
<p><strong>PyTorch 中多损失函数的 <code>backward</code> 方法</strong></p>
<ul>
<li><strong>场景</strong>：在多任务学习、知识蒸馏等场景中，我们常常需要同时优化多个损失函数。</li>
<li><strong>错误的做法</strong>：对每个 loss 单独调用 <code>loss.backward()</code>。<ul>
<li><strong>原因</strong>：PyTorch 在默认情况下，执行完一次 <code>.backward()</code> 后，为了释放内存，会立即销毁计算图。当第二次调用 <code>.backward()</code> 时，就会因找不到计算图而报错。</li>
</ul>
</li>
<li><strong>不推荐的做法</strong>：<code>loss1.backward(retain_graph=True)</code>，<code>loss2.backward()</code>。<ul>
<li><strong>原因</strong>：这会强制保留计算图，不仅消耗大量额外显存，而且对于简单的多任务场景，这种做法在梯度累加的逻辑上也是不正确的。</li>
</ul>
</li>
<li><strong>正确且标准的方法：先求和，再反传</strong><ul>
<li><p><strong>做法</strong>：将所有损失函数按照一定的权重（权重也可以是1）相加，形成一个最终的总损失，然后对这个总损失调用一次 <code>.backward()</code>。Python</p>
<p>  <code>total_loss = w1 * loss1 + w2 * loss2   total_loss.backward()</code></p>
</li>
<li><p><strong>原理</strong>：基于微积分的链式法则，梯度的加法等价于加法的梯度 <code>∇(L1+L2) = ∇L1 + ∇L2</code>。这种方法在数学上完全等价，且计算效率最高。</p>
</li>
</ul>
</li>
</ul>
<p><strong><code>model.eval()</code> 的作用及其对现代 LLM 的影响</strong></p>
<ul>
<li><strong><code>model.eval()</code> 的两大作用</strong>：<ol>
<li><strong>关闭 Dropout</strong>：将模型中所有的 Dropout 层的丢弃概率设为 0。这是为了在推理时使用模型的全部能力，并得到一个确定的输出。</li>
<li><strong>切换 BatchNorm (BN)</strong>：将模型中所有的 BN 层切换到评估模式。此时，BN 不再使用当前批次的均值和方差，而是使用在整个训练集上学习到的、固定的全局统计量。</li>
</ol>
</li>
<li><strong>对现代 LLM 的影响：基本无效</strong><ul>
<li>这是一个非常好的面试“陷阱”题。虽然 <code>model.eval()</code> 在 ResNet 等 CV 模型中至关重要，但对于当前主流的、基于 Transformer Decoder 的大语言模型（如 Llama, GPT系列），调用它<strong>几乎没有任何作用</strong>。</li>
<li><strong>原因</strong>：<ol>
<li>这些模型<strong>不使用 BatchNorm</strong>，而是使用 <strong>LayerNorm</strong>。而 LayerNorm 在训练和推理时的行为完全一样，不受 <code>eval()</code> 模式的影响。</li>
<li>这些模型在训练时，也<strong>普遍不使用或很少使用 Dropout</strong>。它们更依赖于海量数据和其他正则化手段。</li>
</ol>
</li>
<li><strong>结论</strong>：知道这一点，能体现出你对模型架构具体实现的深入了解。</li>
</ul>
</li>
</ul>
<p><strong>神经网络在特定领域（如风控）的挑战</strong></p>
<ul>
<li><strong>1. 可解释性 (Interpretability)</strong>：神经网络是一个“黑盒”，其决策过程难以向业务方或监管机构解释。而在风控领域，决策的“可解释性”是硬性要求。</li>
<li><strong>2. 对稀疏噪声数据的敏感性 (Sensitivity to Sparse &amp; Noisy Data)</strong>：风控数据通常是高维稀疏的，且可能包含噪声或异常点。神经网络强大的拟合能力有时反而会学到这些噪声，导致模型鲁棒性差。</li>
<li><strong>3. 难以融合专家规则 (Difficulty Integrating Expert Rules)</strong>：风控领域高度依赖专家经验（Feature Engineering）。相比于能清晰体现“IF-THEN”规则的树模型（如 GBDT），神经网络更像一个“大力出奇迹”的模型，难以将专家规则直观地融入其中。</li>
<li><strong>业界常用解决方案：GBDT + NN</strong>：结合 GBDT 强大的特征交叉与筛选能力，和神经网络强大的非线性拟合能力。先用 GBDT 对原始特征进行处理，生成一组新的、更有信息量的特征，再将这些新特征输入神经网络进行最终的预测。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://qyp9909.github.io">Yuanpeng QU</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://qyp9909.github.io/2025/08/12/DL_1/">https://qyp9909.github.io/2025/08/12/DL_1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Study/">Study</a></div><div class="post-share"><div class="social-share" data-image="/images/cover/Rec_AD_Cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标"><img class="cover" src="/images/cover/Rec_AD_Cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">搜广推笔记 指标</div></div><div class="info-2"><div class="info-item-1">搜广推[指标]Created by: Yuanpeng QUCreated time: 2025年8月11日 16:14 一、...</div></div></div></a><a class="pagination-related" href="/2025/08/12/ML_1/" title="机器学习小记"><img class="cover" src="/images/cover/Rec_AD_Cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">机器学习小记</div></div><div class="info-2"><div class="info-item-1">机器学习小记Created by: Yuanpeng QUCreated time: 2025年8月13日 10:43 第一部分：神经网络的核心组件 (Core Components of Neural Networks)1.1 激活函数：从经典到现代激活函数是神经网络的灵魂，它负责向网络中引入非线性，使得网络有能力学习和拟合现实世界中复杂的非线性关系。如果没有激活函数，多层神经网络本质上等同于一个单层的线性模型。 1.1.1 Sigmoid &amp; Tanh：经典饱和函数的特性与局限性 1. Sigmoid 函数  公式：   $$  f(x) &#x3D; \frac{1}{1+e^{-x}}  $$  核心特性：  将任意实数输入压缩到 (0, 1) 区间内。 这个特性使其输出可以被直观地解释为概率，因此在逻辑回归以及各类分类模型的输出层中，当需要预测一个概率时，Sigmoid 仍然是标准选择。特别是在CTR&#x2F;CVR预估中，它的地位不可动摇。   主要局限性 (面试重点):  梯度消失 (Vanishing Gradient): 这是 Sigmoid...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/08/Rec_AD_Ranking/" title="搜广推笔记 排序"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-08</div><div class="info-item-2">搜广推笔记 排序</div></div><div class="info-2"><div class="info-item-1">搜广推[排序]Created by: Yuanpeng QUCreated time: 2025年8月9日 14:29 推荐系统排序模型知识体系总结（大纲）第一章：排序模型的基础 (Foundations of Ranking Models)在深入探讨各种复杂精妙的模型之前，我们必须先理解所有排序模型所立足的共同基础。这个基础包含两个层面：一是我们如何从业务逻辑上将“排序”这个抽象任务，转化为一个可以用数学模型解决的问题；二是在这个问题的解决方案中，那个最简单、最核心的基石模型——逻辑回归（LR）——是如何工作的。 1.1...</div></div></div></a><a class="pagination-related" href="/2025/08/06/Rec_AD_Recall/" title="搜广推笔记 召回"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-06</div><div class="info-item-2">搜广推笔记 召回</div></div><div class="info-2"><div class="info-item-1">搜广推[召回]Created by: Yuanpeng QUCreated time: 2025年8月5日 22:51 一、 推荐系统总体架构 (Overall Recommendation System Architecture)1. 级联漏斗范式 (Cascading Funnel Paradigm) 现代大规模推荐系统的核心架构，普遍遵循一种级联漏斗范式 (Cascading Funnel Paradigm)。这个范式的诞生，是为了解决一个根本性的矛盾：一方面，我们的候选物品库是海量的（百万、千万甚至上亿级别）；另一方面，用户的屏幕空间是有限的，且要求响应速度极快（毫秒级）。 因此，我们不可能对所有物品都用最复杂的模型进行最精确的计算。漏斗范式通过设置多个层层递进的过滤环节，实现了从海量到少量，从粗糙到精准的逐级筛选，在计算效率和推荐效果之间取得了极致的平衡。 这个漏斗通常包含以下四个核心层级： 1. 召回层 (Recall...</div></div></div></a><a class="pagination-related" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-11</div><div class="info-item-2">搜广推笔记 指标</div></div><div class="info-2"><div class="info-item-1">搜广推[指标]Created by: Yuanpeng QUCreated time: 2025年8月11日 16:14 一、...</div></div></div></a><a class="pagination-related" href="/2025/08/10/Rec_AD_MTL/" title="搜广推笔记 多目标排序"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-10</div><div class="info-item-2">搜广推笔记 多目标排序</div></div><div class="info-2"><div class="info-item-1">搜广推[多目标排序]Created by: Yuanpeng QUCreated time: 2025年8月10日 21:12 第一部分：多目标学习的核心挑战与基础1. 动机：为何需要多目标学习？现代工业级推荐系统（如电商、短视频）的目标是综合性的，不能只关注单一指标 。例如，一个短视频推荐系统不仅要优化点击率，可能还要同时提升用户的点赞、关注、转发、评论率以及观看时长 。 如果为每个任务单独建模和优化，或者用一个简单的模型直接预测所有任务，就会遇到**“跷跷板效应” (Seesaw Phenomenon)** 。  定义：指的是当模型在优化一个目标时，会导致另一个或多个其他目标的性能下降的现象 。 举例： 任务跷跷板 (Task Seesaw)：一个模型如果过度优化点击率（CTR），可能会倾向于推荐标题党或封面吸引人的内容，但这部分内容的用户实际满意度（如观看时长、点赞率）可能很低，从而损害了长期用户体验 。 领域跷跷板 (Domain...</div></div></div></a><a class="pagination-related" href="/2025/08/12/ML_1/" title="机器学习小记"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-12</div><div class="info-item-2">机器学习小记</div></div><div class="info-2"><div class="info-item-1">机器学习小记Created by: Yuanpeng QUCreated time: 2025年8月13日 10:43 第一部分：神经网络的核心组件 (Core Components of Neural Networks)1.1 激活函数：从经典到现代激活函数是神经网络的灵魂，它负责向网络中引入非线性，使得网络有能力学习和拟合现实世界中复杂的非线性关系。如果没有激活函数，多层神经网络本质上等同于一个单层的线性模型。 1.1.1 Sigmoid &amp; Tanh：经典饱和函数的特性与局限性 1. Sigmoid 函数  公式：   $$  f(x) &#x3D; \frac{1}{1+e^{-x}}  $$  核心特性：  将任意实数输入压缩到 (0, 1) 区间内。 这个特性使其输出可以被直观地解释为概率，因此在逻辑回归以及各类分类模型的输出层中，当需要预测一个概率时，Sigmoid 仍然是标准选择。特别是在CTR&#x2F;CVR预估中，它的地位不可动摇。   主要局限性 (面试重点):  梯度消失 (Vanishing Gradient): 这是 Sigmoid...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/LOGO.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Yuanpeng QU</div><div class="author-info-description">CS PhD, 3rd yr.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><a id="card-info-btn" href="https://qyp9909.github.io/homepage"><i class="fas fa-user-graduate"></i><span>Academic Homepage</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qyp9909/qyp9909.github.io" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qyp9909@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">深度学习小记</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#I-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="toc-number">2.</span> <span class="toc-text">I. 神经网络的核心组件与训练技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%9ABN-%E4%B8%8E-LN-%E7%9A%84%E5%8E%9F%E7%90%86%E3%80%81%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.1.</span> <span class="toc-text">1. 归一化层：BN 与 LN 的原理、区别与应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A2%AF%E5%BA%A6%E9%97%AE%E9%A2%98%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">2.2.</span> <span class="toc-text">2. 梯度问题：梯度消失与梯度爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">2.3.</span> <span class="toc-text">3. 初始化策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#II-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF"><span class="toc-number">3.</span> <span class="toc-text">II. 优化算法的演进之路</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A%E4%B8%80%E9%98%B6%E5%8A%A8%E9%87%8F%E4%B8%8E%E4%BA%8C%E9%98%B6%E5%8A%A8%E9%87%8F"><span class="toc-number">3.1.</span> <span class="toc-text">1. 核心思想：一阶动量与二阶动量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-SGD-%E5%AE%B6%E6%97%8F%EF%BC%9A%E4%BB%8E%E2%80%9C%E7%9B%B2%E4%BA%BA%E6%91%B8%E8%B1%A1%E2%80%9D%E5%88%B0%E2%80%9C%E5%B8%A6%E6%83%AF%E6%80%A7%E4%B8%8B%E5%9D%A1%E2%80%9D"><span class="toc-number">3.2.</span> <span class="toc-text">2. SGD 家族：从“盲人摸象”到“带惯性下坡”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%87%AA%E9%80%82%E5%BA%94%E5%AE%B6%E6%97%8F%EF%BC%9A%E4%B8%BA%E6%AF%8F%E4%B8%AA%E5%8F%82%E6%95%B0%E5%AE%9A%E5%88%B6%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.3.</span> <span class="toc-text">3. 自适应家族：为每个参数定制学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%9A%E9%9D%A2%E5%90%91%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E5%AE%9E%E6%88%98%E9%80%89%E6%8B%A9"><span class="toc-number">3.4.</span> <span class="toc-text">4. 高级优化器：面向工业界的实战选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%EF%BC%9A%E6%8E%8C%E6%8F%A1%E8%AE%AD%E7%BB%83%E7%9A%84%E2%80%9C%E8%8A%82%E5%A5%8F%E2%80%9D"><span class="toc-number">3.5.</span> <span class="toc-text">5. 学习率调度：掌握训练的“节奏”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#III-%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E7%9A%84%E8%BF%9B%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">III. 文本表示的进化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BB%9F%E8%AE%A1%E6%97%B6%E4%BB%A3%EF%BC%9ATF-IDF-%E2%80%94-%E2%80%9C%E4%BB%A5%E6%95%B0%E5%8F%96%E8%83%9C%E2%80%9D%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%E5%A4%A7%E5%B8%88"><span class="toc-number">4.1.</span> <span class="toc-text">1. 统计时代：TF-IDF — “以数取胜”的关键词大师</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%9D%99%E6%80%81%E5%90%91%E9%87%8F%E6%97%B6%E4%BB%A3%EF%BC%9AWord2Vec-%E2%80%94-%E2%80%9C%E7%89%A9%E4%BB%A5%E7%B1%BB%E8%81%9A%E2%80%9D%E7%9A%84%E8%AF%AD%E4%B9%89%E5%85%88%E9%94%8B"><span class="toc-number">4.2.</span> <span class="toc-text">2. 静态向量时代：Word2Vec — “物以类聚”的语义先锋</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%8A%A8%E6%80%81%E5%90%91%E9%87%8F%E6%97%B6%E4%BB%A3%EF%BC%9ABERT-%E2%80%94-%E2%80%9C%E5%8D%83%E4%BA%BA%E5%8D%83%E9%9D%A2%E2%80%9D%E7%9A%84%E8%AF%AD%E5%A2%83%E5%A4%A7%E5%B8%88"><span class="toc-number">4.3.</span> <span class="toc-text">3. 动态向量时代：BERT — “千人千面”的语境大师</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IV-Transformer-%E4%B8%8E-BERT-%E6%A8%A1%E5%9E%8B%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text">IV. Transformer 与 BERT 模型深度剖析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Transformer-%E5%9F%BA%E7%9F%B3%EF%BC%9A%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-Attention"><span class="toc-number">5.1.</span> <span class="toc-text">1. Transformer 基石：自注意力机制 (Self-Attention)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-BERT-%E6%A8%A1%E5%9E%8B%E8%A7%A3%E6%9E%90"><span class="toc-number">5.2.</span> <span class="toc-text">2. BERT 模型解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Transformer-BERT-%E7%9A%84%E5%B1%80%E9%99%90%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">5.3.</span> <span class="toc-text">3. Transformer &amp; BERT 的局限与优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#V-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E7%9A%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5"><span class="toc-number">6.</span> <span class="toc-text">V. 大语言模型（LLM）的训练与应用实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83-PEFT"><span class="toc-number">6.1.</span> <span class="toc-text">1. 参数高效微调 (PEFT)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A7%A3%E7%A0%81%E4%B8%8E%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%EF%BC%9A%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%E7%9A%84%E2%80%9C%E5%88%9B%E9%80%A0%E5%8A%9B%E2%80%9D"><span class="toc-number">6.2.</span> <span class="toc-text">2. 解码与生成策略：控制模型的“创造力”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-LLM-%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E5%88%86%E6%9E%90"><span class="toc-number">6.3.</span> <span class="toc-text">3. LLM 训练中的显存占用分析</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/14/leetcode/LeetCode%20Hot100-Day%201/" title="LeetCode Hot100-Day 1"><img src="/images/cover/hot100.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode Hot100-Day 1"/></a><div class="content"><a class="title" href="/2025/08/14/leetcode/LeetCode%20Hot100-Day%201/" title="LeetCode Hot100-Day 1">LeetCode Hot100-Day 1</a><time datetime="2025-08-14T02:00:00.000Z" title="Created 2025-08-14 11:00:00">2025-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/13/leetcode/LeetCode%20Hot100-Day%200/" title="LeetCode Hot100-Day 0"><img src="/images/cover/hot100.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode Hot100-Day 0"/></a><div class="content"><a class="title" href="/2025/08/13/leetcode/LeetCode%20Hot100-Day%200/" title="LeetCode Hot100-Day 0">LeetCode Hot100-Day 0</a><time datetime="2025-08-13T13:00:00.000Z" title="Created 2025-08-13 22:00:00">2025-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/12/ML_1/" title="机器学习小记"><img src="/images/cover/Rec_AD_Cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习小记"/></a><div class="content"><a class="title" href="/2025/08/12/ML_1/" title="机器学习小记">机器学习小记</a><time datetime="2025-08-12T09:00:00.000Z" title="Created 2025-08-12 18:00:00">2025-08-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/12/DL_1/" title="深度学习小记"><img src="/images/cover/Rec_AD_Cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习小记"/></a><div class="content"><a class="title" href="/2025/08/12/DL_1/" title="深度学习小记">深度学习小记</a><time datetime="2025-08-12T03:00:00.000Z" title="Created 2025-08-12 12:00:00">2025-08-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标"><img src="/images/cover/Rec_AD_Cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="搜广推笔记 指标"/></a><div class="content"><a class="title" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标">搜广推笔记 指标</a><time datetime="2025-08-11T06:00:00.000Z" title="Created 2025-08-11 15:00:00">2025-08-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2025 By Yuanpeng QU</div><div class="footer_custom_text">Welcome to Kyoku's <a href="https://qyp9909.github.io/">blog</a>! All right reserved.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="255,255,255" opacity="0.8" zIndex="-1" count="200" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>