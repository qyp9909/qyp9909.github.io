<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>搜广推笔记 排序 | Kyoku's Blog</title><meta name="author" content="Yuanpeng QU"><meta name="copyright" content="Yuanpeng QU"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="搜广推[排序]Created by: Yuanpeng QUCreated time: 2025年8月9日 14:29 推荐系统排序模型知识体系总结（大纲）第一章：排序模型的基础 (Foundations of Ranking Models)在深入探讨各种复杂精妙的模型之前，我们必须先理解所有排序模型所立足的共同基础。这个基础包含两个层面：一是我们如何从业务逻辑上将“排序”这个抽象任务，转化为一个">
<meta property="og:type" content="article">
<meta property="og:title" content="搜广推笔记 排序">
<meta property="og:url" content="https://qyp9909.github.io/2025/08/08/Rec_AD_Ranking/index.html">
<meta property="og:site_name" content="Kyoku&#39;s Blog">
<meta property="og:description" content="搜广推[排序]Created by: Yuanpeng QUCreated time: 2025年8月9日 14:29 推荐系统排序模型知识体系总结（大纲）第一章：排序模型的基础 (Foundations of Ranking Models)在深入探讨各种复杂精妙的模型之前，我们必须先理解所有排序模型所立足的共同基础。这个基础包含两个层面：一是我们如何从业务逻辑上将“排序”这个抽象任务，转化为一个">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://qyp9909.github.io/images/cover/Rec_AD_Cover.png">
<meta property="article:published_time" content="2025-08-08T09:00:00.000Z">
<meta property="article:modified_time" content="2025-08-11T08:17:49.898Z">
<meta property="article:author" content="Yuanpeng QU">
<meta property="article:tag" content="Study">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qyp9909.github.io/images/cover/Rec_AD_Cover.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "搜广推笔记 排序",
  "url": "https://qyp9909.github.io/2025/08/08/Rec_AD_Ranking/",
  "image": "https://qyp9909.github.io/images/cover/Rec_AD_Cover.png",
  "datePublished": "2025-08-08T09:00:00.000Z",
  "dateModified": "2025-08-11T08:17:49.898Z",
  "author": [
    {
      "@type": "Person",
      "name": "Yuanpeng QU",
      "url": "https://qyp9909.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qyp9909.github.io/2025/08/08/Rec_AD_Ranking/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '搜广推笔记 排序',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" media="defer" onload="this.media='all'"><link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><link rel="preload" as="image" href="/img/1.jpg"><link rel="preload" href="/fonts/circle400w.ttf" as="font" type="font/ttf" crossorigin="anonymous"><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/img/1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/LOGO.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-vihara"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-newspaper"></i><span> Articles</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-mug-hot"></i><span> Life</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-camera-retro"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fab fa-itunes-note"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fab fa-youtube"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-torii-gate"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-star-of-david"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Kyoku's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">搜广推笔记 排序</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-vihara"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-newspaper"></i><span> Articles</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-mug-hot"></i><span> Life</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-camera-retro"></i><span> Gallery</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fab fa-itunes-note"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fab fa-youtube"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-torii-gate"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-star-of-david"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">搜广推笔记 排序</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-08-08T09:00:00.000Z" title="Created 2025-08-08 18:00:00">2025-08-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-08-11T08:17:49.898Z" title="Updated 2025-08-11 17:17:49">2025-08-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Recsys/">Recsys</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="搜广推-排序"><a href="#搜广推-排序" class="headerlink" title="搜广推[排序]"></a>搜广推[排序]</h2><p>Created by: Yuanpeng QU<br>Created time: 2025年8月9日 14:29</p>
<h3 id="推荐系统排序模型知识体系总结（大纲）"><a href="#推荐系统排序模型知识体系总结（大纲）" class="headerlink" title="推荐系统排序模型知识体系总结（大纲）"></a><strong>推荐系统排序模型知识体系总结（大纲）</strong></h3><h2 id="第一章：排序模型的基础-Foundations-of-Ranking-Models"><a href="#第一章：排序模型的基础-Foundations-of-Ranking-Models" class="headerlink" title="第一章：排序模型的基础 (Foundations of Ranking Models)"></a><strong>第一章：排序模型的基础 (Foundations of Ranking Models)</strong></h2><p>在深入探讨各种复杂精妙的模型之前，我们必须先理解所有排序模型所立足的共同基础。这个基础包含两个层面：一是我们如何从业务逻辑上将“排序”这个抽象任务，转化为一个可以用数学模型解决的问题；二是在这个问题的解决方案中，那个最简单、最核心的基石模型——逻辑回归（LR）——是如何工作的。</p>
<h3 id="1-1-排序的核心任务：从“排序列表”到“点击率预估”的思维转变"><a href="#1-1-排序的核心任务：从“排序列表”到“点击率预估”的思维转变" class="headerlink" title="1.1 排序的核心任务：从“排序列表”到“点击率预估”的思维转变"></a>1.1 排序的核心任务：从“排序列表”到“点击率预估”的思维转变</h3><p>一个推荐系统或搜索引擎，其最终呈现给用户的，是一个<strong>有序的物品列表</strong>。系统的终极目标是让这个列表的排序方式，能够最大化某个商业指标（如用户满意度、平台收入、用户总停留时长等）。</p>
<p>直接去优化一个列表的“顺序”本身，在数学上是一件非常困难的事情。我们很难定义一个平滑、可导的损失函数来衡量一个列表“排得有多好”。因此，工业界和学术界普遍采用了一种非常关键的<strong>思维转变</strong>，将这个复杂的“列表排序问题”进行降维和简化。这种转变的核心是：<strong>不再直接比较物品与物品之间的相对顺序，而是为每一个独立的物品进行“打分”。</strong></p>
<p>我们只需要找到一个能够精准衡量“物品对特定用户的吸引力”的分数。分数越高的物品，就理应排在越靠前的位置。在互联网广告、电商推荐、信息流等场景下，<strong>点击率（Click-Through Rate, CTR）</strong> 被证明是这个“分数”的最佳选择之一：</p>
<ol>
<li><strong>可衡量性</strong>：用户的点击行为可以被大规模、精确地记录下来，为模型训练提供了海量的、真实的标注数据（曝光但未点击为负样本，点击为正样本）。</li>
<li><strong>与兴趣的强关联性</strong>：用户的一次点击，是其对某个内容感兴趣的、最直接、最明确的信号。一个物品的预估点击率越高，通常意味着它更能吸引该用户。</li>
<li><strong>概率的天然契合</strong>：CTR 本身是一个概率值（点击的概率），这与许多机器学习分类模型的输出形式（如逻辑回归通过Sigmoid函数输出一个0到1的概率）完美契合。</li>
</ol>
<p>通过这个思维转变，我们成功地将一个模糊的“排序问题”，转换成了一个定义清晰的、标准的<strong>二元分类问题</strong>：</p>
<blockquote>
<p><strong>对于任意一个给定的（用户，物品，上下文）组合，预测该用户点击该物品的概率是多少？</strong></p>
</blockquote>
<p>一旦我们训练好一个能精准预测CTR的模型，排序过程就变得非常简单：</p>
<ol>
<li>对所有候选物品，逐一调用模型，预测出它们各自的pCTR（predicted CTR）分数。</li>
<li>将所有物品按照pCTR分数从高到低进行排序。</li>
<li>将排序后的列表呈现给用户。</li>
</ol>
<p>这个从“优化顺序”到“预测分数”的转变，是整个现代推荐排序系统的基石，它为我们后续讨论的所有复杂模型的应用铺平了道路。</p>
<h3 id="1-2-万变不离其宗的基石：逻辑回归-LR-的原理与局限性"><a href="#1-2-万变不离其宗的基石：逻辑回归-LR-的原理与局限性" class="headerlink" title="1.2 万变不离其宗的基石：逻辑回归 (LR) 的原理与局限性"></a>1.2 万变不离其宗的基石：逻辑回归 (LR) 的原理与局限性</h3><p>逻辑回归（Logistic Regression, LR）是解决CTR预估问题最基础、最经典的“Hello, World”级别的模型。尽管它很简单，但其核心思想贯穿了后续许多复杂模型的始终。</p>
<p><strong>模型原理</strong></p>
<p>LR模型通过两步来完成从输入特征到最终概率的预测：</p>
<p>**1. 线性求和 (Linear Combination)：**首先，模型接收一组输入特征（如年龄、性别、物品品类等），并为每个特征分配一个权重 wi。然后将所有特征进行加权求和，得到一个原始的预测分数 z。</p>
<p><img src="/.io//image.png" alt="image.png"></p>
<p>这里的权重 wi是模型需要从数据中学习的参数，它代表了每个特征对最终结果的影响方向（正或负）和大小。</p>
<p><strong>2. Sigmoid函数</strong>：上述的线性分数 z 是一个没有边界的实数。为了将其转换为一个0到1之间的概率值，LR使用了一个关键的非线性函数——<strong>Sigmoid函数</strong>。</p>
<p><img src="/.io//image%201.png" alt="image.png"></p>
<p>无论输入的 z 多大或多小，经过Sigmoid函数处理后的输出y_hat都会被“压扁”到(0, 1)区间内，这个值就是模型预测的点击率（pCTR）。</p>
<p><strong>LR的优点</strong></p>
<ul>
<li><strong>简单高效</strong>：模型原理简单，计算量小，无论训练还是在线预测，速度都非常快，非常适合大规模工业应用。</li>
<li><strong>可解释性强</strong>：每个特征的权重 wi 都清晰地反映了该特征对结果的影响。例如，一个较大的正权重意味着这个特征是点击行为的强预测因子。这使得模型非常便于理解和调试。</li>
</ul>
<p><strong>LR的局限性 (也是后续模型演进的动力)</strong></p>
<p>LR最大的局限性在于它的<strong>线性假设</strong>。</p>
<ol>
<li><strong>无法学习非线性关系</strong>：模型假设最终的决策边界是线性的，无法捕捉现实世界中普遍存在的非线性模式。</li>
<li><strong>无法自动学习特征交叉</strong>：这是线性假设最直接的后果。LR无法自动学习到“20岁的男性”对“游戏机”这个<strong>组合</strong>的特殊偏好。它只能学习到“20岁”的独立影响和“男性”的独立影响，然后将它们简单相加。</li>
<li><strong>依赖大量人工特征工程</strong>：为了克服上述缺点，算法工程师必须手动地、基于业务经验地去创造交叉特征（例如，创建一个新的特征叫 <code>is_male_and_age_20</code>），然后再喂给LR模型。这个过程极其耗费人力、难以穷举，且效果高度依赖工程师的经验。</li>
</ol>
<p><strong>总结</strong>：逻辑回归是排序模型的起点。它的简单和高效使其成为一个强大的基准，但其无法自动学习特征交叉的根本性缺陷，是催生后续 GBDT+LR、FM 以及所有深度学习排序模型不断演进的核心驱动力。</p>
<h3 id="1-3-数据的预处理：特征工程的核心概念"><a href="#1-3-数据的预处理：特征工程的核心概念" class="headerlink" title="1.3 数据的预处理：特征工程的核心概念"></a>1.3 数据的预处理：特征工程的核心概念</h3><p>在我们讨论的所有模型中，都有一个共同的前提：模型本身只能处理数值。然而，我们拥有的原始数据是各式各样的，有类别、有数字、有文本。<strong>特征工程 (Feature Engineering)</strong> 的核心任务，就是将这些原始数据转换成模型能够理解和利用的、高质量的数值化特征。这是决定模型性能上限的关键一步。</p>
<ul>
<li>1.3.1 类别特征与独热编码 (One-Hot Encoding)</li>
</ul>
<p><strong>类别特征 (Categorical Features)</strong> 是指那些取值为离散标签或符号的特征。这些值之间没有大小、顺序之分。在推荐系统中，这类特征无处不在，例如：</p>
<ul>
<li><code>user_id</code></li>
<li><code>item_id</code></li>
<li><code>city</code> (如：’北京’, ‘上海’, ‘广州’)</li>
<li><code>gender</code> (如：’男性’, ‘女性’)</li>
</ul>
<p><strong>面临的问题</strong>：<br>模型无法直接理解 <code>&#39;北京&#39;</code> 这样的文本。一个最朴素的想法可能是给它们编号，比如 <code>&#39;北京&#39;=1</code>, <code>&#39;上海&#39;=2</code>, <code>&#39;广州&#39;=3</code>。但这样做是<strong>完全错误</strong>的，因为它凭空引入了一种不存在的数学关系，即 <code>广州 &gt; 上海 &gt; 北京</code>。模型会基于这个错误的大小关系去学习，导致结果荒谬。</p>
<p><strong>解决方案：独热编码 (One-Hot Encoding)</strong><br>独热编码是处理类别特征最标准、最有效的方法。它的核心思想是<strong>用一个向量来表示一个类别，且向量中只有一个位置是 <code>1</code>，其他所有位置都是 <code>0</code></strong>。<br>通过这种方式，我们成功地将类别特征转换成了一个<strong>高维度的稀疏向量</strong>（向量中绝大部分元素都是0）。这样做的好处是，它彻底消除了类别之间的伪序关系，并使得线性模型可以为每一个独立的类别（如 <code>&#39;北京&#39;</code> 这个城市）学习到一个专属的、不受其他城市干扰的权重 <code>w_beijing</code>。</p>
<ul>
<li>1.3.2 连续特征与归一化、标准化</li>
</ul>
<p><strong>连续特征 (Continuous Features)</strong> 是指那些取值是连续数值的特征，这些数值有明确的大小和顺序关系。例如：</p>
<ul>
<li><code>age</code> (年龄)</li>
<li><code>price</code> (商品价格)</li>
<li><code>item_historical_ctr</code> (物品历史点击率)</li>
</ul>
<p><strong>面临的问题：</strong><br>不同连续特征之间的**数值尺度（Scale）**可能差异巨大。例如，用户的年龄 <code>age</code> 可能在 <code>18</code> 到 <code>60</code> 之间，而用户的年收入 <code>income</code> 可能在 <code>50000</code> 到 <code>1000000</code> 之间。如果直接将这些原始数值输入模型：</p>
<ul>
<li><strong>大尺度特征会“支配”模型</strong>：在计算损失和梯度时，<code>income</code> 这个特征的数值会远远大于 <code>age</code>，导致模型对 <code>income</code> 的变化极其敏感，而几乎忽略 <code>age</code> 的影响。</li>
<li><strong>训练过程不稳定</strong>：巨大的数值差异会导致损失函数的“等高线”变成一个又扁又窄的“峡谷”，使得梯度下降法很难稳定、高效地找到最优点。</li>
</ul>
<p><strong>解决方案：特征缩放 (Feature Scaling)</strong><br>为了消除不同特征间的尺度差异，我们需要对它们进行缩放处理，让所有特征都处于一个可比较的量级上。最常用的两种方法是归一化和标准化。</p>
<p><strong>1. 归一化 (Normalization)</strong></p>
<ul>
<li><strong>别名</strong>：最小-最大缩放 (Min-Max Scaling)。</li>
<li><strong>目标</strong>：将数据线性地缩放到一个固定的区间，通常是 <code>[0, 1]</code>。</li>
<li><strong>公式</strong>：<code>X_norm = (X - X_min) / (X_max - X_min)</code></li>
<li><strong>说明</strong>：<code>X</code> 是原始值，<code>X_min</code> 和 <code>X_max</code> 分别是该特征在整个数据集中的最小值和最大值。这种方法非常直观，但对**异常值（Outliers）**非常敏感。</li>
</ul>
<p><strong>2. 标准化 (Standardization)</strong></p>
<ul>
<li><strong>别名</strong>：Z-score 标准化。</li>
<li><strong>目标</strong>：将数据转换成一个<strong>均值为 <code>0</code>，标准差为 <code>1</code></strong> 的标准正态分布。</li>
<li><strong>公式</strong>：<code>X_std = (X - μ) / σ</code></li>
<li><strong>说明</strong>：<code>μ</code> 是该特征的均值，<code>σ</code> 是其标准差。这种方法比归一化更<strong>鲁棒</strong>，因为它不受异常值的影响。在大多数深度学习和线性模型中，<strong>标准化是更常用、更受推荐的选择</strong>。</li>
</ul>
<p>数据预处理是构建任何成功排序模型的<strong>必要前提</strong>。通过独热编码处理类别特征，以及通过归一化&#x2F;标准化处理连续特征，我们能够为模型提供一份“干净”、“公平”且易于学习的数值化输入，这是保证模型训练稳定、高效并最终取得良好效果的基石。</p>
<h2 id="第二章：特征交叉的“手动”与“自动”时代-The-Era-of-Manual-vs-Automatic-Feature-Crossing"><a href="#第二章：特征交叉的“手动”与“自动”时代-The-Era-of-Manual-vs-Automatic-Feature-Crossing" class="headerlink" title="第二章：特征交叉的“手动”与“自动”时代 (The Era of Manual vs. Automatic Feature Crossing)"></a><strong>第二章：特征交叉的“手动”与“自动”时代 (The Era of Manual vs. Automatic Feature Crossing)</strong></h2><p>在第一章我们认识到，逻辑回归（LR）最大的瓶颈在于其无法自动学习<strong>特征交叉 (Feature Crossing)</strong>。为了让模型捕捉到现实世界中普遍存在的非线性关系，工程师们开始了从“手动”到“自动”探索特征交叉的漫长而关键的旅程。</p>
<h3 id="2-1-“半自动”的探索：GBDT-LR-组合模型"><a href="#2-1-“半自动”的探索：GBDT-LR-组合模型" class="headerlink" title="2.1 “半自动”的探索：GBDT+LR 组合模型"></a>2.1 “半自动”的探索：GBDT+LR 组合模型</h3><p>在深度学习模型全面流行之前，业界找到了一个极其巧妙的“半自动”解决方案，就是由 Facebook 在2014年提出的 <strong>GBDT+LR</strong> 模型。它不是一个端到端的模型，而是一个两阶段的流程，聪明地利用了两种模型的各自优势。</p>
<ul>
<li><strong>核心思想</strong>：让强大的 <strong>GBDT (梯度提升决策树)</strong> 模型充当“<strong>特征工程大师</strong>”，自动完成最困难的特征交叉和特征离散化工作；然后让简单高效的 <strong>LR</strong> 模型充当“<strong>分类专家</strong>”，对这些加工好的高质量特征进行学习和打分。</li>
<li><strong>工作流程</strong>：<ol>
<li><strong>训练 GBDT</strong>：首先，在数据集上训练一个 GBDT 模型。GBDT由多棵决策树组成。</li>
<li><strong>特征变换</strong>：对于每一个输入样本，让它穿过 GBDT 模型中已经训练好的每一棵树。样本在每棵树中都会从根节点走到一个特定的叶子节点。<ul>
<li><strong>自动特征交叉</strong>：从根节点到叶子节点的这条<strong>路径</strong>，其本身就是一系列特征判断规则的组合（例如，<code>IF age &gt; 30 AND city = &#39;北京&#39;</code>），这构成了一个高阶的、非线性的组合特征。</li>
</ul>
</li>
<li><strong>One-Hot 编码</strong>：模型将样本在<strong>每一棵树</strong>上最终落到的那个<strong>叶子节点的编号 (index)</strong> 收集起来。然后，将这些叶子节点编号进行 <code>One-Hot</code> 编码，形成一个维度很高、但极其稀疏的新特征向量。</li>
<li><strong>训练 LR</strong>：最后，将这个由 GBDT 生成的、包含了丰富交叉信息的新特征向量，作为输入来训练一个 LR 模型，由它做出最终的 CTR 预估。</li>
</ol>
</li>
<li><strong>总结</strong>：GBDT+LR 是一个里程碑式的组合模型。它 brilliantly地利用了 GBDT 这种树模型的结构特性，来自动化地完成特征交叉这个曾经极度依赖人工经验的脏活累活。它虽然不是一个端到端的优雅模型，但其思想为后续的自动化建模探索铺平了道路，是“半自动”时代最耀眼的明星。</li>
</ul>
<h3 id="2-2-“自动交叉”的基石：因子分解机-FM-的原理与突破"><a href="#2-2-“自动交叉”的基石：因子分解机-FM-的原理与突破" class="headerlink" title="2.2 “自动交叉”的基石：因子分解机 (FM) 的原理与突破"></a>2.2 “自动交叉”的基石：因子分解机 (FM) 的原理与突破</h3><p>如果说 GBDT+LR 是用工程组合的方式解决了问题，那么<strong>因子分解机 (Factorization Machine, FM)</strong> 则是从模型结构层面，为“自动特征交叉”提供了一个优雅、高效且影响深远的数学方案。</p>
<ul>
<li><strong>核心动机</strong>：FM 的设计直指传统模型在处理<strong>数据稀疏性 (Data Sparsity)</strong> 时的最大痛点。在推荐场景中，绝大多数的 <code>(用户, 物品)</code> 对在训练数据中从未出现过。对于一个从未见过的特征组合 <code>(x_i, x_j)</code>，模型是无法学习出其交叉权重 <code>w_ij</code> 的。</li>
<li><strong>FM 的突破性思想：因子分解 (Factorization)</strong><br>FM 不再直接学习每一个交叉项的独立权重 <code>w_ij</code>，而是做了一个巧妙的假设：这个交叉权重 <code>w_ij</code> 可以被“分解”为两个低维<strong>隐向量 (Latent Vector)</strong> 的<strong>内积 (Dot Product)</strong>。</li>
</ul>
<p><img src="/.io//image%252.png" alt="image.png"></p>
<p>这里，模型为<strong>每一个</strong>原始特征 <code>i</code>，都学习一个低维（例如，维度为 <code>k=16</code>）的稠密向量 <code>v_i</code>。这个 <code>v_i</code> 就是我们现在所熟知的**“Embedding”**。</p>
<p><strong>1. 起点：FM二阶项的暴力计算公式</strong></p>
<p>我们从 FM 模型中负责特征交叉的<strong>二阶项</strong>的原始定义出发：</p>
<p><img src="/.io//image%253.png" alt="image.png"></p>
<p>其中，n 是特征数量，vi是第 i 个特征的 k 维隐向量。</p>
<p><strong>复杂度分析</strong>：这个公式的计算需要遍历所有不重复的特征对 (i,j)，约有 2n(n−1) 对，复杂度为 O(n2)。对于每一对，都需要计算一次 k 维向量的内积，复杂度为 O(k)。因此，总的计算复杂度为 O(kn2)。对于百万级的特征量，这个计算量是无法接受的。</p>
<p><strong>核心推导：从 O(kn2) 到 O(kn)</strong></p>
<p>我们的目标是对上述公式进行化简。</p>
<p><strong>第一步：展开内积</strong></p>
<p>首先，我们将内积 ⟨vi,vj⟩ 展开为其定义形式，即对应元素乘积的和：</p>
<p><img src="/.io//image%254.png" alt="image.png"></p>
<p>其中 vif是向量 vi的第 f 个元素。将此代入原公式，得到：</p>
<p><img src="/.io//image%255.png" alt="image.png"></p>
<p><strong>第二步：利用对称性，变换求和范围（关键步骤）</strong></p>
<p>直接处理 ∑j&#x3D;i+1n 这个求和上下限不固定的部分比较困难。我们观察所有 vif vjf 项构成的矩阵，可以发现以下关系：</p>
<p><img src="/.io//image%256.png" alt="image.png"></p>
<p>由此，我们可以得到我们想要的求和部分：</p>
<p><img src="/.io//image%257.png" alt="image.png"></p>
<p>我们将这个恒等式应用到我们的问题上。</p>
<p><strong>第三步：交换求和顺序，提取公因式</strong></p>
<p>让我们先只关注一个特定的维度 f。将公式中的 vifxi 看作上面恒等式中的 ai，我们可以得到：</p>
<p><img src="/.io//image%258.png" alt="image.png"></p>
<p>现在，我们把对所有维度 f (从1到k) 的求和重新引入。将 ∑f&#x3D;1~k应用到整个表达式中：</p>
<p><img src="/.io//image%259.png" alt="image.png"></p>
<p>最后，我们将常数1&#x2F;2提到最外面，就得到了最终的化简公式。经过上述推导，FM二阶项的计算可以表示为：</p>
<p><img src="/.io//image%10.png" alt="image.png"></p>
<ul>
<li>ŷ &#x3D; w_0 + Σ(w_i * x_i) + ΣΣ(&lt;v_i, v_j&gt; * x_i * x_j)</li>
<li>这个公式清晰地展示了 FM 的两大部分：<ol>
<li><strong>一阶部分 <code>Σ(w_i * x_i)</code></strong>：与 LR 完全一样，负责建模每个特征的独立线性效应。</li>
<li><strong>二阶部分 <code>ΣΣ(&lt;v_i, v_j&gt; * x_i * x_j)</code></strong>：这是 FM 的核心创新，负责自动建模所有特征之间的两两交叉效应。</li>
</ol>
</li>
<li><strong>FM 如何解决稀疏问题？</strong><br>这正是因子分解的魔力所在。即使特征 <code>i</code> 和特征 <code>j</code> 在训练数据中从未同时出现过，模型也依然可以估算出它们的交叉强度。只要特征 <code>i</code> 与其他特征（如 <code>a</code>, <code>b</code>, <code>c</code>）有过共现，模型就能学出它的隐向量 <code>v_i</code>；同理也能学出 <code>v_j</code>。最终，对于未见过的组合 <code>(i, j)</code>，模型可以通过计算它们各自学到的隐向量的内积 <code>&lt;v_i, v_j&gt;</code> 来<strong>泛化</strong>和预测其交叉强度。</li>
<li><strong>总结</strong>：FM 通过引入<strong>隐向量</strong>和<strong>因子分解</strong>，从根本上解决了稀疏场景下的二阶特征交叉问题。它不仅能自动学习所有特征的两两组合，还能泛化到从未见过的组合上。这个“为每个特征学习一个低维稠密向量来表达其潜在特性”的思想，成为了后续所有深度学习推荐模型的基石。</li>
</ul>
<p><strong>复杂度分析</strong>：</p>
<ol>
<li>我们来看括号内的计算。对于每一个维度 f，我们需要先计算 ∑i&#x3D;1nvifxi。这个求和需要遍历 n 个特征，其复杂度为 O(n)。（对于稀疏特征，只需遍历非零项，复杂度更低）。</li>
<li>这个计算需要对 f 从 1 到 k 循环 k 次。</li>
<li>在循环的每一步中，计算平方、减法等都是常数时间操作。</li>
</ol>
<h3 id="2-3-核心概念：理解“阶”的含义（一阶与二阶）"><a href="#2-3-核心概念：理解“阶”的含义（一阶与二阶）" class="headerlink" title="2.3 核心概念：理解“阶”的含义（一阶与二阶）"></a>2.3 核心概念：理解“阶”的含义（一阶与二阶）</h3><p>在特征交叉的讨论中，“阶” (Order) 是一个核心术语，它精确地定义了特征之间交互的复杂度。我们可以将排序模型的预测公式类比为一个大的多项式方程，其中的变量就是模型的输入特征 xi。</p>
<ul>
<li><strong>一阶 (First-Order)</strong><ul>
<li><strong>数学形式</strong>: wixi</li>
<li><strong>定义</strong>: 一个特征项中，只包含<strong>一个</strong>独立的原始特征变量。</li>
<li><strong>意义</strong>: 建模<strong>单个特征的独立、线性效应</strong>。它反映了该特征本身对最终结果的贡献大小，而不考虑它与其他特征的任何组合。例如，在预测商品购买概率时，价格 xprice 越高，购买概率可能越低，一阶项 wpricexprice 就负责捕捉这种直接的、线性的关系。逻辑回归以及FM的线性部分，都是由一阶项构成的。</li>
</ul>
</li>
<li><strong>二阶 (Second-Order)</strong><ul>
<li><strong>数学形式</strong>: wij xi xj</li>
<li><strong>定义</strong>: 一个特征项中，包含<strong>两个</strong>独立的原始特征变量的乘积。</li>
<li><strong>意义</strong>: 建模<strong>两个特征之间的交互效应 (Interaction Effect)</strong> 或<strong>交叉效应 (Crossing Effect)</strong>。它捕捉的是当两个特征<strong>同时出现</strong>时，所产生的 <code>1+1 ≠ 2</code> 的特殊效果。例如，特征 <code>user_gender=女性</code> (xi) 和特征 <code>item_category=美妆</code> (xj)，它们各自的一阶效应可能并不突出，但当它们同时出现时（即 xixj&#x3D;1），会产生一个非常强烈的正向购买信号。这个强烈的信号就是由二阶项捕捉的。FM模型的核心创新，就是高效地对所有特征进行二阶交叉建模。</li>
</ul>
</li>
</ul>
<h3 id="2-4-核心概念：Embedding-——-将万物向量化"><a href="#2-4-核心概念：Embedding-——-将万物向量化" class="headerlink" title="2.4 核心概念：Embedding —— 将万物向量化"></a>2.4 核心概念：Embedding —— 将万物向量化</h3><p>如果说特征交叉是排序模型的核心“战术”，那么 <strong>Embedding</strong> 则是驱动这一切的、最核心的“技术引擎”。它是连接现实世界中离散符号（如ID）与神经网络数学世界的桥梁。</p>
<p><strong>1. 为什么需要 Embedding？—— One-Hot编码的局限</strong></p>
<p>我们之前提到，处理 <code>user_id</code>, <code>item_id</code> 这类类别特征的标准方法是<strong>独热编码 (One-Hot Encoding)</strong>。这种方法虽然解决了类别间无序性的问题，但它有两个致命缺陷：</p>
<ul>
<li><strong>维度灾难与稀疏性</strong>：当类别数量巨大时（如上亿的物品ID），独热编码会产生一个维度高达上亿的、并且只有一个位置是1的极其稀疏的向量。这在计算和存储上都是巨大的挑战。</li>
<li><strong>无法表达语义相似性</strong>：更重要的是，任意两个不同类别的独热向量都是<strong>正交</strong>的，它们的内积为0。这意味着模型无法从向量层面得知 <code>iPhone 17</code> 和 <code>iPhone 17 Pro</code> 是相似的，也无法知道 <code>iPhone 17</code> 和 <code>牛肉干</code> 是不相似的。这严重限制了模型的泛化能力。</li>
</ul>
<p><strong>2. Embedding 的解决方案：从稀疏到稠密</strong></p>
<p>Embedding 技术通过一个“<strong>查询表 (Lookup Table)</strong>”的思路，将高维稀疏的独热向量，映射成一个低维稠密的浮点数向量。</p>
<ul>
<li><strong>定义</strong>：一个 Embedding 是一个<strong>可学习的</strong>、<strong>低维的</strong>、<strong>稠密的</strong>向量表示。</li>
<li><strong>工作原理</strong>：<ol>
<li>我们可以想象有一个巨大的查询矩阵，我们称之为 <strong>Embedding 矩阵</strong>。这个矩阵的行数等于该特征的词汇表大小（如总物品数），列数等于我们设定的 Embedding 维度 k（如 k&#x3D;64）。</li>
<li>每一个物品ID，都对应这个矩阵中的<strong>一行</strong>。这一行就是一个 k 维的稠密向量。</li>
<li>在模型训练开始时，这个矩阵的数值是随机初始化的。在训练过程中，模型通过反向传播算法，不断地学习和调整矩阵中的数值。</li>
</ol>
</li>
</ul>
<p><strong>3. Embedding 的核心价值</strong></p>
<ul>
<li><strong>降维与信息压缩</strong>：将千万甚至亿级别的稀疏维度，压缩到几十或几百维的稠密向量中，极大地提升了计算和存储效率。</li>
<li><strong>捕捉语义相似性（最关键）</strong>：在训练过程中，那些经常被相似用户群体互动、或者本身属性相似的物品，它们的 Embedding 向量在向量空间中的位置会变得越来越<strong>接近</strong>。最终，“iPhone 17”和“iPhone 17 Pro”的向量会很相似，而它们与“牛肉干”的向量会相距很远。Embedding 成功地<strong>将行为和内容上的相似性，转化为了空间几何上的相近性</strong>。</li>
<li><strong>赋能泛化</strong>：正是因为捕捉到了语义相似性，模型才能实现泛化。模型从“iPhone 17”上学到的知识（比如购买它的用户画像），可以很自然地迁移应用到“iPhone 17 Pro”上，因为它俩的向量表示很接近。这是独热编码完全无法做到的。</li>
</ul>
<h3 id="2-5-核心概念：防止过拟合的利器-——-正则化-L1-vs-L2"><a href="#2-5-核心概念：防止过拟合的利器-——-正则化-L1-vs-L2" class="headerlink" title="2.5 核心概念：防止过拟合的利器 —— 正则化 (L1 vs L2)"></a>2.5 核心概念：防止过拟合的利器 —— 正则化 (L1 vs L2)</h3><p>在模型训练中，我们经常会遇到一个棘手的问题，叫做<strong>过拟合 (Overfitting)</strong>。</p>
<ul>
<li><strong>什么是过拟合？</strong><br>简单来说，就是模型“太努力”了，它在训练数据上表现得近乎完美，但对于从未见过的新数据（测试数据），表现却一塌糊涂。这就像一个只会“死记硬背”的学生，他能完美回答所有练习册上的原题，但一到真正的考试，遇到新题型就完全不会了。</li>
<li><strong>为什么会过拟合？</strong><br>通常是因为模型过于复杂，而训练数据又不足以支撑这种复杂性。模型不仅学习到了数据中普适的“规律”，还把训练数据特有的“噪声”和“偶然性”也当作规律记了下来。这在模型参数上的体现，往往是某些特征的权重（wi）变得<strong>异常巨大</strong>。</li>
</ul>
<p><strong>正则化 (Regularization)</strong> 就是一类专门用来对抗过拟合、提升模型<strong>泛化能力</strong>的技术。其核心思想是在模型的原始损失函数（如对数损失）的基础上，增加一个<strong>惩罚项 (Penalty Term)</strong>，以此来“惩罚”或“约束”模型的权重，不让它们变得过大。</p>
<p><strong><code>*新的损失函数=原始损失函数+惩罚项*</code></strong></p>
<p>模型在优化时，就必须在“降低原始损失（拟合数据）”和“降低惩罚项（保持权重简单）”之间做出权衡。最常见的两种正则化方法就是 L1 和 L2 正则化。</p>
<p><strong>1. L1 正则化 (Lasso Regression)</strong></p>
<ul>
<li><p><strong>惩罚项公式</strong>：L1 正则化的惩罚项是所有模型权重 wi 的<strong>绝对值之和</strong>，再乘以一个超参数 λ。</p>
<p>  <img src="/.io//image%11.png" alt="image.png"></p>
<p>  这里的 λ 控制着正则化的强度，λ 越大，惩罚越重。</p>
</li>
<li><p><strong>核心效果：稀疏性 (Sparsity)</strong><br>L1 正则化最显著的特点是它能够产生<strong>稀疏解</strong>，也就是说，它会倾向于将许多<strong>不那么重要的特征的权重直接置为精准的 0</strong>。</p>
</li>
<li><p><strong>为什么能产生稀疏解？</strong><br>从优化的角度看，L1 惩罚项的“菱形”约束边界使得损失函数在优化时，最优解更容易出现在坐标轴的顶点上，而顶点处恰好意味着某些特征的权重为0。</p>
</li>
<li><p><strong>主要作用</strong>：由于 L1 正则化能将权重归零，它也常被用作一种<strong>自动的特征选择 (Feature Selection)</strong> 方法。在拥有海量特征，但怀疑其中很多特征是无关紧要的场景下，L1 正则化非常有用。</p>
</li>
</ul>
<p><strong>2. L2 正则化 (Ridge Regression)</strong></p>
<ul>
<li><p><strong>惩罚项公式</strong>：L2 正则化的惩罚项是所有模型权重 wi 的<strong>平方和</strong>，再乘以超参数 λ。</p>
<p>  <img src="/.io//image%12.png" alt="image.png"></p>
</li>
<li><p><strong>核心效果：权重衰减 (Weight Decay)</strong><br>L2 正则化最显著的特点是它会使得模型的权重<strong>整体上都变得比较小，趋近于0，但很难精确等于0</strong>。</p>
</li>
<li><p><strong>为什么不会归零？</strong><br>L2 惩罚项的“圆形”约束边界比较平滑，最优解很难恰好落在坐标轴上。从梯度上看，L2 惩罚项的梯度与权重大小成正比，当权重越接近0时，它产生的惩罚梯度也越小，因此很难将权重“推到”绝对的0点。</p>
</li>
<li><p><strong>主要作用</strong>：L2 正则化是<strong>应用最广泛</strong>的正则化方法。它通过惩罚较大的权重，使得模型的决策不再过分依赖于少数几个特征，而是让所有特征都贡献一点力量，从而使得模型更加<strong>平滑</strong>和<strong>鲁棒</strong>，拥有更好的泛化能力。在深度学习中，它几乎是默认的正则化选项。</p>
</li>
</ul>
<p><strong>总结对比</strong></p>
<table>
<thead>
<tr>
<th>对比维度</th>
<th><strong>L1 正则化 (Lasso)</strong></th>
<th><strong>L2 正则化 (Ridge)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>惩罚公式</strong></td>
<td>λsum(w_i)</td>
<td>λsum(w_i^2)</td>
</tr>
<tr>
<td><strong>对权重的影响</strong></td>
<td>倾向于将部分权重<strong>精确地置为 0</strong></td>
<td>倾向于让所有权重都<strong>变得很小，趋近于 0</strong></td>
</tr>
<tr>
<td><strong>核心特性</strong></td>
<td><strong>稀疏性 (Sparsity)</strong></td>
<td><strong>权重衰减 (Weight Decay)</strong></td>
</tr>
<tr>
<td><strong>主要应用场景</strong></td>
<td><strong>特征选择</strong>，创造稀疏模型</td>
<td><strong>通用的过拟合防治</strong>，提升模型泛化能力</td>
</tr>
<tr>
<td><strong>几何约束</strong></td>
<td>菱形或更高维度的多面体</td>
<td>圆形或更高维度的超球面</td>
</tr>
</tbody></table>
<h2 id="第三章：深度学习排序模型的“三驾马车”-The-“Big-Three”-of-Deep-Ranking-Models"><a href="#第三章：深度学习排序模型的“三驾马车”-The-“Big-Three”-of-Deep-Ranking-Models" class="headerlink" title="第三章：深度学习排序模型的“三驾马车” (The “Big Three” of Deep Ranking Models)"></a><strong>第三章：深度学习排序模型的“三驾马车” (The “Big Three” of Deep Ranking Models)</strong></h2><h3 id="3-1-Wide-Deep-“记忆”与“泛化”的首次结合"><a href="#3-1-Wide-Deep-“记忆”与“泛化”的首次结合" class="headerlink" title="3.1 Wide &amp; Deep: “记忆”与“泛化”的首次结合"></a>3.1 Wide &amp; Deep: “记忆”与“泛化”的首次结合</h3><p>由 Google 在2016年提出的 <strong>Wide &amp; Deep</strong> 模型，是深度学习在推荐系统领域应用的开创性工作。它并未盲目地用一个“大而全”的深度网络去解决所有问题，而是深刻地洞察到推荐系统需要同时具备两种看似矛盾的能力：<strong>记忆（Memorization）和泛化（Generalization）</strong>。</p>
<ul>
<li><strong>记忆能力 (Memorization)</strong><ul>
<li><strong>定义</strong>：指模型学习并利用历史数据中频繁共现的、非常具体和直接的规则的能力。它善于“记住”那些显而易见的、强相关的模式。</li>
<li><strong>例子</strong>：模型能够牢牢记住“<strong>用户下载过A应用，又看到了A应用的辅助工具B，那么该用户点击B的概率极高</strong>”这样的强关联规则。</li>
<li><strong>实现</strong>：线性模型（如逻辑回归）是实现“记忆”的绝佳工具，因为它能为每一个具体的特征组合学习一个独立的权重。</li>
</ul>
</li>
<li><strong>泛化能力 (Generalization)</strong><ul>
<li><strong>定义</strong>：指模型探索和发现数据中稀疏的、从未见过的、更“抽象”的模式的能力。它善于“泛化”知识，从而提高推荐的多样性和惊喜度。</li>
<li><strong>例子</strong>：模型通过学习大量的用户行为，可能会发现“<strong>喜欢看科幻电影的用户</strong>”和“<strong>喜欢购买硬核科技产品的用户</strong>”在底层兴趣上存在关联，从而为一个科幻迷推荐一款他从未见过的新款无人机。</li>
<li><strong>实现</strong>：深度神经网络（DNN）通过将高维稀疏特征映射到低维稠密的 <strong>Embedding</strong> 向量中，非常擅长发现这种潜在的、语义层面的关联。</li>
</ul>
</li>
</ul>
<p><strong>Wide &amp; Deep 的核心架构</strong></p>
<p>为了将这两种能力有机地结合起来，Wide &amp; Deep 设计了一种优雅的<strong>并行结构</strong>，由 <strong>Wide</strong> 和 <strong>Deep</strong> 两个部分组成，并进行<strong>联合训练 (Joint Training)</strong>。</p>
<ol>
<li><strong>Wide 部分 (负责记忆)</strong><ul>
<li><strong>结构</strong>：一个简单的广义线性模型，等价于一个逻辑回归。</li>
<li><strong>输入</strong>：<strong>人工设计的、高维稀疏的交叉特征</strong>。这是 Wide 部分的关键，其效果高度依赖算法工程师的经验来手动创造出有意义的特征组合，例如 <code>AND(gender=&#39;female&#39;, item_category=&#39;makeup&#39;)</code>。</li>
</ul>
</li>
<li><strong>Deep 部分 (负责泛化)</strong><ul>
<li><strong>结构</strong>：一个标准的前馈神经网络 (DNN&#x2F;MLP)。</li>
<li><strong>输入</strong>：原始的连续特征，以及被转换成低维稠密 <strong>Embedding</strong> 向量的类别特征（如 <code>user_id</code>, <code>item_id</code>）。</li>
</ul>
</li>
<li><strong>输出与联合训练</strong><ul>
<li><p>模型的最终输出由 Wide 部分的输出与 Deep 部分的输出<strong>相加</strong>，然后共同经过一个 Sigmoid 函数得到。其简化后的公式可以理解为：</p>
<p>  <img src="/.io//image%13.png" alt="image.png"></p>
</li>
<li><p>整个模型是<strong>端到端 (End-to-End)</strong> 训练的。最终的损失梯度会同时反向传播到 Wide 和 Deep 两个部分，并同时更新两边的参数。这比 GBDT+LR 的两阶段训练模式更为先进和统一。</p>
</li>
</ul>
</li>
</ol>
<p>损失函数：BCELOSS</p>
<p><img src="/.io//image%14.png" alt="image.png"></p>
<ul>
<li>N 是批次大小 (batch size)。</li>
<li>yi 是第 i 个样本的真实标签（点击为1，未点击为0）。</li>
<li>y^i 是模型对第 i 个样本<strong>最终预测</strong>的点击概率。</li>
</ul>
<p>y_hat &#x3D; sigmoid(z_deep+z_DNN+b), z_wide&#x3D;w_wide x_wide (LR的线性部分), z_deep&#x3D;DNN(x_deep), x_deep是经过embedding的稠密向量。得到预测的y_hat后在BCE中和gt作比较。</p>
<p><strong>打个比方</strong>：<br>整个训练过程就像一个<strong>联合项目</strong>。</p>
<ol>
<li><strong>项目交付 (前向传播)</strong>：Wide团队和Deep团队各自完成自己的部分，最后将成果汇总，交付一个最终产品（预测值 y^）。</li>
<li><strong>客户反馈 (计算损失)</strong>：根据产品的好坏（与真实标签 y 的差距），得到一个总的项目评分（损失 L）。</li>
<li><strong>责任分配 (反向传播)</strong>：项目经理（反向传播算法）根据总评分，分析出两个团队各自的贡献和不足，并给出具体的改进意见（梯度）。</li>
<li><strong>内部优化 (权重更新)</strong>：<ul>
<li>Wide团队的经理（FTRL优化器）拿到意见后，采用“末位淘汰”式的管理方法，将表现差的员工（权重）直接“开除”（置为0）。</li>
<li>Deep团队的经理（Adam优化器）拿到意见后，采用“个性化辅导”的管理方法，根据每个员工的历史表现，动态调整对他的培养力度（自适应学习率）。</li>
</ul>
</li>
</ol>
<h3 id="3-2-DeepFM-FM与DNN的无缝集成"><a href="#3-2-DeepFM-FM与DNN的无缝集成" class="headerlink" title="3.2 DeepFM: FM与DNN的无缝集成"></a>3.2 DeepFM: FM与DNN的无缝集成</h3><p>DeepFM (2017) 的诞生，旨在解决其两大“前辈”——<strong>Wide &amp; Deep</strong> 和 <strong>FNN</strong>——各自的核心缺陷，从而打造一个既不需要人工特征工程，又能端到端训练，同时高效学习所有阶特征交叉的强大模型。</p>
<p><strong>1. DeepFM 的动机：站在巨人的肩膀上解决问题</strong></p>
<p>为了理解 DeepFM 的精妙之处，我们必须先看清它要解决的问题：</p>
<ul>
<li><strong>Wide &amp; Deep 的痛点</strong>：其 <strong>Wide</strong> 部分虽然能够很好地“记忆”低阶交叉特征，但这些交叉特征需要<strong>依赖算法工程师手动设计</strong>。这是一个巨大的瓶颈，费时费力且高度依赖经验，无法穷举所有有用的组合。</li>
<li><strong>FNN 的痛点</strong>：FNN (Factorization-machine supported Neural Network) 尝试用预训练的 FM 模型来初始化 DNN 的 Embedding，以期引入低阶交叉信息。但它的训练是<strong>两阶段的、非端到端的</strong>，这意味着 FM 学到的 Embedding 无法在 DNN 的训练中得到微调。同时，低阶特征信息在送入深层网络后，很容易被复杂的非线性变换所“稀释”，导致学习效率不高。</li>
</ul>
<p><strong>DeepFM 的核心目标</strong>：设计一个<strong>端到端</strong>的统一模型，它既能像 FM 一样自动学习低阶特征交叉，又能像 DNN 一样自动学习高阶特征交叉，并且完全<strong>不需要任何人工特征工程</strong>。</p>
<p><strong>2. DeepFM 的核心架构：“共享”与“并行”的艺术</strong></p>
<p>DeepFM 的结构设计堪称典范，它由并行的 <strong>FM 组件</strong>和 <strong>DNN 组件</strong>组成，而这两大组件的“无缝集成”，依赖于一个核心设计——<strong>共享 Embedding 层</strong>。</p>
<p><em><center>DeepFM 简化架构图</center></em></p>
<p><strong>a) 共享的基石：唯一的 Embedding 层</strong><br>这是 DeepFM 相对于 FNN 最关键的改进。模型中只有一个全局的 Embedding 矩阵。所有输入的稀疏类别特征（如 <code>user_id</code>, <code>item_id</code>）都会被映射到这个共享的 Embedding 空间中，得到各自的低维稠密向量。</p>
<p>这个共享的 Embedding 层，是连接 FM 和 DNN 两大组件的<strong>唯一纽带</strong>。</p>
<p><strong>b) FM 组件：低阶特征交叉的“自动”专家</strong><br>这个部分的作用等同于 Wide &amp; Deep 中的 “Wide” 部分，但它实现了完全的自动化。它直接在共享的 Embedding 向量上进行运算，同时负责两部分计算：</p>
<ul>
<li><strong>一阶特征 (1st-order Interactions)</strong>：对所有原始特征的线性加权求和，与逻辑回归完全相同。</li>
</ul>
<p><img src="/.io//image%15.png" alt="image.png"></p>
<p><strong>二阶特征 (2nd-order Interactions)</strong>：对每一对特征的 Embedding 向量进行<strong>内积 (Dot Product)</strong> 运算，以此来建模所有特征的两两交叉。</p>
<p><img src="/.io//image%16.png" alt="image.png"></p>
<p>这里的v_i 和v_j 正是从共享 Embedding 层中查询得到的向量。</p>
<p><strong>c) DNN 组件：高阶特征交叉的“黑盒”大师</strong><br>这个部分负责探索那些更复杂、更抽象的高阶非线性关系。</p>
<ul>
<li><strong>输入</strong>：将所有稀疏特征从<strong>共享 Embedding 层</strong>中查到的 Embedding 向量，与原始的稠密特征（如有）进行<strong>拼接 (Concatenate)</strong>，形成一个长长的、稠密的向量。</li>
<li><strong>结构</strong>：一个标准的多层感知机 (MLP)，包含若干个全连接层和非线性激活函数（如 ReLU）。</li>
</ul>
<p><img src="/.io//image%17.png" alt="image.png"></p>
<ul>
<li><strong>输出</strong>：经过多层变换后，最终输出一个代表了所有高阶交叉信息的 Logit。</li>
</ul>
<p><strong>d) 最终的“无缝”集成</strong></p>
<p>DeepFM 的集成方式极其简单和高效：将 FM 组件的输出（包含一阶和二阶）和 DNN 组件的输出</p>
<p><strong>直接相加</strong>，然后将总和送入一个 Sigmoid 函数，得到最终的 CTR 预测值。</p>
<p><img src="/.io//image%18.png" alt="image.png"></p>
<p>这种简单的相加操作，意味着两个组件在以一种<strong>并行、互补</strong>的方式工作，共同对最终的预测结果做出贡献。</p>
<p><strong>3. DeepFM 的突破性优势</strong></p>
<ul>
<li><strong>真正的端到端学习</strong>：得益于共享 Embedding，梯度可以从最终的损失函数，无阻碍地同时反向传播到 FM 和 DNN 两个部分，并最终<strong>共同更新同一份 Embedding 参数</strong>。这意味着 Embedding 的学习同时受到了来自“低阶交叉任务”和“高阶交叉任务”的双重指导，使其表示能力更强、学习更充分。</li>
<li><strong>完全无需人工特征工程</strong>：FM 组件完美地替代了 Wide &amp; Deep 中需要手动设计的 Wide 部分，实现了所有低阶交叉的自动化。</li>
<li><strong>兼具效率与性能</strong>：模型中不包含任何复杂的特殊结构，FM 部分有线性时间复杂度的解法，DNN 部分也是标准的 MLP，整个模型的训练和预测效率都非常高，非常适合工业界大规模部署。</li>
</ul>
<p><strong>总结：</strong><br>DeepFM 之所以是“重中之重”，因为它<strong>用最简洁、最优雅的工程设计，完美地解决了前代模型的关键痛点</strong>。它通过“共享 Embedding + 并行结构”这一核心思想，将 FM 对低阶特征交叉的精准刻画能力，与 DNN 对高阶特征交叉的强大泛化能力无缝地结合在了一个可以端到端训练的统一框架下，并且完全实现了自动化。这使其成为了后续非序列深度学习排序模型的一个<strong>黄金标准和核心范式</strong>。</p>
<h3 id="3-3-DCN-xDeepFM-对“显式特征交叉”的极致探索"><a href="#3-3-DCN-xDeepFM-对“显式特征交叉”的极致探索" class="headerlink" title="3.3 DCN &amp; xDeepFM: 对“显式特征交叉”的极致探索"></a>3.3 DCN &amp; xDeepFM: 对“显式特征交叉”的极致探索</h3><p><strong>1. 动机：打开 DNN 的“黑箱”</strong></p>
<p>DeepFM 的 DNN 部分虽然强大，但它学习高阶特征交叉的过程是**“隐式”的 (Implicit)**。也就是说，特征交叉是通过多层复杂的、不可控的非线性变换自动发生的，我们很难知道模型具体学习到了哪些阶数的、哪些有效的交叉组合。这种“黑箱”式的学习可能有以下缺点：</p>
<ul>
<li><strong>效率不高</strong>：可能需要大量的数据和参数才能学习到一些相对简单的交叉关系。</li>
<li><strong>不可控</strong>：我们无法控制模型学习交叉的阶数。</li>
</ul>
<p>因此，一个新的研究方向应运而生：我们能否设计一种<strong>全新的网络结构</strong>，它既能<strong>自动</strong>学习高阶交叉，又能让这个过程变得<strong>显式 (Explicit)</strong> 和<strong>高效可控</strong>？DCN 和 xDeepFM 就是这个方向上最重要的两个探索者。</p>
<hr>
<p><strong>2. DCN (Deep &amp; Cross Network, 2017): 显式交叉的开创者</strong></p>
<p>DCN 的架构直接脱胎于 Wide &amp; Deep，它保留了并行的 Deep 部分，但用一个创新设计的 <strong>Cross Network</strong> 替换了需要人工特征工程的 Wide 部分。</p>
<ul>
<li><p><strong>核心组件：Cross Network</strong></p>
<ul>
<li><strong>目标</strong>：以一种高效、可控的方式，让特征交叉的阶数随着网络层数的增加而自动增加。</li>
<li><strong>核心公式</strong>：Cross Network 的每一层都遵循一个独特的更新规则：</li>
</ul>
<p>  <img src="/.io//image%19.png" alt="image.png"></p>
</li>
</ul>
<p>我们来拆解这个公式：</p>
<ul>
<li>xl 是第 l 层的输出向量。</li>
<li>x0 是整个网络最开始的<strong>原始输入向量</strong>（通常是所有特征 Embedding 的拼接）。</li>
<li><code>x0xlTwl</code> 是最关键的<strong>交叉操作</strong>。它将上一层的输出 xl 与原始输入 x0 进行了一次显式的交叉，并由一个可学习的权重向量 wl 来控制交叉的模式。</li>
<li>+xl 是一个<strong>残差连接 (Residual Connection)</strong>。这是一个点睛之笔，它保证了在学习更高阶交叉的同时，所有在前面层已经学到的低阶交叉信息都能被完整地保留下来，极大地稳定了训练过程。</li>
</ul>
<p><strong>工作原理</strong>：</p>
<ul>
<li><strong>第1层</strong>：x0 与自身进行交叉，产出所有特征的<strong>二阶</strong>交叉信息。</li>
<li><strong>第2层</strong>：x0 与包含了二阶信息的 x1 进行交叉，产出<strong>三阶</strong>交叉信息。</li>
<li>以此类推，一个深度为 L 的 Cross Network 能够显式地建模所有从一阶到 L+1 阶的特征交叉。</li>
</ul>
<p><strong>DCN 的局限</strong>：<br>DCN 的交叉是所谓的 <strong>bit-wise (比特级&#x2F;元素级)</strong>。交叉项 <code>x_l^T w_l</code> 的结果是一个标量（单个数值），这个标量再去乘以整个原始输入向量 x0。这种方式虽然实现了交叉，但表达能力有限，不够灵活。</p>
<p><strong>3. xDeepFM (eXtreme DeepFM, 2018): 从 bit-wise 到 vector-wise 的进化</strong></p>
<p>xDeepFM 在 DCN 的思想上更进一步，它认为特征交叉应该发生在更有意义的<strong>向量级别 (vector-wise)</strong>，而不是比特级别。为此，它设计了一个全新的核心组件——<strong>CIN (Compressed Interaction Network)</strong>。</p>
<ul>
<li><strong>核心组件：CIN (压缩交互网络)</strong><ul>
<li><strong>目标</strong>：在 <strong>Embedding 向量</strong>的粒度上，显式地、逐层地构建高阶特征交叉。</li>
<li><strong>工作原理 (概念)</strong>：<ol>
<li><strong>输入</strong>：CIN 的输入不再是一个拍平的长向量，而是一个矩阵 X0，其每一行都是一个特征的 Embedding 向量。</li>
<li><strong>逐层交叉</strong>：CIN 的第 k 层，会计算第 k−1 层的输出矩阵 Xk−1 与原始输入矩阵 X0 之间，<strong>每一对 Embedding 向量</strong>的<strong>哈达玛积 (Element-wise Product, ⊙)</strong>。这就在向量层面生成了更高一阶的交叉特征。</li>
<li><strong>压缩</strong>：由于上述操作会产生巨量的交叉向量，CIN 会通过类似卷积的操作（使用一组可学习的滤波器）对这些交叉向量进行加权求和，将其“压缩”成一个更紧凑的矩阵 Xk，作为下一层的输入。</li>
</ol>
</li>
<li><strong>最终输出</strong>：xDeepFM 的完整架构通常包含三部分：<strong>传统的线性部分</strong>（负责一阶）、<strong>CIN 部分</strong>（负责显式的、vector-wise 的高阶交叉）和<strong>传统的 DNN 部分</strong>（负责隐式的、bit-wise 的高阶交叉），将三部分的输出融合后进行最终预测。</li>
</ul>
</li>
</ul>
<p><strong>总结与对比</strong></p>
<table>
<thead>
<tr>
<th>模型</th>
<th><strong>高阶交叉方式</strong></th>
<th><strong>显式&#x2F;隐式</strong></th>
<th><strong>交叉粒度</strong></th>
<th><strong>核心思想</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>DeepFM</strong></td>
<td>标准 DNN (MLP)</td>
<td><strong>隐式</strong></td>
<td>Bit-wise</td>
<td>通过多层非线性变换“黑箱”学习高阶关系。</td>
</tr>
<tr>
<td><strong>DCN</strong></td>
<td>Cross Network</td>
<td><strong>显式</strong></td>
<td>Bit-wise</td>
<td>通过独特的残差交叉结构，逐层、可控地增加交叉阶数。</td>
</tr>
<tr>
<td><strong>xDeepFM</strong></td>
<td>CIN (压缩交互网络)</td>
<td><strong>显式</strong></td>
<td><strong>Vector-wise</strong></td>
<td>在更有意义的 Embedding 向量粒度上，进行显式交叉。</td>
</tr>
</tbody></table>
<h3 id="3-4-核心概念：共享-Embedding-Shared-Embedding-的原理与作用"><a href="#3-4-核心概念：共享-Embedding-Shared-Embedding-的原理与作用" class="headerlink" title="3.4 核心概念：共享 Embedding (Shared Embedding) 的原理与作用"></a>3.4 核心概念：共享 Embedding (Shared Embedding) 的原理与作用</h3><p><strong>1. 原理：一份输入，多路更新</strong></p>
<p>共享 Embedding 指的是，模型中不同的部分（例如 DeepFM 中的 FM 组件和 DNN 组件）在处理同一个类别特征时，会查询和使用<strong>同一个、全局唯一的 Embedding 矩阵</strong>。</p>
<ul>
<li><p><strong>前向传播</strong>：当一个类别特征（如 <code>item_id=123</code>）输入时，模型从共享的 Embedding 矩阵中查出其对应的向量 v123。接着，这个向量的<strong>副本</strong>会同时被送往 FM 部分和 DNN 部分，参与各自的计算。</p>
</li>
<li><p><strong>反向传播（核心）</strong>：在计算完最终的损失后，梯度会从损失函数反向传播。FM 部分和 DNN 部分会各自计算出它们对向量 v123 的梯度贡献，我们称之为 ∇FM 和 ∇DNN。最终，作用于共享 Embedding 矩阵中那个唯一的 v123 向量的更新梯度，是<strong>所有部分梯度贡献的总和</strong>：</p>
<p>  ∇total&#x3D;∇FM+∇DNN</p>
<p>  这意味着，Embedding 向量的学习同时受到了“低阶交叉任务（来自FM）”和“高阶交叉任务（来自DNN）”的双重指导。</p>
</li>
</ul>
<p><strong>2. 作用：为何共享如此强大？</strong></p>
<ul>
<li><strong>参数高效</strong>：这是最直接的优势。模型只需要维护一份 Embedding 矩阵，而不是每个组件都维护一份，极大地减少了模型的总参数量，节省了内存，并加快了训练速度。</li>
<li><strong>学习更丰富的表示（最重要的优势）</strong>：这是共享 Embedding 的精髓所在。由于 Embedding 的更新同时接收来自不同模型组件的信号，它被迫学习一种能够<strong>同时满足多种需求</strong>的表示。FM 部分会“教”它学习显式的、低阶的共现关系；而 DNN 部分则会“教”它学习抽象的、高阶的非线性关系。最终学到的 Embedding 表示会比单一任务学到的更丰富、更鲁棒。</li>
<li><strong>加速收敛与提升泛化</strong>：更丰富的梯度信号有助于 Embedding 更快地学习到有意义的表示。同时，对于长尾的、交互稀疏的特征，即使 DNN 很难有效学习，FM 部分简单的共现信号依然可以对其进行有效的更新，从而提升了模型的泛化能力。</li>
<li><strong>实现端到端训练</strong>：共享 Embedding 是将多个独立模型组件“粘合”在一起，实现真正端到端联合训练的关键纽带。</li>
</ul>
<h3 id="3-5-核心概念：激活函数的作用与演进-ReLU-PReLU-Dice"><a href="#3-5-核心概念：激活函数的作用与演进-ReLU-PReLU-Dice" class="headerlink" title="3.5 核心概念：激活函数的作用与演进 (ReLU, PReLU, Dice)"></a>3.5 核心概念：激活函数的作用与演进 (ReLU, PReLU, Dice)</h3><p><strong>1. 激活函数的核心作用：引入非线性</strong></p>
<p>激活函数是神经网络的“灵魂”所在。如果没有激活函数，一个无论多深的神经网络，本质上都只是一系列线性变换的叠加，其最终效果等价于一个简单的线性模型。<strong>激活函数的核心作用就是引入非线性</strong>，它对每一层线性变换后的结果进行“弯曲”或“折叠”，从而赋予了网络拟合任意复杂函数的能力。</p>
<p><strong>2. 激活函数的演进之路</strong></p>
<p>在排序模型中，激活函数的演进主要围绕着如何让训练更稳定、更高效展开。</p>
<ul>
<li><p><strong>ReLU (Rectified Linear Unit)</strong></p>
<ul>
<li><strong>公式</strong>：<code>f(x)=max(0,x)</code></li>
<li><strong>地位</strong>：现代神经网络的<strong>默认标准激活函数</strong>。它计算极其简单，并且在输入为正数时梯度恒为1，极大地缓解了深度网络中的“梯度消失”问题。</li>
<li><strong>缺陷</strong>：存在**“死亡ReLU问题” (Dying ReLU Problem)**。如果一个神经元的输入在训练中持续为负，那么它的输出和梯度将永远是0，导致该神经元无法再进行学习。</li>
</ul>
</li>
<li><p><strong>PReLU (Parametric ReLU)</strong></p>
<ul>
<li><strong>公式</strong>：<code>f(s)=max(αs,s)</code>，其中 α 是一个可学习的参数。</li>
<li><strong>改进</strong>：PReLU 是对 ReLU 的直接改进。它通过引入一个可学习的、微小的负半轴斜率 α，实现了“泄漏 (Leaky)”。当输入为负时，梯度不再是0而是 α，从而<strong>解决了“神经元死亡”的问题</strong>。因为它比固定的 Leaky ReLU 更灵活，所以通常效果更好。</li>
</ul>
</li>
<li><p><strong>Dice (Data Adaptive Activation Function)</strong></p>
<ul>
<li><strong>动机</strong>：PReLU 解决了神经元死亡问题，但它的状态切换点依然是<strong>固定在0</strong>。在数据分布剧烈变化的工业级场景中（即“内部协变量偏移”问题），一个固定的切换点并非最优。</li>
<li><strong>核心思想</strong>：让激活函数的行为能够<strong>自适应数据的分布</strong>。</li>
</ul>
<p>  <img src="/.io//image%20.png" alt="image.png"></p>
<ul>
<li><strong>实现方式</strong>：Dice 可以被看作一个在 <code>f(s)=s</code> 和 <code>f(s)=αs</code> 之间的“软开关”。这个开关的位置，不再是固定的0点，而是通过一个基于<strong>批归一化 (Batch Normalization)</strong> 思想的门控函数，<strong>动态地调整到了当前这批 (mini-batch) 数据的均值 E[s] 处,而不是ReLU和PReLU的0处</strong>。</li>
</ul>
<p>  <img src="/.io//image!.png" alt="image.png"></p>
<ul>
<li><strong>意义</strong>：Dice 通过让激活函数的“激活点”主动去追随数据的分布中心，极大地提升了模型在非平稳数据分布下的训练稳定性和效率。它是为大规模、动态变化的推荐场景量身定制的、更“智能”的激活函数。</li>
</ul>
</li>
</ul>
<h2 id="第四章：用户行为序列建模时代-The-Era-of-User-Behavior-Sequence-Modeling"><a href="#第四章：用户行为序列建模时代-The-Era-of-User-Behavior-Sequence-Modeling" class="headerlink" title="第四章：用户行为序列建模时代 (The Era of User Behavior Sequence Modeling)"></a><strong>第四章：用户行为序列建模时代 (The Era of User Behavior Sequence Modeling)</strong></h2><h3 id="4-1-DIN-Deep-Interest-Network-开启注意力机制的应用"><a href="#4-1-DIN-Deep-Interest-Network-开启注意力机制的应用" class="headerlink" title="4.1 DIN (Deep Interest Network): 开启注意力机制的应用"></a>4.1 DIN (Deep Interest Network): 开启注意力机制的应用</h3><p>在 DIN (2018) 诞生之前，深度学习排序模型（如 Wide &amp; Deep, DeepFM）在处理用户历史行为序列时，普遍采用一种简单粗暴的方式：<strong>简单池化 (Pooling)</strong>。</p>
<p><strong>1. DIN 的动机：挑战“兴趣大杂烩”</strong></p>
<ul>
<li><strong>当时的主流做法</strong>：将用户历史上交互过的所有物品（比如点击过的50个商品）的 Embedding 向量取出来，然后通过一个池化操作（如 <code>Average Pooling</code> 或 <code>Max Pooling</code>），将它们压缩成一个<strong>固定长度的向量</strong>，以此作为代表该用户“兴趣”的唯一输入。</li>
<li><strong>这种做法的致命缺陷</strong>：它将用户丰富、多样、且在不同场景下关注点不同的兴趣，粗暴地“平均”成了一个大杂烩。</li>
<li><strong>一个经典的例子</strong>：一位年轻的母亲，她的购物历史里既有大量的“连衣裙”、“高跟鞋”、“手提包”，也有“婴儿纸尿裤”和“奶粉”。<ul>
<li>当系统要向她推荐一款新的**“手提包”<strong>时，一个经过平均池化的兴趣向量，会因为包含了大量的母婴用品信息而变得“面目模糊”。“母婴”兴趣的信号会严重</strong>稀释和干扰**她此刻对“时尚”的兴趣表达。</li>
</ul>
</li>
<li><strong>DIN 的核心洞察</strong>：用户的兴趣表示，不应该是一个一成不变的静态向量，而应该是<strong>动态的、自适应的</strong>。它应该<strong>根据当前要预测的候选商品（Target Item）的不同而变化</strong>。</li>
</ul>
<p><strong>2. DIN 的核心架构：目标感知的注意力机制</strong></p>
<p>为了实现上述目标，DIN 首次将<strong>注意力机制 (Attention Mechanism)</strong> 引入了 CTR 预估领域，设计了一套全新的用户兴趣表示方法。</p>
<p><strong>a) 激活单元 (Activation Unit) - 注意力网络</strong><br>DIN 的核心是一个小型的神经网络，即“激活单元”，它负责计算<strong>每一个历史行为</strong>与<strong>当前候选商品</strong>之间的“相关性得分”，也就是<strong>注意力权重</strong>。</p>
<ul>
<li><strong>输入</strong>：对于用户历史中的第 i 个商品，激活单元的输入包括：<ol>
<li>历史商品 i 的 Embedding 向量 ei。</li>
<li>候选商品（广告） a 的 Embedding 向量 ea。</li>
<li>两者的交互特征：为了让模型更好地学习它们的关联性，通常还会把它们的<strong>差值</strong> <code>(ea−ei)</code> 和<strong>元素积</strong> <code>(ea⊙ei)</code> 也作为输入。</li>
</ol>
</li>
<li><strong>结构</strong>：将上述所有向量拼接起来，送入一个浅层的 MLP (多层感知机)。</li>
<li><strong>输出</strong>：MLP 最终输出一个标量（单个数值）wi，这个值就代表了历史行为 i 在当前候选商品 a “在场”的情况下的注意力得分。</li>
</ul>
<p><strong>b) 加权求和：构建动态兴趣向量</strong><br>在为所有 N 个历史行为都计算出对应的注意力得分 w1,w2,…,wN 之后，模型通过<strong>加权求和</strong>的方式，来构建最终的用户兴趣向量 vU：</p>
<p><img src="/.io//image%22.png" alt="image.png"></p>
<p>这个最终的兴趣向量 vU 是<strong>动态的</strong>。当候选商品变化时，注意力得分会重新计算，vU 也会随之改变，从而精准地反映出用户在此情此景下的特定兴趣。</p>
<p><strong>c) 关键创新：摒弃 Softmax</strong><br>传统的注意力机制通常会在最后使用 <code>Softmax</code> 函数将所有权重归一化，使其和为1。DIN 创新地<strong>摒弃了 Softmax</strong>。</p>
<ul>
<li><strong>原因</strong>：Softmax 会“拉平”兴趣的强度。一个在某个领域有100次点击的用户和一个只有10次点击的用户，他们的兴趣强度是截然不同的。Softmax 会将这种绝对强度的信息抹去，只保留相对大小。</li>
<li><strong>好处</strong>：不使用 Softmax，直接使用注意力得分进行加权，可以<strong>保留用户兴趣的绝对强度</strong>。如果用户是某领域的狂热粉丝，其加权后的兴趣向量模长就会更大，这本身就是一个非常强的信号，可以被后续的 DNN 有效捕捉。</li>
</ul>
<p><strong>3. DIN 的其他重要贡献</strong></p>
<p>除了注意力机制，DIN 论文还贡献了两个非常实用的工程技术，以保证模型在工业级海量数据下的训练效果和效率：</p>
<ul>
<li><strong>Dice 激活函数</strong>：一种数据自适应的激活函数，能够根据每个 mini-batch 的数据分布动态调整其激活点，以应对工业数据分布剧烈变化的挑战，使训练更稳定。</li>
<li><strong>小批量感知正则化 (Mini-batch Aware Regularization)</strong>：一种针对海量稀疏特征（如 <code>item_id</code>）的正则化优化方法。它只在每个批次中，对当次出现的特征的 Embedding 进行正则化计算，极大地降低了计算开销。</li>
</ul>
<p><strong>总结：</strong><br>DIN 是深度学习推荐系统发展史上的一个分水岭。它通过引入<strong>目标感知的注意力机制</strong>，将用户兴趣的建模方式从“静态的、无差别的兴趣平均”，带入了**“动态的、上下文相关的兴趣聚焦”<strong>的新时代。DIN 不仅是一个具体的模型，它所开创的“根据目标物品来动态计算用户兴趣”这一</strong>范式**，被后续几乎所有的序列推荐模型（如 DIEN, SIM）所继承和发展，影响至今。</p>
<h3 id="4-2-DIEN-Deep-Interest-Evolution-Network-捕捉兴趣的“进化”轨迹"><a href="#4-2-DIEN-Deep-Interest-Evolution-Network-捕捉兴趣的“进化”轨迹" class="headerlink" title="4.2 DIEN (Deep Interest Evolution Network): 捕捉兴趣的“进化”轨迹"></a>4.2 DIEN (Deep Interest Evolution Network): 捕捉兴趣的“进化”轨迹</h3><p><strong>1. 动机：从“兴趣是什么”到“兴趣将去向何方”</strong></p>
<ul>
<li><strong>DIN 的成就</strong>：DIN brilliantly地回答了这个问题：“考虑到用户要看的是<strong>这个</strong>商品，他过去的<strong>哪些</strong>兴趣是相关的？” 它捕捉了兴趣的<strong>多样性</strong>。</li>
<li><strong>DIN 的局限</strong>：DIN 将用户的历史行为视为一个无序的集合，完全忽略了<strong>时间顺序</strong>。它无法区分一个兴趣是正在快速增长，还是早已衰退。例如，一个用户上周买了相机、昨天买了镜头，和一个用户一年前买了相机、此后再无动作，DIN 对这两种情况的理解是相似的。但前者明显表现出对“摄影”这个兴趣的<strong>演进和增强</strong>。</li>
<li><strong>DIEN 的目标</strong>：DIEN 的核心目标，就是要<strong>显式地对用户兴趣的演进轨迹进行建模</strong>。它不仅要捕捉兴趣的相关性，更要捕捉兴趣的动态变化过程，从而预测兴趣的“最终状态”，用这个“最终状态”来进行更精准的推荐。</li>
</ul>
<p><strong>2. DIEN 的核心架构：两阶段 RNN</strong></p>
<p>为了对“进化”这一时序过程进行建模，DIEN 创新地设计了一个两阶段的循环神经网络 (RNN) 架构。</p>
<p><strong>第一阶段：兴趣提取层 (Interest Extractor Layer)</strong></p>
<ul>
<li><strong>目标</strong>：从用户原始的行为序列（一串物品ID）中，学习并提取出抽象的、潜在的“兴趣状态”序列。</li>
<li><strong>实现</strong>：使用一个 <strong>GRU (Gated Recurrent Unit)</strong> 网络。<ul>
<li>将用户的历史行为 Embedding 按照时间顺序，依次输入到 GRU 中。</li>
<li>GRU 在每个时间步 <code>t</code> 都会输出一个隐藏状态 <code>h_t</code>。这个 <code>h_t</code> 融合了 <code>t</code> 时刻及之前所有的行为信息，可以被看作是用户在该时刻的“潜在兴趣状态”向量。</li>
</ul>
</li>
</ul>
<p><strong>第二阶段：兴趣进化层 (Interest Evolving Layer) - [DIEN 的核心创新]</strong></p>
<ul>
<li><strong>目标</strong>：在第一阶段生成的“兴趣状态”序列 <code>h_1, h_2, ..., h_T</code> 的基础上，根据<strong>当前候选商品（Target Item）</strong>，抽取出最相关的一条<strong>兴趣进化链</strong>，并模拟其演进过程。</li>
<li><strong>关键设计：AUGRU (GRU with Attention Update Gate)</strong><br>DIEN 的作者认为，并非所有的兴趣进化都与当前推荐任务相关。因此，他们设计了一种由注意力机制引导的、全新的 GRU 单元——AUGRU。<ol>
<li><strong>计算注意力</strong>：首先，模型会计算<strong>候选商品</strong>的 Embedding <code>e_a</code> 与<strong>每一个</strong>历史兴趣状态 <code>h_t</code> 之间的相关性，得到一个注意力得分 <code>a_t</code>。这个得分衡量了历史上的某个兴趣状态与当前目标的关联程度。</li>
<li><strong>改造更新门</strong>：GRU 有一个关键的“更新门” <code>u_t</code>，它决定了当前时刻的信息在多大程度上被用来更新 GRU 的记忆。AUGRU 对这个更新门做了巧妙的改造，将原始更新门与注意力得分相乘：<code>u&#39;_t = a_t * u_t</code>。</li>
<li><strong>核心作用</strong>：<ul>
<li>如果某个历史兴趣 <code>h_t</code> 与当前候选商品<strong>高度相关</strong>，其注意力得分 <code>a_t</code> 就很高，这会“放大”更新门的作用，使得这个兴趣状态在进化网络中产生<strong>更强的影响力</strong>。</li>
<li>反之，如果 <code>h_t</code> 与候选商品<strong>无关</strong>，其注意力得分 <code>a_t</code> 很低，这会“抑制”更新门，使得这条无关的兴趣信息在进化网络中被<strong>有效削弱</strong>。</li>
</ul>
</li>
<li><strong>最终输出</strong>：AUGRU 网络的最终一个隐藏状态，就代表了这条“与目标相关的兴趣进化轨迹”的最终结果。这个向量被认为是用户兴趣的“进化终点”，并被用于最终的 CTR 预测。</li>
</ol>
</li>
</ul>
<p><strong>3. 锦上添花：辅助损失 (Auxiliary Loss)</strong></p>
<p>为了让第一阶段的 GRU 能够学习到更有意义的兴趣状态表示，DIEN 还引入了一个<strong>辅助损失函数</strong>。</p>
<ul>
<li><strong>动机</strong>：只靠最终的 CTR 预测任务来监督整个长序列的学习，梯度信号会非常稀疏和微弱。</li>
<li><strong>做法</strong>：在兴趣提取层，额外增加一个“用前一个行为预测后一个行为”的学习任务。即用 <code>h_t</code> 去预测真实的用户下一个行为 <code>e_&#123;t+1&#125;</code>。</li>
<li><strong>好处</strong>：这个辅助任务为兴趣提取层的每一步都提供了直接的监督信号，迫使 GRU 学会如何从行为序列中提取出真正有预测性的“兴趣状态”。</li>
</ul>
<p><strong>总结：</strong><br>DIEN 是对 DIN 的一次深刻升华。它成功地将推荐系统从对用户<strong>静态、多样</strong>兴趣的捕捉，推进到了对<strong>动态、时序</strong>兴趣的建模。其核心贡献可以概括为：</p>
<ol>
<li>首次使用 <strong>RNN 结构</strong>来显式地捕捉用户行为的<strong>序列性</strong>。</li>
<li>创新地设计了 <strong>AUGRU</strong> 单元，巧妙地将<strong>注意力机制</strong>与 <strong>RNN 的门控机制</strong>相结合，实现了对“与目标相关的兴趣进化”的精准模拟。</li>
<li>引入<strong>辅助损失</strong>，有效提升了底层序列表示的学习效果。</li>
</ol>
<h3 id="4-3-SIM-Search-based-Interest-Model-攻克工业级“超长序列”难题"><a href="#4-3-SIM-Search-based-Interest-Model-攻克工业级“超长序列”难题" class="headerlink" title="4.3 SIM (Search-based Interest Model): 攻克工业级“超长序列”难题"></a>4.3 SIM (Search-based Interest Model): 攻克工业级“超长序列”难题</h3><p><strong>1. 动机：当序列长度成为瓶颈</strong></p>
<ul>
<li><strong>现实的挑战</strong>：一个活跃用户的历史行为记录，可以轻易达到数千、数万甚至更长。</li>
<li><strong>现有模型的困境</strong>：<ul>
<li><strong>DIEN (RNN-based)</strong>：RNN 的计算是串行的，序列越长，计算耗时越长。处理上万长度的序列，对于要求在几十毫秒内返回结果的在线推荐服务来说，是完全不可接受的。</li>
<li><strong>Transformer-based</strong>：标准 Transformer 的自注意力机制，其计算复杂度是序列长度的平方，即 <code>O(n^2)</code>。这使得它在处理长序列时比 RNN 的情况更糟。</li>
<li><strong>DIN (Attention-based)</strong>：虽然 DIN 的注意力计算是并行的，但对上万个历史行为逐一计算与候选商品的注意力得分，依然是一笔巨大的计算开销。</li>
</ul>
</li>
</ul>
<p><strong>SIM (Search-based Interest Model, 2020)</strong> 的提出，不是对模型结构进行小修小补，而是从根本上改变了处理超长序列的<strong>思想范式</strong>。</p>
<p><strong>2. SIM 的核心思想：从“一次性建模”到“两阶段检索”</strong></p>
<p>SIM 的核心洞察是：我们不必，也不能在一次在线预测中，对用户的全部上万条历史记录进行复杂建模。对于一个特定的候选商品，用户的历史行为中，真正能提供决策信息的，往往只是其中一小部分最相关的行为。</p>
<p>因此，SIM 将用户兴趣建模的过程，巧妙地重构成了一个**“在用户历史中进行搜索”**的任务。它将推荐系统中经典的“召回-排序”两阶段漏斗思想，“微缩”并应用到了用户兴趣建模的内部。</p>
<p><strong>3. SIM 的核心架构：GSU + ESU 两阶段搜索</strong></p>
<p><strong>第一阶段：通用兴趣检索 (General Search Unit, GSU) - “硬搜索”</strong></p>
<ul>
<li><strong>目标</strong>：从用户上万条的完整历史中，<strong>快速、低成本地</strong>“召回”出一个几百条的、高度相关的历史行为候选子集。</li>
<li><strong>实现方式</strong>：<ol>
<li><strong>离线建索引</strong>：对于每个用户，系统会离线地将他的<strong>全部</strong>历史行为物品的 Embedding 向量，存入一个高效的<strong>近似最近邻（ANN）检索引擎</strong>（例如 Faiss）。这相当于为每个用户都建立了一个私有的、可被高速检索的“行为数据库”。</li>
<li><strong>在线检索</strong>：当一次推荐请求到来时，模型会先构建一个<strong>查询向量 (Query Vector)</strong>。这个查询向量代表了用户当前的“大致兴趣方向”，通常由用户<strong>最近的少数几个行为</strong>（如最近点击的5个商品）的 Embedding 平均池化得到。</li>
<li>模型用这个查询向量，去用户的“行为数据库”（ANN索引）中进行一次极速的向量检索，找出与当前兴趣方向最相似的 Top-K（比如200）个历史行为。这个过程因为是硬性地筛选出一部分，所以被称为“<strong>硬搜索</strong>”。</li>
</ol>
</li>
</ul>
<p><strong>第二阶段：精准兴趣检索 (Exact Search Unit, ESU) - “软搜索”</strong></p>
<ul>
<li><p><strong>目标</strong>：在 GSU 阶段筛选出的那 200 条相关历史中，根据<strong>当前候选商品（Target Item）</strong>，进行更精细的兴趣匹配和建模。</p>
</li>
<li><p><strong>实现方式</strong>：<strong>这一阶段的实现，几乎就是 DIN 的一个翻版！</strong></p>
<ol>
<li><strong>输入</strong>：当前候选商品的 Embedding <code>e_a</code>，以及 GSU 筛选出的 200 个历史行为的 Embedding。</li>
<li><strong>注意力计算</strong>：使用与 DIN 相同的<strong>目标感知注意力网络</strong>，计算 <code>e_a</code> 与这 200 个历史行为中每一个的相关性（注意力得分）。因为是计算连续的权重，所以被称为“<strong>软搜索</strong>”。</li>
<li><strong>加权求和</strong>：将这 200 个历史行为的 Embedding，根据其注意力得分进行加权求和，得到最终的、动态的、与当前候选商品强相关的用户兴趣向量。</li>
</ol>
<p>  <img src="/.io//image#.png" alt="image.png"></p>
</li>
</ul>
<p><strong>总结：</strong><br>SIM 是一个算法与工程完美结合的典范。它没有试图用一个单一的、复杂的模型去硬“啃”超长序列，而是创造性地提出了**“先捞后筛”**的两阶段兴趣建模范式。</p>
<ul>
<li><strong>GSU</strong> 负责从汪洋大海般的历史中，高效地“捞”出最可能相关的几百条数据。</li>
<li><strong>ESU</strong> 则负责在这几百条数据中，用 DIN 的思想进行精细化的“筛选”和“整合”。</li>
</ul>
<h2 id="第五章：生成式与大语言模型的前沿探索-The-Frontier-Generative-LLM-based-Models"><a href="#第五章：生成式与大语言模型的前沿探索-The-Frontier-Generative-LLM-based-Models" class="headerlink" title="第五章：生成式与大语言模型的前沿探索 (The Frontier: Generative &amp; LLM-based Models)"></a><strong>第五章：生成式与大语言模型的前沿探索 (The Frontier: Generative &amp; LLM-based Models)</strong></h2><h3 id="5-1-范式转移：从“判别式”到“生成式”推荐"><a href="#5-1-范式转移：从“判别式”到“生成式”推荐" class="headerlink" title="5.1 范式转移：从“判别式”到“生成式”推荐"></a>5.1 范式转移：从“判别式”到“生成式”推荐</h3><p>到目前为止，我们讨论的所有模型，从 LR 到 DIEN，都属于<strong>判别式模型 (Discriminative Models)</strong>。而最新的研究浪潮，正推动着我们走向<strong>生成式模型 (Generative Models)</strong>。</p>
<p><strong>1. 判别式推荐 (我们一直在做的事)</strong></p>
<ul>
<li><strong>核心目标</strong>：学习一个条件概率 <code>P(y|x)</code>。</li>
<li><strong>翻译成大白话</strong>：给定一个“输入” <code>x</code>（即一个<code>(用户, 物品)</code>对），模型需要“判别”出对应的“标签” <code>y</code>（即用户是否会点击）的概率。</li>
<li><strong>模型角色</strong>：像一个“<strong>裁判</strong>”或“<strong>打分器</strong>”。它的任务是，对于每一个候选物品，都给出一个“好”或“不好”的概率分数。它回答的问题是：“这个物品，给这个用户，匹配度有多高？”</li>
</ul>
<p><strong>2. 生成式推荐 (未来的方向)</strong></p>
<ul>
<li><strong>核心目标</strong>：学习输入数据 <code>x</code> 本身的联合概率分布 <code>P(x)</code>。</li>
<li><strong>翻译成大白话</strong>：模型不再是去“判断”一个已有的组合，而是去<strong>学习用户行为序列本身是如何“生成”的</strong>。它试图理解用户行为的内在逻辑和模式。</li>
<li><strong>模型角色</strong>：像一个“<strong>故事续写者</strong>”或“<strong>语言模型</strong>”。它的任务是，在看到用户过去的一系列行为后，去“生成”或“预测”他下一步最有可能产生的行为。它回答的问题是：“根据这个用户的历史，他下一步会做什么？”</li>
</ul>
<p><strong>为什么会发生这种转变？</strong><br>生成式范式借鉴了 GPT 等大语言模型的成功，它为推荐系统带来了几个潜在的巨大优势：</p>
<ol>
<li><strong>更全面的行为建模</strong>：它不再孤立地看待每一次推荐，而是将用户的整个行为序列视为一个连贯的整体来学习，能更好地捕捉用户的长期动态意图。</li>
<li><strong>更高的灵活性</strong>：一个训练好的生成式模型，不仅能用于预测下一个物品（排序），还能用于生成整个会话（session）、生成推荐理由等更多元的任务。</li>
<li><strong>与LLM的天然结合</strong>：将物品视为“词汇”，将用户行为视为“句子”，这使得推荐系统可以直接利用在海量文本上预训练好的、拥有强大语义理解能力的 Transformer 架构。</li>
</ol>
<h3 id="5-2-Kuaiformer-LLM4Rec-借鉴LLM架构解决推荐三大挑战"><a href="#5-2-Kuaiformer-LLM4Rec-借鉴LLM架构解决推荐三大挑战" class="headerlink" title="5.2 Kuaiformer (LLM4Rec): 借鉴LLM架构解决推荐三大挑战"></a>5.2 Kuaiformer (LLM4Rec): 借鉴LLM架构解决推荐三大挑战</h3><p>Kuaiformer 是快手团队提出的一个典型的生成式推荐模型，它展示了如何在工业级的超大规模场景下，借鉴并改造 LLM 架构来解决推荐的核心难题。</p>
<p><strong>面临的三大挑战：</strong></p>
<ol>
<li><strong>超大候选集</strong>：推荐系统的物品“词汇表”是十亿甚至百亿级别，远超LLM的几十万词汇表，无法直接使用 <code>Softmax</code>。</li>
<li><strong>用户兴趣多样性</strong>：用户的兴趣是发散且多样的，需要模型能同时捕捉多个兴趣点。</li>
<li><strong>超长用户序列</strong>：用户的历史行为序列可达上万，标准 Transformer 的 <code>O(n^2)</code> 复杂度无法承受。</li>
</ol>
<p><strong>Kuaiformer 的解决方案：</strong></p>
<p><strong>1. 应对超长序列 → 分段建模 (Sequence Segmentation)</strong><br>为了破解 <code>O(n^2)</code> 的魔咒，Kuaiformer 采用“分而治之”的策略：</p>
<ul>
<li>将用户的超长历史行为序列，按时间顺序切分成若干个<strong>段落 (Segment)</strong>。</li>
<li>对每个段落<strong>并行地</strong>使用一个 <strong>Bi-Transformer</strong> (双向Transformer) 进行内部信息编码，然后通过<strong>平均池化</strong>将每个段落压缩成一个代表性向量。</li>
<li>最后将所有段落的代表性向量拼接起来，形成一个对原始长序列的<strong>浓缩摘要</strong>。</li>
</ul>
<p><strong>2. 应对多重兴趣 → 查询式注意力 (Query-based Attention)</strong><br>在得到浓缩的历史摘要后，模型通过一个 <strong>Causal Transformer</strong> 来提取用户的多个兴趣：</p>
<ul>
<li>模型引入 <code>k</code> 个可学习的<strong>查询向量 (Query Vector)</strong>，并将它们与历史摘要一同输入到 Transformer 中。</li>
<li>在 Transformer 内部，每个 Query 向量会通过自注意力机制，像“探针”一样去扫描和聚合历史摘要中与自己相关的部分。</li>
<li>最终，<code>k</code> 个 Query 向量在 Transformer 输出层的位置上，就形成了代表用户 <code>k</code> 个不同兴趣中心的<strong>多兴趣向量</strong> <code>&#123;u_1, u_2, ..., u_k&#125;</code>。</li>
</ul>
<p><strong>3. 应对超大候选集 → 双塔预测 与 In-batch 训练</strong></p>
<ul>
<li><p><strong>预测方式</strong>：最终的预测是一个双塔问题。Kuaiformer 本身构成了复杂的 <strong>User Tower</strong>（输出 <code>k</code> 个兴趣向量），而物品侧则是一个简单的 <strong>Item Tower</strong>（输出物品 Embedding）。用户对一个物品的最终得分为：</p>
<p>  <img src="/.io//image$.png" alt="image.png"></p>
<p>  即物品 Embedding 与用户所有兴趣向量相似度的<strong>最大值</strong>。</p>
</li>
<li><p><strong>训练方式</strong>：由于无法对全量物品计算 <code>Softmax</code>，模型采用 <strong>In-batch Softmax</strong> 的方式进行训练，即只在当前一个批次 (mini-batch) 内的样本中进行 <code>Softmax</code> 计算。</p>
</li>
<li><p><strong>偏差修正</strong>：为了解决 In-batch 采样带来的<strong>流行度偏差</strong>和<strong>标签噪声</strong>问题，Kuaiformer 在训练时采用了我们之前讨论过的 <strong>LogQ修正</strong> 和 <strong>标签平滑</strong> 等高级技巧，以保证训练的稳定和无偏。</p>
</li>
</ul>
<h3 id="5-3-核心概念：高级训练技巧-——-偏差修正与标签平滑"><a href="#5-3-核心概念：高级训练技巧-——-偏差修正与标签平滑" class="headerlink" title="5.3 核心概念：高级训练技巧 —— 偏差修正与标签平滑"></a>5.3 核心概念：高级训练技巧 —— 偏差修正与标签平滑</h3><p>当我们使用像 Kuaiformer 这样的大模型，并采用 <strong>In-batch Softmax</strong> 策略进行训练时，会遇到两个普遍存在且严重影响模型性能的问题：<strong>流行度偏差</strong> 和 <strong>标签噪声</strong>。为了解决这两个问题，研究者们引入了两种极其重要的高级训练技巧。</p>
<p><strong>1. 偏差修正：对抗“流行即原罪”的 LogQ 修正</strong></p>
<ul>
<li><p><strong>问题背景：In-batch Softmax 带来的流行度偏差</strong></p>
<ul>
<li>在拥有亿级物品池的推荐系统中，我们无法在每次训练时对所有物品计算 <code>Softmax</code>。因此，我们采用 <code>In-batch Softmax</code>，即对于一个正样本，只把它和<strong>当前批次 (mini-batch) 内</strong>的其他样本作为负样本来计算损失。</li>
<li>这样做的问题在于，一个 mini-batch 内的样本<strong>并非对全体物品的均匀随机采样</strong>。<strong>热门物品</strong>因为本身与用户交互多，所以它们出现在任意一个 batch 中的概率天然就更高。</li>
<li><strong>后果</strong>：模型在训练时，看到的负样本大多是“热门选手”。为了在竞争中胜出，模型可能会学到一种错误的偏见：“<strong>热门的物品通常不是好的推荐</strong>”，从而系统性地压低对所有热门物品的预测分数。这与“热门物品之所以热门，是因为它们质量高、受欢迎”的常识相悖，会导致模型性能严重退化。这与我们在召回阶段讨论的 SSB (Sampling-Bias-Corrected) 问题根源完全相同。</li>
</ul>
</li>
<li><p><strong>解决方案：LogQ 修正 (LogQ Correction)</strong></p>
<ul>
<li><strong>核心思想</strong>：在计算损失前，对每个物品的原始预测分数（logit）进行一次“纠偏”，以抵消其因流行度带来的采样概率不均。</li>
<li><strong>实现方式</strong>：将每个物品的原始分数 <code>s_i</code> 调整为 <code>s&#39;_i</code>：<br>  <code>s&#39;_i = s_i - log(Q_i)</code><br>  其中：<ul>
<li><code>s_i</code> 是模型对物品 <code>i</code> 输出的原始分数。</li>
<li><code>Q_i</code> 是物品 <code>i</code> 被采样到 batch 中的概率，通常用它的<strong>全局流行度</strong>来近似。</li>
</ul>
</li>
<li><strong>损失函数</strong>：使用修正后的分数 <code>s&#39;_i</code> 来计算 In-batch Softmax 损失：</li>
</ul>
<p>  <img src="/.io//cbde9e5e-09d1-49c0-84b6-5315b7f5c408.png" alt="image.png"></p>
<ul>
<li><strong>作用效果</strong>：<ul>
<li>对于热门物品，其 <code>Q_j</code> 很大，<code>log(Q_j)</code> 也很大。从它的分数中减去一个大数，相当于<strong>降低了它作为负样本时的“惩罚”力度</strong>，给了它一个公平竞争的机会。</li>
<li>对于冷门物品，其 <code>Q_j</code> 很小，<code>log(Q_j)</code> 是一个较大的负数。从分数中减去一个负数，等于<strong>增加了它的分数</strong>，给了它一个“补偿”，让它不至于被热门物品淹没。</li>
</ul>
</li>
<li><strong>最终目的</strong>：通过 LogQ 修正，使得 In-batch Softmax 损失在数学上成为对全局 Softmax 损失的一个<strong>无偏估计</strong>，从而让模型学习到物品的真实匹配度，而非被流行度所误导。</li>
</ul>
</li>
</ul>
<p><strong>2. 标签平滑：接受“不完美”的用户行为</strong></p>
<ul>
<li><strong>问题背景：过分自信与标签噪声</strong><ul>
<li><strong>标签非完美</strong>：用户的行为数据是有噪声的。一次点击（标签为<code>1</code>）不代表用户100%满意，可能只是“手滑”或“好奇”；一次未点击（标签为<code>0</code>）更不代表用户100%讨厌。</li>
<li><strong>模型过分自信 (Overconfidence)</strong>：标准的交叉熵损失函数会驱使模型将对正样本的预测概率无限推向 <code>1.0</code>。为了达到这个“绝对正确”的目标，模型可能会学得过于极端，对训练数据中的噪声也进行了过度拟合，从而导致<strong>泛化能力下降</strong>。</li>
</ul>
</li>
<li><strong>解决方案：标签平滑 (Label Smoothing)</strong><ul>
<li><p><strong>核心思想</strong>：对模型“温柔”一点，不要强求它做出“非黑即白”的判断。我们主动地给标签增加一点“不确定性”。</p>
</li>
<li><p><strong>实现方式</strong>：我们不再使用 <code>[0, 1]</code> 这样的硬标签，而是使用“软标签”。</p>
<ul>
<li>选定一个很小的平滑值 <code>ε</code>（例如 <code>0.1</code>）。</li>
<li>对于<strong>正样本</strong>，我们将其目标标签从 <code>1.0</code> 修改为 <code>1.0 - ε</code>（即 <code>0.9</code>）。</li>
<li>对于<strong>负样本</strong>，我们不再要求其目标为 <code>0</code>，而是将剩余的概率 <code>ε</code> 平均分配给所有 <code>K-1</code> 个负样本，每个负样本的目标变为 <code>ε / (K-1)</code>。</li>
</ul>
</li>
<li><p><strong>损失函数</strong>：相应的，损失函数也从只惩罚正样本，变为同时考虑所有样本：</p>
<p>  <img src="/.io//image%25.png" alt="image.png"></p>
<p>  （注：这里的 pj 是对负样本的预测概率，为了简化，有时会写成 log(1−pj) 的形式，但核心思想是让模型不要把负样本的概率压到绝对的0）</p>
</li>
<li><p><strong>作用效果</strong>：</p>
<ul>
<li><strong>防止过分自信</strong>：由于目标不再是绝对的1，模型不会把正样本的logit无限推高，使其权重分布更合理。</li>
<li><strong>提升泛化能力</strong>：强迫模型为负样本也保留一点点概率，这鼓励了不同类别之间的Embedding在向量空间中形成一个更合理的、有间隔的分布，而不是把所有正样本都挤在一个点上。</li>
<li><strong>作为一种正则化</strong>：标签平滑是一种简单而高效的正则化技术，能有效防止模型过拟合，使训练过程更稳定。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="第六章：工业界全景与总结-The-Industrial-Landscape-Summary"><a href="#第六章：工业界全景与总结-The-Industrial-Landscape-Summary" class="headerlink" title="第六章：工业界全景与总结 (The Industrial Landscape &amp; Summary)"></a><strong>第六章：工业界全景与总结 (The Industrial Landscape &amp; Summary)</strong></h2><h3 id="6-1-工业界的标准流程：多阶段排序漏斗（召回、粗排、精排）"><a href="#6-1-工业界的标准流程：多阶段排序漏斗（召回、粗排、精排）" class="headerlink" title="6.1 工业界的标准流程：多阶段排序漏斗（召回、粗排、精排）"></a>6.1 工业界的标准流程：多阶段排序漏斗（召回、粗排、精排）</h3><p>现代大规模推荐系统面临着一个核心的工程矛盾：一方面，候选的物品池是<strong>海量</strong>的（通常是十亿甚至百亿级别）；另一方面，对用户的每一次推荐请求，系统必须在极短的时间内（通常在 <strong>50-100毫秒</strong>）返回一个高质量的、个性化的排序列表。</p>
<p><strong>在一个单一阶段内，用一个复杂的模型（如DeepFM）去处理十亿级的物品，是绝对不可能的。</strong> 这种矛盾催生了工业界标准的<strong>多阶段排序漏斗 (Multi-stage Ranking Funnel)</strong> 架构。</p>
<p>这个架构的核心思想是**“层层筛选，逐步聚焦”<strong>，通过多个阶段，不断地在</strong>计算成本<strong>和</strong>排序精度**之间做出权衡，将海量的物品集一步步精炼成最终呈现给用户的几十个物品。</p>
<p><strong>第一阶段：召回 (Recall)</strong></p>
<ul>
<li><strong>目标</strong>：<strong>“大海捞针”</strong>。从全量的、百亿级的物料库中，快速、低成本地找出几千个用户<strong>可能</strong>感兴趣的候选物品。</li>
<li><strong>核心指标</strong>：<strong>召回率 (Recall)</strong>。这一阶段的使命是“宁可错杀一千，不可放过一个”，要尽可能地保证用户未来真正会喜欢的物品，能够出现在这个候选池中。如果召回阶段就漏掉了，后续阶段再强大也无力回天。</li>
<li><strong>候选集规模</strong>：从 <code>百亿级</code> 筛选到 <code>千级</code>。</li>
<li><strong>技术与模型</strong>：追求极致的速度。<ul>
<li><strong>多路召回</strong>：最常见的策略，并行使用多种不同的简单召回方法，并将结果合并，以保证结果的多样性。例如：<ul>
<li><strong>热门召回</strong>：返回全局热门的物品，作为基础流量保障。</li>
<li><strong>协同过滤</strong>：经典的 User-CF, Item-CF，找到相似用户喜欢的，或相似物品。</li>
<li><strong>向量检索召回</strong>：目前最主流的方法。利用双塔模型等提前计算好所有用户和物品的 Embedding 向量，将物品向量存入高效的近似最近邻（ANN）索引（如 <code>Faiss</code>）。在线上，根据用户的 Embedding 向量，进行一次极速的向量检索，找出最相似的 Top-K 物品。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>第二阶段：粗排 (Pre-ranking &#x2F; Coarse Ranking)</strong></p>
<ul>
<li><strong>目标</strong>：<strong>“精挑细选”</strong>。对召回层送来的几千个候选物品，进行一次初步的、计算量适中的排序，淘汰掉其中大量“明显不靠谱”的物品，将候选集进一步压缩。</li>
<li><strong>核心指标</strong>：在<strong>极低延迟</strong>和<strong>较高精度</strong>之间做权衡。它需要比精排快得多，但比召回准得多。</li>
<li><strong>候选集规模</strong>：从 <code>千级</code> 筛选到 <code>百级</code>。</li>
<li><strong>技术与模型</strong>：使用相对简单的模型和少量核心特征。<ul>
<li><strong>逻辑回归 (LR)</strong> 或 <strong>浅层的 MLP</strong>。</li>
<li><strong>简化的深度模型</strong>：比如一个“青春版”的 DeepFM，网络层数更少、宽度更窄，使用的特征也更少。</li>
<li><strong>知识蒸馏</strong>：这是保证粗排效果的关键技术。让复杂的**精排模型（老师）<strong>来指导简单的</strong>粗排模型（学生）**进行学习，让学生模型能够模仿老师模型的打分行为，从而保证两个阶段的排序偏好尽可能一致，避免错杀“潜力股”。</li>
</ul>
</li>
</ul>
<p><strong>第三阶段：精排 (Fine-ranking)</strong></p>
<ul>
<li><strong>目标</strong>：<strong>“精雕细琢”</strong>。对粗排层送来的几百个高质量候选物品，不计成本地使用最强大、最复杂的模型，进行最精准的个性化打分和排序。</li>
<li><strong>核心指标</strong>：<strong>精准率 (Precision)</strong>。这一阶段的目标是追求排序结果的极致准确，尤其是列表顶部的准确性。</li>
<li><strong>候选集规模</strong>：从 <code>百级</code> 筛选到最终呈现给用户的 <code>几十个</code>。</li>
<li><strong>技术与模型</strong>：所有我们之前详细讨论过的**“重武器”模型**都在这一层大展身手。<ul>
<li><strong>特征交互模型</strong>：<code>DeepFM</code>, <code>DCN</code>, <code>xDeepFM</code> 等。</li>
<li><strong>序列建模</strong>：<code>DIN</code>, <code>DIEN</code>, <code>SIM</code> 等。</li>
<li><strong>多任务与多场景</strong>：<code>MMoE</code>, <code>CAN</code> 等。</li>
<li><strong>前沿模型</strong>：<code>LLM-based</code> 模型。</li>
<li>在这一层，模型会使用上千维的、包含大量<strong>实时特征</strong>的、最全面的特征信息来进行预测。</li>
</ul>
</li>
</ul>
<h3 id="6-2-当前工业界的主力模型：为何DeepFM架构是“万能骨架”？"><a href="#6-2-当前工业界的主力模型：为何DeepFM架构是“万能骨架”？" class="headerlink" title="6.2 当前工业界的主力模型：为何DeepFM架构是“万能骨架”？"></a>6.2 当前工业界的主力模型：为何DeepFM架构是“万能骨架”？</h3><h3 id="6-3-技术演进路线图：从LR到LLM的回顾与展望"><a href="#6-3-技术演进路线图：从LR到LLM的回顾与展望" class="headerlink" title="6.3 技术演进路线图：从LR到LLM的回顾与展望"></a>6.3 技术演进路线图：从LR到LLM的回顾与展望</h3></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://qyp9909.github.io">Yuanpeng QU</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://qyp9909.github.io/2025/08/08/Rec_AD_Ranking/">https://qyp9909.github.io/2025/08/08/Rec_AD_Ranking/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Study/">Study</a></div><div class="post-share"><div class="social-share" data-image="/images/cover/Rec_AD_Cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/06/Rec_AD_Recall/" title="搜广推笔记 召回"><img class="cover" src="/images/cover/Rec_AD_Cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">搜广推笔记 召回</div></div><div class="info-2"><div class="info-item-1">搜广推[召回]Created by: Yuanpeng QUCreated time: 2025年8月5日 22:51 一、 推荐系统总体架构 (Overall Recommendation System Architecture)1. 级联漏斗范式 (Cascading Funnel Paradigm) 现代大规模推荐系统的核心架构，普遍遵循一种级联漏斗范式 (Cascading Funnel Paradigm)。这个范式的诞生，是为了解决一个根本性的矛盾：一方面，我们的候选物品库是海量的（百万、千万甚至上亿级别）；另一方面，用户的屏幕空间是有限的，且要求响应速度极快（毫秒级）。 因此，我们不可能对所有物品都用最复杂的模型进行最精确的计算。漏斗范式通过设置多个层层递进的过滤环节，实现了从海量到少量，从粗糙到精准的逐级筛选，在计算效率和推荐效果之间取得了极致的平衡。 这个漏斗通常包含以下四个核心层级： 1. 召回层 (Recall...</div></div></div></a><a class="pagination-related" href="/2025/08/10/Rec_AD_MTL/" title="搜广推笔记 多目标排序"><img class="cover" src="/images/cover/Rec_AD_Cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">搜广推笔记 多目标排序</div></div><div class="info-2"><div class="info-item-1">搜广推[多目标排序]Created by: Yuanpeng QUCreated time: 2025年8月10日 21:12 第一部分：多目标学习的核心挑战与基础1. 动机：为何需要多目标学习？现代工业级推荐系统（如电商、短视频）的目标是综合性的，不能只关注单一指标 。例如，一个短视频推荐系统不仅要优化点击率，可能还要同时提升用户的点赞、关注、转发、评论率以及观看时长 。 如果为每个任务单独建模和优化，或者用一个简单的模型直接预测所有任务，就会遇到**“跷跷板效应” (Seesaw Phenomenon)** 。  定义：指的是当模型在优化一个目标时，会导致另一个或多个其他目标的性能下降的现象 。 举例： 任务跷跷板 (Task Seesaw)：一个模型如果过度优化点击率（CTR），可能会倾向于推荐标题党或封面吸引人的内容，但这部分内容的用户实际满意度（如观看时长、点赞率）可能很低，从而损害了长期用户体验 。 领域跷跷板 (Domain...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/06/Rec_AD_Recall/" title="搜广推笔记 召回"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-06</div><div class="info-item-2">搜广推笔记 召回</div></div><div class="info-2"><div class="info-item-1">搜广推[召回]Created by: Yuanpeng QUCreated time: 2025年8月5日 22:51 一、 推荐系统总体架构 (Overall Recommendation System Architecture)1. 级联漏斗范式 (Cascading Funnel Paradigm) 现代大规模推荐系统的核心架构，普遍遵循一种级联漏斗范式 (Cascading Funnel Paradigm)。这个范式的诞生，是为了解决一个根本性的矛盾：一方面，我们的候选物品库是海量的（百万、千万甚至上亿级别）；另一方面，用户的屏幕空间是有限的，且要求响应速度极快（毫秒级）。 因此，我们不可能对所有物品都用最复杂的模型进行最精确的计算。漏斗范式通过设置多个层层递进的过滤环节，实现了从海量到少量，从粗糙到精准的逐级筛选，在计算效率和推荐效果之间取得了极致的平衡。 这个漏斗通常包含以下四个核心层级： 1. 召回层 (Recall...</div></div></div></a><a class="pagination-related" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-11</div><div class="info-item-2">搜广推笔记 指标</div></div><div class="info-2"><div class="info-item-1">搜广推[指标]Created by: Yuanpeng QUCreated time: 2025年8月11日 16:14 一、...</div></div></div></a><a class="pagination-related" href="/2025/08/10/Rec_AD_MTL/" title="搜广推笔记 多目标排序"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-10</div><div class="info-item-2">搜广推笔记 多目标排序</div></div><div class="info-2"><div class="info-item-1">搜广推[多目标排序]Created by: Yuanpeng QUCreated time: 2025年8月10日 21:12 第一部分：多目标学习的核心挑战与基础1. 动机：为何需要多目标学习？现代工业级推荐系统（如电商、短视频）的目标是综合性的，不能只关注单一指标 。例如，一个短视频推荐系统不仅要优化点击率，可能还要同时提升用户的点赞、关注、转发、评论率以及观看时长 。 如果为每个任务单独建模和优化，或者用一个简单的模型直接预测所有任务，就会遇到**“跷跷板效应” (Seesaw Phenomenon)** 。  定义：指的是当模型在优化一个目标时，会导致另一个或多个其他目标的性能下降的现象 。 举例： 任务跷跷板 (Task Seesaw)：一个模型如果过度优化点击率（CTR），可能会倾向于推荐标题党或封面吸引人的内容，但这部分内容的用户实际满意度（如观看时长、点赞率）可能很低，从而损害了长期用户体验 。 领域跷跷板 (Domain...</div></div></div></a><a class="pagination-related" href="/2025/08/12/DL_1/" title="深度学习小记"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-12</div><div class="info-item-2">深度学习小记</div></div><div class="info-2"><div class="info-item-1">深度学习小记Created by: Yuanpeng QUCreated time: 2025年8月12日 17:41 I. 神经网络的核心组件与训练技巧1. 归一化层：BN 与 LN 的原理、区别与应用场景 核心目的：解决“内部协变量偏移 (Internal Covariate Shift)”问题。即在训练中，由于前层网络参数不断变化，导致后层网络接收到的数据分布一直在变，拖慢收敛速度。归一化层通过将每层网络的输入强制拉回到一个稳定的分布（如均值为0，方差为1），从而加速训练。 Batch Normalization (BN) 原理：“纵向”或“按特征”归一化。它在一个批次（mini-batch）内，对每一个特征维度计算均值和方差，并进行归一化。 关键机制：引入了两个可学习的参数 γ (缩放) 和 β (平移)，让网络可以自主决定是否以及在多大程度上恢复原始的分布，以保证模型的表达能力。 训练 vs 推理：训练时使用当前批次的统计量，同时用滑动平均记录全局统计量；推理时则使用保存下来的全局统计量，以保证输出的确定性。 应用场景：在卷积神经网络 (CNN)...</div></div></div></a><a class="pagination-related" href="/2025/08/12/ML_1/" title="机器学习小记"><img class="cover" src="/images/cover/Rec_AD_Cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-12</div><div class="info-item-2">机器学习小记</div></div><div class="info-2"><div class="info-item-1">机器学习小记Created by: Yuanpeng QUCreated time: 2025年8月13日 10:43 第一部分：神经网络的核心组件 (Core Components of Neural Networks)1.1 激活函数：从经典到现代激活函数是神经网络的灵魂，它负责向网络中引入非线性，使得网络有能力学习和拟合现实世界中复杂的非线性关系。如果没有激活函数，多层神经网络本质上等同于一个单层的线性模型。 1.1.1 Sigmoid &amp; Tanh：经典饱和函数的特性与局限性 1. Sigmoid 函数  公式：   $$  f(x) &#x3D; \frac{1}{1+e^{-x}}  $$  核心特性：  将任意实数输入压缩到 (0, 1) 区间内。 这个特性使其输出可以被直观地解释为概率，因此在逻辑回归以及各类分类模型的输出层中，当需要预测一个概率时，Sigmoid 仍然是标准选择。特别是在CTR&#x2F;CVR预估中，它的地位不可动摇。   主要局限性 (面试重点):  梯度消失 (Vanishing Gradient): 这是 Sigmoid...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/LOGO.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Yuanpeng QU</div><div class="author-info-description">CS PhD, 3rd yr.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">5</div></a></div><a id="card-info-btn" href="https://qyp9909.github.io/homepage"><i class="fas fa-user-graduate"></i><span>Academic Homepage</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qyp9909/qyp9909.github.io" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qyp9909@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%9C%E5%B9%BF%E6%8E%A8-%E6%8E%92%E5%BA%8F"><span class="toc-number">1.</span> <span class="toc-text">搜广推[排序]</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E6%80%BB%E7%BB%93%EF%BC%88%E5%A4%A7%E7%BA%B2%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">推荐系统排序模型知识体系总结（大纲）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E7%A1%80-Foundations-of-Ranking-Models"><span class="toc-number">2.</span> <span class="toc-text">第一章：排序模型的基础 (Foundations of Ranking Models)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%8E%92%E5%BA%8F%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BB%BB%E5%8A%A1%EF%BC%9A%E4%BB%8E%E2%80%9C%E6%8E%92%E5%BA%8F%E5%88%97%E8%A1%A8%E2%80%9D%E5%88%B0%E2%80%9C%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E4%BC%B0%E2%80%9D%E7%9A%84%E6%80%9D%E7%BB%B4%E8%BD%AC%E5%8F%98"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 排序的核心任务：从“排序列表”到“点击率预估”的思维转变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E4%B8%87%E5%8F%98%E4%B8%8D%E7%A6%BB%E5%85%B6%E5%AE%97%E7%9A%84%E5%9F%BA%E7%9F%B3%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-LR-%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 万变不离其宗的基石：逻辑回归 (LR) 的原理与局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 数据的预处理：特征工程的核心概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E7%9A%84%E2%80%9C%E6%89%8B%E5%8A%A8%E2%80%9D%E4%B8%8E%E2%80%9C%E8%87%AA%E5%8A%A8%E2%80%9D%E6%97%B6%E4%BB%A3-The-Era-of-Manual-vs-Automatic-Feature-Crossing"><span class="toc-number">3.</span> <span class="toc-text">第二章：特征交叉的“手动”与“自动”时代 (The Era of Manual vs. Automatic Feature Crossing)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E2%80%9C%E5%8D%8A%E8%87%AA%E5%8A%A8%E2%80%9D%E7%9A%84%E6%8E%A2%E7%B4%A2%EF%BC%9AGBDT-LR-%E7%BB%84%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 “半自动”的探索：GBDT+LR 组合模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E2%80%9C%E8%87%AA%E5%8A%A8%E4%BA%A4%E5%8F%89%E2%80%9D%E7%9A%84%E5%9F%BA%E7%9F%B3%EF%BC%9A%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA-FM-%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E7%AA%81%E7%A0%B4"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 “自动交叉”的基石：因子分解机 (FM) 的原理与突破</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E7%90%86%E8%A7%A3%E2%80%9C%E9%98%B6%E2%80%9D%E7%9A%84%E5%90%AB%E4%B9%89%EF%BC%88%E4%B8%80%E9%98%B6%E4%B8%8E%E4%BA%8C%E9%98%B6%EF%BC%89"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 核心概念：理解“阶”的含义（一阶与二阶）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9AEmbedding-%E2%80%94%E2%80%94-%E5%B0%86%E4%B8%87%E7%89%A9%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 核心概念：Embedding —— 将万物向量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E5%88%A9%E5%99%A8-%E2%80%94%E2%80%94-%E6%AD%A3%E5%88%99%E5%8C%96-L1-vs-L2"><span class="toc-number">3.5.</span> <span class="toc-text">2.5 核心概念：防止过拟合的利器 —— 正则化 (L1 vs L2)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E2%80%9C%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E2%80%9D-The-%E2%80%9CBig-Three%E2%80%9D-of-Deep-Ranking-Models"><span class="toc-number">4.</span> <span class="toc-text">第三章：深度学习排序模型的“三驾马车” (The “Big Three” of Deep Ranking Models)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Wide-Deep-%E2%80%9C%E8%AE%B0%E5%BF%86%E2%80%9D%E4%B8%8E%E2%80%9C%E6%B3%9B%E5%8C%96%E2%80%9D%E7%9A%84%E9%A6%96%E6%AC%A1%E7%BB%93%E5%90%88"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 Wide &amp; Deep: “记忆”与“泛化”的首次结合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-DeepFM-FM%E4%B8%8EDNN%E7%9A%84%E6%97%A0%E7%BC%9D%E9%9B%86%E6%88%90"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 DeepFM: FM与DNN的无缝集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-DCN-xDeepFM-%E5%AF%B9%E2%80%9C%E6%98%BE%E5%BC%8F%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E2%80%9D%E7%9A%84%E6%9E%81%E8%87%B4%E6%8E%A2%E7%B4%A2"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 DCN &amp; xDeepFM: 对“显式特征交叉”的极致探索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%85%B1%E4%BA%AB-Embedding-Shared-Embedding-%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BD%9C%E7%94%A8"><span class="toc-number">4.4.</span> <span class="toc-text">3.4 核心概念：共享 Embedding (Shared Embedding) 的原理与作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E4%B8%8E%E6%BC%94%E8%BF%9B-ReLU-PReLU-Dice"><span class="toc-number">4.5.</span> <span class="toc-text">3.5 核心概念：激活函数的作用与演进 (ReLU, PReLU, Dice)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E6%97%B6%E4%BB%A3-The-Era-of-User-Behavior-Sequence-Modeling"><span class="toc-number">5.</span> <span class="toc-text">第四章：用户行为序列建模时代 (The Era of User Behavior Sequence Modeling)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-DIN-Deep-Interest-Network-%E5%BC%80%E5%90%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 DIN (Deep Interest Network): 开启注意力机制的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-DIEN-Deep-Interest-Evolution-Network-%E6%8D%95%E6%8D%89%E5%85%B4%E8%B6%A3%E7%9A%84%E2%80%9C%E8%BF%9B%E5%8C%96%E2%80%9D%E8%BD%A8%E8%BF%B9"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 DIEN (Deep Interest Evolution Network): 捕捉兴趣的“进化”轨迹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-SIM-Search-based-Interest-Model-%E6%94%BB%E5%85%8B%E5%B7%A5%E4%B8%9A%E7%BA%A7%E2%80%9C%E8%B6%85%E9%95%BF%E5%BA%8F%E5%88%97%E2%80%9D%E9%9A%BE%E9%A2%98"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 SIM (Search-based Interest Model): 攻克工业级“超长序列”难题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E7%94%9F%E6%88%90%E5%BC%8F%E4%B8%8E%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E6%B2%BF%E6%8E%A2%E7%B4%A2-The-Frontier-Generative-LLM-based-Models"><span class="toc-number">6.</span> <span class="toc-text">第五章：生成式与大语言模型的前沿探索 (The Frontier: Generative &amp; LLM-based Models)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%8C%83%E5%BC%8F%E8%BD%AC%E7%A7%BB%EF%BC%9A%E4%BB%8E%E2%80%9C%E5%88%A4%E5%88%AB%E5%BC%8F%E2%80%9D%E5%88%B0%E2%80%9C%E7%94%9F%E6%88%90%E5%BC%8F%E2%80%9D%E6%8E%A8%E8%8D%90"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 范式转移：从“判别式”到“生成式”推荐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Kuaiformer-LLM4Rec-%E5%80%9F%E9%89%B4LLM%E6%9E%B6%E6%9E%84%E8%A7%A3%E5%86%B3%E6%8E%A8%E8%8D%90%E4%B8%89%E5%A4%A7%E6%8C%91%E6%88%98"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 Kuaiformer (LLM4Rec): 借鉴LLM架构解决推荐三大挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E9%AB%98%E7%BA%A7%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7-%E2%80%94%E2%80%94-%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3%E4%B8%8E%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 核心概念：高级训练技巧 —— 偏差修正与标签平滑</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E5%B7%A5%E4%B8%9A%E7%95%8C%E5%85%A8%E6%99%AF%E4%B8%8E%E6%80%BB%E7%BB%93-The-Industrial-Landscape-Summary"><span class="toc-number">7.</span> <span class="toc-text">第六章：工业界全景与总结 (The Industrial Landscape &amp; Summary)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%A0%87%E5%87%86%E6%B5%81%E7%A8%8B%EF%BC%9A%E5%A4%9A%E9%98%B6%E6%AE%B5%E6%8E%92%E5%BA%8F%E6%BC%8F%E6%96%97%EF%BC%88%E5%8F%AC%E5%9B%9E%E3%80%81%E7%B2%97%E6%8E%92%E3%80%81%E7%B2%BE%E6%8E%92%EF%BC%89"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 工业界的标准流程：多阶段排序漏斗（召回、粗排、精排）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%BD%93%E5%89%8D%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E4%B8%BB%E5%8A%9B%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%B8%BA%E4%BD%95DeepFM%E6%9E%B6%E6%9E%84%E6%98%AF%E2%80%9C%E4%B8%87%E8%83%BD%E9%AA%A8%E6%9E%B6%E2%80%9D%EF%BC%9F"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 当前工业界的主力模型：为何DeepFM架构是“万能骨架”？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B%E8%B7%AF%E7%BA%BF%E5%9B%BE%EF%BC%9A%E4%BB%8ELR%E5%88%B0LLM%E7%9A%84%E5%9B%9E%E9%A1%BE%E4%B8%8E%E5%B1%95%E6%9C%9B"><span class="toc-number">7.3.</span> <span class="toc-text">6.3 技术演进路线图：从LR到LLM的回顾与展望</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/14/leetcode/LeetCode%20Hot100-Day%201/" title="LeetCode Hot100-Day 1"><img src="/images/cover/hot100.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode Hot100-Day 1"/></a><div class="content"><a class="title" href="/2025/08/14/leetcode/LeetCode%20Hot100-Day%201/" title="LeetCode Hot100-Day 1">LeetCode Hot100-Day 1</a><time datetime="2025-08-14T02:00:00.000Z" title="Created 2025-08-14 11:00:00">2025-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/13/leetcode/LeetCode%20Hot100-Day%200/" title="LeetCode Hot100-Day 0"><img src="/images/cover/hot100.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode Hot100-Day 0"/></a><div class="content"><a class="title" href="/2025/08/13/leetcode/LeetCode%20Hot100-Day%200/" title="LeetCode Hot100-Day 0">LeetCode Hot100-Day 0</a><time datetime="2025-08-13T13:00:00.000Z" title="Created 2025-08-13 22:00:00">2025-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/12/ML_1/" title="机器学习小记"><img src="/images/cover/Rec_AD_Cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习小记"/></a><div class="content"><a class="title" href="/2025/08/12/ML_1/" title="机器学习小记">机器学习小记</a><time datetime="2025-08-12T09:00:00.000Z" title="Created 2025-08-12 18:00:00">2025-08-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/12/DL_1/" title="深度学习小记"><img src="/images/cover/Rec_AD_Cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习小记"/></a><div class="content"><a class="title" href="/2025/08/12/DL_1/" title="深度学习小记">深度学习小记</a><time datetime="2025-08-12T03:00:00.000Z" title="Created 2025-08-12 12:00:00">2025-08-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标"><img src="/images/cover/Rec_AD_Cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="搜广推笔记 指标"/></a><div class="content"><a class="title" href="/2025/08/11/Rec_AD_Metrics/" title="搜广推笔记 指标">搜广推笔记 指标</a><time datetime="2025-08-11T06:00:00.000Z" title="Created 2025-08-11 15:00:00">2025-08-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2025 By Yuanpeng QU</div><div class="footer_custom_text">Welcome to Kyoku's <a href="https://qyp9909.github.io/">blog</a>! All right reserved.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="255,255,255" opacity="0.8" zIndex="-1" count="200" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>